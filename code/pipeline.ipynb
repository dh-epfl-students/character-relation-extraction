{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hollow-quarterly",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Installations\" data-toc-modified-id=\"Installations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Installations</a></span></li><li><span><a href=\"#Clean-all-cached-files\" data-toc-modified-id=\"Clean-all-cached-files-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Clean all cached files</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#NER-Person-Entity-extraction\" data-toc-modified-id=\"NER-Person-Entity-extraction-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>NER Person Entity extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-purpose-functions\" data-toc-modified-id=\"General-purpose-functions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>General purpose functions</a></span></li><li><span><a href=\"#Original-pipeline's-NER-method,-using-MobileBERT\" data-toc-modified-id=\"Original-pipeline's-NER-method,-using-MobileBERT-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Original pipeline's NER method, using MobileBERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-Entity-Extraction-(Using-David-Copperfield,-GT-#766)\" data-toc-modified-id=\"Test-Entity-Extraction-(Using-David-Copperfield,-GT-#766)-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Test Entity Extraction (Using David Copperfield, GT #766)</a></span></li><li><span><a href=\"#Test-Entity-Extraction-(Using-Lady-Susan,-GT-#946)\" data-toc-modified-id=\"Test-Entity-Extraction-(Using-Lady-Susan,-GT-#946)-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Test Entity Extraction (Using Lady Susan, GT #946)</a></span></li></ul></li><li><span><a href=\"#Final-pipeline's-NER-method,-using-LUKE\" data-toc-modified-id=\"Final-pipeline's-NER-method,-using-LUKE-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Final pipeline's NER method, using LUKE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-(using-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-David-Copperfield,-PG-#766)-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Test (using David Copperfield, PG #766)</a></span></li></ul></li></ul></li><li><span><a href=\"#Embeddings\" data-toc-modified-id=\"Embeddings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#General-purpose-functions\" data-toc-modified-id=\"General-purpose-functions-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>General purpose functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-each-entity's-surrounding-context\" data-toc-modified-id=\"Extract-each-entity's-surrounding-context-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Extract each entity's surrounding context</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-both-functions-(using-Lady-Susan)\" data-toc-modified-id=\"Test-both-functions-(using-Lady-Susan)-5.1.1.1\"><span class=\"toc-item-num\">5.1.1.1&nbsp;&nbsp;</span>Test both functions (using Lady Susan)</a></span></li></ul></li><li><span><a href=\"#Get-nearest-relations\" data-toc-modified-id=\"Get-nearest-relations-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Get nearest relations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-(using-PG-#766,-David-Copperfield)\" data-toc-modified-id=\"Test-(using-PG-#766,-David-Copperfield)-5.1.2.1\"><span class=\"toc-item-num\">5.1.2.1&nbsp;&nbsp;</span>Test (using PG #766, David Copperfield)</a></span></li></ul></li></ul></li><li><span><a href=\"#Word2Vec-(both-CBOW-and-Skip-Gram)-and-FastText-embeddings\" data-toc-modified-id=\"Word2Vec-(both-CBOW-and-Skip-Gram)-and-FastText-embeddings-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Word2Vec (both CBOW and Skip-Gram) and FastText embeddings</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Test-embedding-functions-(using-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-embedding-functions-(using-David-Copperfield,-PG-#766)-5.2.0.1\"><span class=\"toc-item-num\">5.2.0.1&nbsp;&nbsp;</span>Test embedding functions (using David Copperfield, PG #766)</a></span></li></ul></li></ul></li><li><span><a href=\"#Non-fine-tuned-BERT-embeddings\" data-toc-modified-id=\"Non-fine-tuned-BERT-embeddings-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Non-fine-tuned BERT embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-(using-bert-large-cased-and-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-bert-large-cased-and-David-Copperfield,-PG-#766)-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Test (using <code>bert-large-cased</code> and David Copperfield, PG #766)</a></span></li><li><span><a href=\"#Test-(using-bert-large-uncased-and-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-bert-large-uncased-and-David-Copperfield,-PG-#766)-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Test (using <code>bert-large-uncased</code> and David Copperfield, PG #766)</a></span></li><li><span><a href=\"#Test-(using-grouped_entities-and-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-grouped_entities-and-David-Copperfield,-PG-#766)-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>Test (using <code>grouped_entities</code> and David Copperfield, PG #766)</a></span></li></ul></li><li><span><a href=\"#Fine-tuned-BERT-embeddings\" data-toc-modified-id=\"Fine-tuned-BERT-embeddings-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Fine-tuned BERT embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tune-the-(BERT)-model\" data-toc-modified-id=\"Fine-tune-the-(BERT)-model-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Fine-tune the (BERT) model</a></span></li><li><span><a href=\"#Test-the-fine-tuned-BERT-model-to-check-the-fine-tuning-was-successful\" data-toc-modified-id=\"Test-the-fine-tuned-BERT-model-to-check-the-fine-tuning-was-successful-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Test the fine-tuned BERT model to check the fine-tuning was successful</a></span></li><li><span><a href=\"#Use-the-finetuned-model-to-generate-the-embeddings\" data-toc-modified-id=\"Use-the-finetuned-model-to-generate-the-embeddings-5.4.3\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>Use the finetuned model to generate the embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-(using-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-David-Copperfield,-PG-#766)-5.4.3.1\"><span class=\"toc-item-num\">5.4.3.1&nbsp;&nbsp;</span>Test (using David Copperfield, PG #766)</a></span></li></ul></li></ul></li><li><span><a href=\"#Weighted,-Unweighted-and-Mixed-(Weighted)-BERT-embeddings\" data-toc-modified-id=\"Weighted,-Unweighted-and-Mixed-(Weighted)-BERT-embeddings-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Weighted, Unweighted and Mixed (Weighted) BERT embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Non-weigthed-sums\" data-toc-modified-id=\"Non-weigthed-sums-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Non-weigthed sums</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-non-weighted-embeddings-for-all-results-for-entities\" data-toc-modified-id=\"Get-non-weighted-embeddings-for-all-results-for-entities-5.5.1.1\"><span class=\"toc-item-num\">5.5.1.1&nbsp;&nbsp;</span>Get non-weighted embeddings for all results for entities</a></span></li><li><span><a href=\"#Get-non-weighted-embeddings-for-top-N(=10)-results-for-entities\" data-toc-modified-id=\"Get-non-weighted-embeddings-for-top-N(=10)-results-for-entities-5.5.1.2\"><span class=\"toc-item-num\">5.5.1.2&nbsp;&nbsp;</span>Get non-weighted embeddings for top N(=10) results for entities</a></span></li><li><span><a href=\"#Get-non-weighted-embeddings-for-all-results-for-verbs\" data-toc-modified-id=\"Get-non-weighted-embeddings-for-all-results-for-verbs-5.5.1.3\"><span class=\"toc-item-num\">5.5.1.3&nbsp;&nbsp;</span>Get non-weighted embeddings for all results for verbs</a></span></li><li><span><a href=\"#Get-non-weighted-embeddings-for-all-post-N(=10)-results-for-verbs\" data-toc-modified-id=\"Get-non-weighted-embeddings-for-all-post-N(=10)-results-for-verbs-5.5.1.4\"><span class=\"toc-item-num\">5.5.1.4&nbsp;&nbsp;</span>Get non-weighted embeddings for all post N(=10) results for verbs</a></span></li></ul></li><li><span><a href=\"#Weighted-sums\" data-toc-modified-id=\"Weighted-sums-5.5.2\"><span class=\"toc-item-num\">5.5.2&nbsp;&nbsp;</span>Weighted sums</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-weighted-embeddings-for-all-results-for-entities\" data-toc-modified-id=\"Get-weighted-embeddings-for-all-results-for-entities-5.5.2.1\"><span class=\"toc-item-num\">5.5.2.1&nbsp;&nbsp;</span>Get weighted embeddings for all results for entities</a></span></li><li><span><a href=\"#Get-weighted-embeddings-for-top-N(=10)-results-for-entities\" data-toc-modified-id=\"Get-weighted-embeddings-for-top-N(=10)-results-for-entities-5.5.2.2\"><span class=\"toc-item-num\">5.5.2.2&nbsp;&nbsp;</span>Get weighted embeddings for top N(=10) results for entities</a></span></li><li><span><a href=\"#Get-weighted-embeddings-for-all-results-for-verbs\" data-toc-modified-id=\"Get-weighted-embeddings-for-all-results-for-verbs-5.5.2.3\"><span class=\"toc-item-num\">5.5.2.3&nbsp;&nbsp;</span>Get weighted embeddings for all results for verbs</a></span></li><li><span><a href=\"#Get-weighted-embeddings-for-all-post-N(=10)-results-for-verbs\" data-toc-modified-id=\"Get-weighted-embeddings-for-all-post-N(=10)-results-for-verbs-5.5.2.4\"><span class=\"toc-item-num\">5.5.2.4&nbsp;&nbsp;</span>Get weighted embeddings for all post N(=10) results for verbs</a></span></li></ul></li><li><span><a href=\"#Mixed-weighted-sums\" data-toc-modified-id=\"Mixed-weighted-sums-5.5.3\"><span class=\"toc-item-num\">5.5.3&nbsp;&nbsp;</span>Mixed weighted sums</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(CONCATENATED)\" data-toc-modified-id=\"Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(CONCATENATED)-5.5.3.1\"><span class=\"toc-item-num\">5.5.3.1&nbsp;&nbsp;</span>Weighted sum of ALL entitires and POST N(=10) verbs (CONCATENATED)</a></span></li><li><span><a href=\"#Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(SUMMED)\" data-toc-modified-id=\"Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(SUMMED)-5.5.3.2\"><span class=\"toc-item-num\">5.5.3.2&nbsp;&nbsp;</span>Weighted sum of ALL entitires and POST N(=10) verbs (SUMMED)</a></span></li><li><span><a href=\"#Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(CONCATENATED)---FINETUNED-VERSION\" data-toc-modified-id=\"Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(CONCATENATED)---FINETUNED-VERSION-5.5.3.3\"><span class=\"toc-item-num\">5.5.3.3&nbsp;&nbsp;</span>Weighted sum of ALL entitires and POST N(=10) verbs (CONCATENATED) - FINETUNED VERSION</a></span></li><li><span><a href=\"#Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(SUMMED)---FINETUNED-VERSION\" data-toc-modified-id=\"Weighted-sum-of-ALL-entitires-and-POST-N(=10)-verbs-(SUMMED)---FINETUNED-VERSION-5.5.3.4\"><span class=\"toc-item-num\">5.5.3.4&nbsp;&nbsp;</span>Weighted sum of ALL entitires and POST N(=10) verbs (SUMMED) - FINETUNED VERSION</a></span></li><li><span><a href=\"#Create-two-function-to-easily-generate-mixed-BERT-embeddings\" data-toc-modified-id=\"Create-two-function-to-easily-generate-mixed-BERT-embeddings-5.5.3.5\"><span class=\"toc-item-num\">5.5.3.5&nbsp;&nbsp;</span>Create two function to easily generate mixed BERT embeddings</a></span></li></ul></li></ul></li><li><span><a href=\"#Create-simpler-&quot;embeddings&quot;-based-only-on-the-nearest-entities-and-verbs\" data-toc-modified-id=\"Create-simpler-&quot;embeddings&quot;-based-only-on-the-nearest-entities-and-verbs-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Create simpler \"embeddings\" based only on the nearest entities and verbs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Entities\" data-toc-modified-id=\"Entities-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Entities</a></span></li><li><span><a href=\"#Verbs\" data-toc-modified-id=\"Verbs-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Verbs</a></span></li></ul></li></ul></li><li><span><a href=\"#Evalutation-metrics\" data-toc-modified-id=\"Evalutation-metrics-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evalutation metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-the-ground-truth-dataset-(for-David-Copperfield)\" data-toc-modified-id=\"Loading-the-ground-truth-dataset-(for-David-Copperfield)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Loading the ground truth dataset (for David Copperfield)</a></span></li><li><span><a href=\"#Streamlined-evaluation-pipeline\" data-toc-modified-id=\"Streamlined-evaluation-pipeline-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Streamlined evaluation pipeline</a></span></li></ul></li><li><span><a href=\"#Book-Entities\" data-toc-modified-id=\"Book-Entities-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Book Entities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-in-relevant-background-knowledge-(honorifics,-gendered-names)\" data-toc-modified-id=\"Read-in-relevant-background-knowledge-(honorifics,-gendered-names)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Read in relevant background knowledge (honorifics, gendered names)</a></span></li><li><span><a href=\"#Define-Book-Entity-and-related-classes-and-helper-functions\" data-toc-modified-id=\"Define-Book-Entity-and-related-classes-and-helper-functions-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Define Book Entity and related classes and helper functions</a></span></li><li><span><a href=\"#Embedding-and-plotting-related-functions\" data-toc-modified-id=\"Embedding-and-plotting-related-functions-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Embedding and plotting related functions</a></span></li><li><span><a href=\"#Create-a-wrapper-function-for-creating-BookEntities\" data-toc-modified-id=\"Create-a-wrapper-function-for-creating-BookEntities-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Create a wrapper function for creating BookEntities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Test-(using-David-Copperfield,-PG-#766)\" data-toc-modified-id=\"Test-(using-David-Copperfield,-PG-#766)-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Test (using David Copperfield, PG #766)</a></span></li></ul></li></ul></li><li><span><a href=\"#Expanding-to-other-books\" data-toc-modified-id=\"Expanding-to-other-books-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Expanding to other books</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1-in-Pipeline---use-LUKE-model-for-NER\" data-toc-modified-id=\"Step-1-in-Pipeline---use-LUKE-model-for-NER-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Step 1 in Pipeline - use LUKE model for NER</a></span></li><li><span><a href=\"#Step-2-in-Pipeline---create-BookEntities\" data-toc-modified-id=\"Step-2-in-Pipeline---create-BookEntities-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Step 2 in Pipeline - create BookEntities</a></span><ul class=\"toc-item\"><li><span><a href=\"#Moby-Dick-(PG-#2701)\" data-toc-modified-id=\"Moby-Dick-(PG-#2701)-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Moby Dick (PG #2701)</a></span></li><li><span><a href=\"#The-Brothers-Karamazov-(PG-#28054)\" data-toc-modified-id=\"The-Brothers-Karamazov-(PG-#28054)-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>The Brothers Karamazov (PG #28054)</a></span></li><li><span><a href=\"#Pride-and-Prejudice-(PG-#42671)\" data-toc-modified-id=\"Pride-and-Prejudice-(PG-#42671)-8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;</span>Pride and Prejudice (PG #42671)</a></span></li><li><span><a href=\"#The-Great-Gatsby-(PG-#64317)\" data-toc-modified-id=\"The-Great-Gatsby-(PG-#64317)-8.2.4\"><span class=\"toc-item-num\">8.2.4&nbsp;&nbsp;</span>The Great Gatsby (PG #64317)</a></span></li></ul></li><li><span><a href=\"#Bonus---combine-several-books'-characters'-visualization\" data-toc-modified-id=\"Bonus---combine-several-books'-characters'-visualization-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Bonus - combine several books' characters' visualization</a></span></li></ul></li><li><span><a href=\"#Future-work-suggestions\" data-toc-modified-id=\"Future-work-suggestions-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Future work suggestions</a></span><ul class=\"toc-item\"><li><span><a href=\"#N-grams-approach\" data-toc-modified-id=\"N-grams-approach-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>N-grams approach</a></span></li><li><span><a href=\"#Extract-Noun-Phrases-instead-of-entities\" data-toc-modified-id=\"Extract-Noun-Phrases-instead-of-entities-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Extract Noun Phrases instead of entities</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-authorization",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-cleveland",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section includes all the necessary libraries and files to reproduce this notebook's working environment, as well as the commands to install or download them.\n",
    "\n",
    "Bash\n",
    "```bash\n",
    "conda install pytorch torchvision -c pytorch\n",
    "#conda install -c huggingface transformers\n",
    "conda install -c conda-forge ipywidgets \n",
    "conda install -c conda-forge spacy \n",
    "conda install -c conda-forge nlp \n",
    "conda install -c anaconda scikit-learn \n",
    "conda install -c anaconda gensim \n",
    "conda install -c anaconda nltk \n",
    "conda install -c plotly plotly \n",
    "pip install tqdm\n",
    "pip install Fuzzy\n",
    "pip install stop-words\n",
    "pip install nameparser\n",
    "pip install editdistance\n",
    "pip install pyjarowinkler\n",
    "pip install libindic-utils\n",
    "pip install libindic-soundex\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "conda install -c conda-forge keras\n",
    "conda install -c conda-forge textblob\n",
    "conda install -c conda-forge nameparser \n",
    "\n",
    "pip install git+https://github.com/huggingface/transformers.git@refs/pull/11223/head\n",
    "\n",
    "curl https://raw.githubusercontent.com/carltonnorthern/nickname-and-diminutive-names-lookup/master/names.csv -o names.csv\n",
    "curl https://raw.githubusercontent.com/carltonnorthern/nickname-and-diminutive-names-lookup/master/python-parser.py -o python_parser.py\n",
    "curl https://media.geeksforgeeks.org/wp-content/uploads/male.txt -o ./male_names.txt\n",
    "curl https://media.geeksforgeeks.org/wp-content/uploads/female.txt -o ./female_names.txt\n",
    "```\n",
    "\n",
    "Python\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-spiritual",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-membership",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clean all cached files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5659d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section provides an easy easy to clear this notebook's cached files. \n",
    "\n",
    "*NB: Running this cell carelessly is **not** recommended, as some of these files can take some time to generate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-version",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !rm notebooks_data/*.csv\n",
    "# !rm notebooks_data/book_dfs/*.csv\n",
    "# !rm -r notebooks_data/book_dfs/tmp/*\n",
    "# !rm notebooks_data/BERT_embeddings/*.json\n",
    "# !rm notebooks_data/nearest_relations/*.json\n",
    "# !rm model_finetuning/BERT/*.csv\n",
    "# !rm model_finetuning/BERT/datasets/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-chrome",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-collaboration",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-chapel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:38:34.917582Z",
     "start_time": "2021-06-11T13:38:34.903657Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nameparser import HumanName\n",
    "from nameparser.config import Constants\n",
    "\n",
    "import fuzzy\n",
    "import string\n",
    "import editdistance\n",
    "from libindic.soundex import Soundex\n",
    "from stop_words import get_stop_words\n",
    "from python_parser import NameDenormalizer\n",
    "from pyjarowinkler.distance import get_jaro_distance\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertConfig, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         AutoTokenizer, AutoModelForTokenClassification, pipeline, BertTokenizer, BertModel, \\\n",
    "                         LukeTokenizer, LukeForEntitySpanClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-services",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-seller",
   "metadata": {},
   "source": [
    "## NER Person Entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c02f9",
   "metadata": {},
   "source": [
    "### General purpose functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ee6b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:05.199324Z",
     "start_time": "2021-06-11T13:17:05.190654Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_book_text(gutenberg_id):\n",
    "    '''Given a book ID, returns the book's text, excluding Project Gutenberg's header and outro.\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    '''\n",
    "    \n",
    "    context = ''\n",
    "    with open(f'../gutenberg/data/raw/PG{gutenberg_id}_raw.txt', mode='r', encoding='utf-8') as f:\n",
    "        context = f.read()\n",
    "    return ' '.join([l for l in (context.split('End of the Project Gutenberg EBook of ')[0]\n",
    "                                        .split('*** END OF THE PROJECT GUTENBERG EBOOK')[0]\n",
    "                                        .split('\\n')) if l][16:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f563b",
   "metadata": {},
   "source": [
    "### Original pipeline's NER method, using MobileBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-dayton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:05.217460Z",
     "start_time": "2021-06-11T13:17:05.201497Z"
    },
    "code_folding": [
     0,
     80
    ]
   },
   "outputs": [],
   "source": [
    "def get_line_entities(l, ner_entities_tokens, ner_entities_words, sentence_index, tokenizer, nlp,\n",
    "                     grouped_entities):\n",
    "    '''Given a line, lists for tokens and words, and word index at the end of the sentence, as well as\n",
    "    the tokenizer and nlp model instances (from huggingface's transformers), updates the tokens and\n",
    "    words lists and the word index to include the given line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the sentence\n",
    "    tokenizer : AutoTokenizer\n",
    "        huggingface's tokenizer being used in the NER pipeline\n",
    "    nlp : pipeline\n",
    "        huggingface's NER pipeline object\n",
    "    grouped_entities : bool\n",
    "        Flag indicating whether the NER pipeline is configured to output grouped_entities or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found so far, across this and all the previous lines\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), their index in the sentence and \n",
    "        in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0), across this and all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the next sentence\n",
    "    '''\n",
    "    new_entity_tokens = []\n",
    "    if grouped_entities:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity_group']]\n",
    "    else:\n",
    "        new_entity_tokens = [e for e in nlp(l) if 'PER' in e['entity']]\n",
    "    ner_entities_tokens += new_entity_tokens\n",
    "    \n",
    "    tokenized_line = tokenizer(l)\n",
    "    line_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "                    for w in word_tokenize(\n",
    "                                           re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                                  l).replace('cannot', \n",
    "                                                            'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    \n",
    "    # go from token to word with\n",
    "    for et in new_entity_tokens:\n",
    "        if grouped_entities:\n",
    "            # find index of grouped entity\n",
    "            reconstructed_line = ' '.join([lw.lower() for lw in line_words])\n",
    "            first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word']))[0]\n",
    "            if et['word'][0] == '#':\n",
    "                first_word = word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', et['word'][2:]))[0]\n",
    "            \n",
    "            word_index = len(reconstructed_line[:reconstructed_line.index(first_word)].split())\n",
    "\n",
    "            if et['word'] not in stopwords.words('english') and et['word'].isalpha():\n",
    "                # record grouped entity\n",
    "                ner_entities_words += [{'full_word': et['word'], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "        else:\n",
    "            # record non-grouped entity\n",
    "            word_index = tokenized_line.word_ids()[et['index']]\n",
    "            if line_words[word_index] not in stopwords.words('english') and line_words[word_index].isalpha():\n",
    "                ner_entities_words += [{'full_word': line_words[word_index], \n",
    "                                        'sentence_word_index': word_index, \n",
    "                                        'total_word_index': sentence_index+word_index,\n",
    "                                        'score': et['score']}]\n",
    "    sentence_index += len(line_words)\n",
    "    return ner_entities_tokens, ner_entities_words, sentence_index\n",
    "\n",
    "def get_person_entities(gutenberg_id, grouped_entities=False, max_chunk_len=512, split_chunk_len=256):\n",
    "    '''Given a book ID, returns its text (excluding Project Gutenberg's intro and outro), all its \n",
    "    tokens classified as PER (Person) entities, and all the words corresponding to those tokens, as \n",
    "    well as their index in the sentence and in the book, and their classification score as a PER entity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline is configured to outout grouped_entities or not \n",
    "        (default is False)\n",
    "    max_chunk_len : int, optional\n",
    "        Maximum character-level length of each sentence passed to the model (default is 512)\n",
    "    split_chunk_len : int, optional\n",
    "        Maximum character-level length of each sub-sentence passed to the model, when splitting an\n",
    "        overly big sentence into smaller sub-sentences (default is 256)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    ner_entities_tokens : list\n",
    "        A list containing all the Person tokens found across the whole book\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found across the whole book \n",
    "        (the full word corresponding to them (i.e. not separated tokens), their index in the sentence \n",
    "        and in the book overall, and their PER-entity classification score, a number between 0.0 and \n",
    "        1.0)\n",
    "    '''\n",
    "    # code is correct, but gives a warning about the model not having a predefined maximum length,\n",
    "    # suppressing those warnings to not interfere with tdqm progress bar\n",
    "    import warnings\n",
    "    from transformers import logging\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "    logging.set_verbosity_error()\n",
    "    \n",
    "    # read in gutenberg book\n",
    "    book_text =  get_book_text(gutenberg_id)\n",
    "\n",
    "    # load NER model and tokenizer\n",
    "    ner_model = 'mrm8488/mobilebert-finetuned-ner'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ner_model, max_length=max_chunk_len)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(ner_model, max_length=max_chunk_len)\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=grouped_entities)\n",
    "    \n",
    "    # prepare for iteration over the book\n",
    "    sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "    ner_entities_tokens = []\n",
    "    ner_entities_words = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    # iterate over sentence-level chunks\n",
    "    for l in tqdm(sentence_level_book):\n",
    "        if len(l) > max_chunk_len:\n",
    "            for m in range(len(l) // split_chunk_len + 1):\n",
    "                new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:(m+1)*split_chunk_len])\n",
    "                ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(new_l, \n",
    "                                                                                            ner_entities_tokens, \n",
    "                                                                                            ner_entities_words,\n",
    "                                                                                            sentence_index, \n",
    "                                                                                            tokenizer,\n",
    "                                                                                            nlp,\n",
    "                                                                                            grouped_entities)\n",
    "        else:\n",
    "            ner_entities_tokens, ner_entities_words, sentence_index = get_line_entities(l, \n",
    "                                                                                        ner_entities_tokens, \n",
    "                                                                                        ner_entities_words,\n",
    "                                                                                        sentence_index, \n",
    "                                                                                        tokenizer, \n",
    "                                                                                        nlp,\n",
    "                                                                                        grouped_entities)\n",
    "\n",
    "    return book_text, ner_entities_tokens, ner_entities_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-bathroom",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test Entity Extraction (Using David Copperfield, GT #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-constitution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T11:07:56.964263Z",
     "start_time": "2021-06-11T10:58:52.452576Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all token and entity info\n",
    "david_copperfield_id = '766'\n",
    "(david_copperfield_text, \n",
    " david_copperfield_ent_tokens, \n",
    " david_copperfield_ent_words) = get_person_entities(david_copperfield_id)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "david_copperfield_df = pd.DataFrame(david_copperfield_ent_words)\n",
    "david_copperfield_df['full_word'] =  david_copperfield_df['full_word'].apply(lambda s: s.lower())\n",
    "david_copperfield_df.to_csv('notebooks_data/book_dfs/david_copperfield_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(david_copperfield_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-teaching",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test Entity Extraction (Using Lady Susan, GT #946)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-germany",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T11:08:37.855074Z",
     "start_time": "2021-06-11T11:07:56.966294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get all token and entity info\n",
    "lady_susan_path = '946'\n",
    "(lady_susan_text, \n",
    " lady_susan_ent_tokens, \n",
    " lady_susan_ent_words) = get_person_entities(lady_susan_path)\n",
    "\n",
    "# save entities into a dataframe, and to the disk\n",
    "lady_susan_df = pd.DataFrame(lady_susan_ent_words)\n",
    "lady_susan_df['full_word'] =  lady_susan_df['full_word'].apply(lambda s: s.lower())\n",
    "lady_susan_df.to_csv('notebooks_data/book_dfs/lady_susan_df.csv', index=False) \n",
    "\n",
    "# view top 25 entities\n",
    "(lady_susan_df\n",
    " .drop_duplicates('total_word_index')\n",
    " .groupby('full_word')\n",
    " .count()\n",
    " .sort_values(by='score', ascending=False)\n",
    ")['score'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-opinion",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c25c56",
   "metadata": {},
   "source": [
    "### Final pipeline's NER method, using LUKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03871eee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:07.721275Z",
     "start_time": "2021-06-11T13:17:07.706946Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_LUKE_tokens_labels(l, tokenizer, model, nlp):\n",
    "    '''Given a line, and the relevant NER and Laguage models and tokenizer, returns a list of tuples\n",
    "    containing all the identified entities in that line.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    tokenizer : LukeTokenizer\n",
    "        The LUKE NER model's tokenizer\n",
    "    model : LukeForEntitySpanClassification\n",
    "        The LUKE NER model instance\n",
    "    nlp : Language\n",
    "        spaCy's language model (code written for the \"en_core_web_sm\" version)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    joint_per_entities : list\n",
    "        A list of tuples, each tuple containing the joint entity (a single string with its several \n",
    "        tokens), and the (ordered and consecutive) indices of all its tokens\n",
    "    line_len : int\n",
    "        The length, in tokens, of l\n",
    "    '''\n",
    "    doc = nlp(l)\n",
    "\n",
    "    entity_spans = []\n",
    "    original_word_spans = []\n",
    "    for token_start in doc:\n",
    "        for token_end in doc[token_start.i:]:\n",
    "            entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n",
    "            original_word_spans.append((token_start.i, token_end.i + 1))\n",
    "\n",
    "    inputs = tokenizer(l, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
    "#     inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    max_logits, max_indices = logits[0].max(dim=1)\n",
    "    predictions = []\n",
    "    for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n",
    "        if index != 0:  # the span is not NIL\n",
    "            predictions.append((logit, span, model.config.id2label[int(index)]))\n",
    "            \n",
    "    joint_per_entities = []            \n",
    "    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n",
    "        if label == 'PER':\n",
    "            joint_per_entities.append((' '.join([str(doc[i]) for i in range(span[0], span[1])]),\n",
    "                                       [i for i in range(span[0], span[1])]))\n",
    "\n",
    "    return joint_per_entities, len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc480e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:07.912268Z",
     "start_time": "2021-06-11T13:17:07.897765Z"
    },
    "code_folding": [
     0,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def get_LUKE_line_entities(l, ner_entities_words, sentence_index, tokenizer, model, nlp):\n",
    "    '''Given a line, and the relevant NER and Laguage models and tokenizer, updates the entities'\n",
    "    words list to reflect new entities found in that line, and the overall sentence index,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    l : str\n",
    "        The line to analyze\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), and their index in the sentence \n",
    "        and in the book overall), across all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the sentence\n",
    "    tokenizer : LukeTokenizer\n",
    "        The LUKE NER model's tokenizer\n",
    "    model : LukeForEntitySpanClassification\n",
    "        The LUKE NER model instance\n",
    "    nlp : Language\n",
    "        spaCy's language model (code written for the \"en_core_web_sm\" version)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found so far (the full \n",
    "        word corresponding to them (i.e. not separated tokens), and their index in the sentence \n",
    "        and in the book overall), across this and all the previous lines\n",
    "    sentence_index : int\n",
    "        The overall (book-wise) index of the first word of the next sentence\n",
    "    '''\n",
    "    \n",
    "    # guarantee line is not empty\n",
    "    if not l.strip():\n",
    "        return ner_entities_words, sentence_index\n",
    "    \n",
    "    # process line tokens and labels\n",
    "    entities, sentence_len = get_LUKE_tokens_labels(l, tokenizer, model, nlp)\n",
    "    for ent in entities:\n",
    "        ner_entities_words += [{'full_word': ent[0], \n",
    "                                'sentence_word_index': ent[1][0], \n",
    "                                'total_word_index': sentence_index + ent[1][0]}]\n",
    "    sentence_index += sentence_len\n",
    "    return ner_entities_words, sentence_index\n",
    "\n",
    "def get_LUKE_person_entities(gutenberg_id, checkpoint_directory_path, max_chunk_len=256, \n",
    "                             split_chunk_len=42, last_checkpoint=-1, ner_entities_words=[]):\n",
    "    '''Given a book ID, returns its text (excluding Project Gutenberg's intro and outro), \n",
    "    all its tokens classified as PER (Person) entities, and all the words corresponding to \n",
    "    those token, as well as their index in the sentence and in the book.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    checkpoint_directory_path : str\n",
    "        The directory in which to save checkpoints at every 500th iteration\n",
    "    max_chunk_len : int, optional\n",
    "        Maximum character-level length of each sentence passed to the model (default is 512)\n",
    "    split_chunk_len : int, optional\n",
    "        Maximum word-level length (i.e. maximum number of words) of each sub-sentence passed \n",
    "        to the model, when splitting an overly big sentence into smaller sub-sentences (default \n",
    "        is 42)\n",
    "    last_checkpoint : int, optional\n",
    "        Checkpoint to resume from (default is -1)\n",
    "    ner_entities_words : list, optional\n",
    "        List containing dictionary entries of all the Person entities found up to 'last_checkpoint' \n",
    "        (default is [])\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_text : str\n",
    "        The book's text, excluding Project Gutenberg's header and outro\n",
    "    ner_entities_words : list\n",
    "        A list containing dictionary entries of all the Person entities found across the whole book \n",
    "        (the full word corresponding to them (i.e. not separated tokens), and their index in the sentence \n",
    "        and in the book overall)\n",
    "    '''\n",
    "    \n",
    "    # read in gutenberg book\n",
    "    book_text = get_book_text(gutenberg_id)\n",
    "\n",
    "    # Load the model checkpoint and tokenizer\n",
    "    model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\", )\n",
    "    model.eval()\n",
    "#     model.to(\"cuda\")\n",
    "    tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "\n",
    "    # prepare for iteration over the book\n",
    "    sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "    sentence_index = 0\n",
    "    \n",
    "    # iterate over sentence-level chunks\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for i, l in enumerate(tqdm(sentence_level_book)):\n",
    "        \n",
    "        if i <= last_checkpoint:\n",
    "            sentence_index += len(nlp(l))\n",
    "            continue\n",
    "            \n",
    "        if len(l) > max_chunk_len:\n",
    "            for m in range(len(l) // split_chunk_len + 1):\n",
    "                new_l = ' '.join(l.split(' ')[m*split_chunk_len:][:split_chunk_len])\n",
    "                ner_entities_words, sentence_index = get_LUKE_line_entities(new_l, ner_entities_words,\n",
    "                                                                            sentence_index, tokenizer, \n",
    "                                                                            model, nlp)\n",
    "        else:\n",
    "            ner_entities_words, sentence_index = get_LUKE_line_entities(l, ner_entities_words,\n",
    "                                                                        sentence_index, tokenizer, \n",
    "                                                                        model, nlp)\n",
    "    \n",
    "        if i % 500 == 0:\n",
    "            # save a snapshot of the current progress\n",
    "            tmp_df = pd.DataFrame(ner_entities_words)\n",
    "            tmp_df.to_csv(f'{checkpoint_directory_path}luke_tmp_df_{i:05d}.csv', index=False)\n",
    "\n",
    "    return book_text, ner_entities_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a13ea6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test (using David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb749fdd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = 766\n",
    "book_luke_checkpoints_tmp_dir = f'notebooks_data/book_dfs/tmp/{book_pg_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f9092",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continue_from_ckpt = False\n",
    "\n",
    "if not continue_from_ckpt:\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir, \n",
    "                                                             ner_entities_words = [])\n",
    "    \n",
    "else:\n",
    "    # continue from checkpoint, if needed\n",
    "    latest_checkpoint = 0\n",
    "    luke_df = pd.read_csv(f'{book_luke_checkpoints_tmp_dir}luke_tmp_df_{latest_checkpoint:05d}.csv')\n",
    "    ner_entities_checkpoint = [dict(zip(['full_word', 'sentence_word_index', 'total_word_index'], v)) \n",
    "                               for v in [tuple(r) for r in luke_df.to_numpy()]]\n",
    "\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir,\n",
    "                                                             last_checkpoint = latest_checkpoint,\n",
    "                                                             ner_entities_words = ner_entities_checkpoint)\n",
    "    \n",
    "luke_df = pd.DataFrame(ner_entities_words)\n",
    "luke_df.to_csv(f'notebooks_data/book_dfs/luke_{book_pg_id}_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907eac10",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a97c6d",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-easter",
   "metadata": {},
   "source": [
    "### General purpose functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9f589",
   "metadata": {},
   "source": [
    "#### Extract each entity's surrounding context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-decision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:10.301797Z",
     "start_time": "2021-06-11T13:17:10.289330Z"
    },
    "code_folding": [
     0,
     31
    ]
   },
   "outputs": [],
   "source": [
    "def get_name_window(total_word_index, gutenberg_id, window_size=3):\n",
    "    '''Given the index of a single word in the book, and the ID of the book, returns the book window\n",
    "    surrounding that word.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_word_index : int\n",
    "        The book-wise index of the word for which to extract the context\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 3 will return a context of 7 words (3 + 1 + 3)) (default is 3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context : str\n",
    "        The context surrounding the given word's index\n",
    "    '''\n",
    "    \n",
    "    book_text = get_book_text(gutenberg_id)\n",
    "\n",
    "    # prepare for iteration over the book\n",
    "    book_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "              for w in word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                            ' '.join(re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text))\n",
    "                                           ).replace('cannot', 'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    \n",
    "    word_window = book_words[max(total_word_index - window_size, 0): total_word_index + window_size]\n",
    "    return re.sub(r' ([^a-zA-Z0-9]) ', '[_]\\g<0>[_] ', ' '.join(word_window)).replace('[_] ', '')\n",
    "\n",
    "def get_all_name_windows(total_word_indexes, gutenberg_id, window_size=3):\n",
    "    '''Given several word indexes in the book, and the ID of the book, returns the book windows surrounding \n",
    "    each of those words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_word_indexes : list\n",
    "        The list of book-wise indexes of the words for which to extract the context\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 3 will return a context of 7 words (3 + 1 + 3)) (default is 3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    contexts : list\n",
    "        The list of contexts surrounding each given word's indexes\n",
    "    '''\n",
    "    \n",
    "    book_text = get_book_text(gutenberg_id)\n",
    "\n",
    "    # prepare for iteration over the book\n",
    "    book_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "              for w in word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                            ' '.join(re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text))\n",
    "                                           ).replace('cannot', 'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    \n",
    "    result = []\n",
    "    for total_word_index in tqdm(total_word_indexes):\n",
    "        word_window = book_words[max(total_word_index - window_size, 0): total_word_index + window_size]\n",
    "        result.append(re.sub(r' ([^a-zA-Z0-9]) ', '[_]\\g<0>[_] ', ' '.join(word_window)).replace('[_] ', ''))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-cycle",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Test both functions (using Lady Susan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-silicon",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lady_susan_df = pd.read_csv('notebooks_data/lady_susan_df.csv')\n",
    "#\n",
    "# for i, row in lady_susan_df.drop_duplicates('total_word_index').iterrows():\n",
    "#     print(get_name_window(row.total_word_index, '946'))\n",
    "#     if i > 9:\n",
    "#         break\n",
    "#        \n",
    "# get_all_name_windows(lady_susan_df.drop_duplicates('total_word_index')['total_word_index'][:10].to_list(), \n",
    "#                      '946')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62c880",
   "metadata": {},
   "source": [
    "#### Get nearest relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47932ea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:11.401801Z",
     "start_time": "2021-06-11T13:17:11.382348Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_nearest_relations(gutenberg_id, window_size = 15, grouped_entities=False):\n",
    "    '''Given a book ID, return a dictionary with the count, for each of the book's entities,\n",
    "    of its nearest verbs and other entities, where proximity is constrained by the `window_size`\n",
    "    parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 15 will find nearest entities in a 31-word (15 + 1 + 15) window surrounding each entity, \n",
    "        and the nearest verbs in a 30-word (15*2) after it) (default is 15)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nearest_relations : dictionary\n",
    "        A dictionary containing the nearest entities and verbs to each entity and their count\n",
    "    '''\n",
    "    \n",
    "    # check if df already exists on the disk\n",
    "    nearest_relations_path = f'notebooks_data/nearest_relations/{gutenberg_id}_{window_size}.json'\n",
    "    if grouped_entities:\n",
    "        nearest_relations_path = nearest_relations_path[:-5] + '_GE.json'\n",
    "    if os.path.isfile(nearest_relations_path):\n",
    "        with open(nearest_relations_path) as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "        \n",
    "    book_text = get_book_text(gutenberg_id)\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "\n",
    "    # prepare for iteration over the book\n",
    "    book_words = [w if w != 'word_tokenize_splits_cannot_into_2_words' else 'cannot'\n",
    "              for w in word_tokenize(re.sub(r'[^a-zA-Z0-9]', ' \\g<0> ', \n",
    "                                            ' '.join(re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text))\n",
    "                                           ).replace('cannot', 'word_tokenize_splits_cannot_into_2_words'))]\n",
    "    book_df['total_word_index'].to_list()\n",
    "    \n",
    "    nearest_relations = {}\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    for i, row in tqdm(book_df.iterrows(), total=book_df.shape[0]):\n",
    "        # get main entity, its lenght, and its index in the book\n",
    "        entity = row['full_word']\n",
    "        main_entity_words = len(entity.split())\n",
    "        total_word_index = row['total_word_index']\n",
    "        \n",
    "        # create all relevant access indexes\n",
    "        word_idx = total_word_index\n",
    "        after_word_idx = total_word_index + main_entity_words\n",
    "        ent_start_idx = max(total_word_index - window_size, 0)\n",
    "        ent_end_idx = total_word_index + main_entity_words + window_size\n",
    "        verb_end_idx = total_word_index + main_entity_words + 2*window_size\n",
    "        \n",
    "        # get nearest entities\n",
    "        near_entities_window = book_words[ent_start_idx:word_idx] + book_words[after_word_idx:ent_end_idx]\n",
    "        near_entities_text = re.sub(r' ([^a-zA-Z0-9]) ', '[_]\\g<0>[_] ', \n",
    "                                    ' '.join(near_entities_window)).replace('[_] ', '')\n",
    "        near_entities = [w[0].lower() for w in nltk.pos_tag(word_tokenize(near_entities_text)) \n",
    "                         if (w[1].startswith('NNP') \n",
    "                             and w[0].lower() not in stopwords.words('english')\n",
    "                             and w[0].isalpha())]\n",
    "        \n",
    "        # get nearest verbs\n",
    "        near_verbs_window = book_words[after_word_idx: verb_end_idx]\n",
    "        near_verbs_text = re.sub(r' ([^a-zA-Z0-9]) ', '[_]\\g<0>[_] ', \n",
    "                                 ' '.join(near_verbs_window)).replace('[_] ', '')\n",
    "        near_verbs = [lemma_function.lemmatize(w[0], pos='v') for w in nltk.pos_tag(word_tokenize(near_verbs_text)) \n",
    "                      if w[1].startswith('V') and w[0].isalpha() and len(w[0]) > 1]\n",
    "\n",
    "        # record (and count) each near entity and verb\n",
    "        if entity not in nearest_relations:\n",
    "            nearest_relations[entity] = {'near_entities': {}, 'near_verbs': {}}\n",
    "\n",
    "        for e in near_entities:\n",
    "            if e in nearest_relations[entity]['near_entities']:\n",
    "                nearest_relations[entity]['near_entities'][e] = nearest_relations[entity]['near_entities'][e] + 1\n",
    "            else:\n",
    "                nearest_relations[entity]['near_entities'][e] = 1\n",
    "        for v in near_verbs:\n",
    "            if v in nearest_relations[entity]['near_verbs']:\n",
    "                nearest_relations[entity]['near_verbs'][v] = nearest_relations[entity]['near_verbs'][v] + 1\n",
    "            else:\n",
    "                nearest_relations[entity]['near_verbs'][v] = 1\n",
    "\n",
    "\n",
    "    # save embeddings to disk and then return them\n",
    "    with open(nearest_relations_path, 'w+') as f:\n",
    "        json.dump(nearest_relations, f)\n",
    "    return nearest_relations      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9f8b0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Test (using PG #766, David Copperfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc7fb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nr = get_nearest_relations('766')\n",
    "\n",
    "character = 'copperfield'\n",
    "list_len = 5\n",
    "\n",
    "(pd.concat([pd.DataFrame(nr[character])[['near_entities']].dropna().sort_values(by='near_entities', ascending=False)\n",
    " .reset_index().add_prefix('Ent_'), pd.DataFrame(nr[character])[['near_verbs']].dropna()\n",
    " .sort_values(by='near_verbs', ascending=False).reset_index().add_prefix('Verb_')], axis=1\n",
    ")[:list_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d120c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-tobacco",
   "metadata": {},
   "source": [
    "### Word2Vec (both CBOW and Skip-Gram) and FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-luther",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:12.469532Z",
     "start_time": "2021-06-11T13:17:12.451165Z"
    },
    "code_folding": [
     0,
     35,
     97
    ]
   },
   "outputs": [],
   "source": [
    "def get_book_df(gutenberg_id, grouped_entities=False):\n",
    "    '''Creates an Entities dataframe for the given book. If it already exists, just loads it from memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    book_df : DataFrame\n",
    "        A DataFrame containing the book's entities, their sentence-wise and book-wise indexes and their\n",
    "        PER-classification scores\n",
    "    '''\n",
    "    \n",
    "    # check if df already exists on the disk\n",
    "    book_csv_path = f'notebooks_data/book_dfs/{gutenberg_id}.csv'\n",
    "    if grouped_entities:\n",
    "        book_csv_path = f'notebooks_data/book_dfs/{gutenberg_id}_grouped.csv'\n",
    "    if os.path.isfile(book_csv_path):\n",
    "        return pd.read_csv(book_csv_path)\n",
    "    \n",
    "    # if df doesn't already exist, create it\n",
    "    (book_text, book_ent_tokens, book_ent_words) = get_person_entities(gutenberg_id, \n",
    "                                                                       grouped_entities=grouped_entities)\n",
    "    book_df = pd.DataFrame(book_ent_words)\n",
    "    book_df['full_word'] =  book_df['full_word'].apply(lambda s: s.lower())\n",
    "    \n",
    "    # save df to disk and then return it\n",
    "    book_df.to_csv(book_csv_path, index=False) \n",
    "    return book_df\n",
    "\n",
    "def get_embeddings_model(gutenberg_id, whole_book_embedding=True, embedding_model='fasttext', \n",
    "                         context_window_size=6, min_count=5, grouped_entities=False):\n",
    "    '''Given a book ID, creates an embedding model for it, according to the specifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    whole_book_embedding : bool, optional\n",
    "        Wether to use the whole book to generate the embeddings (when true) or just context \n",
    "        windows surrounding entity references (default is True)\n",
    "    embedding_model : str, optional\n",
    "        Which embedding model to use, between skipgram, cbow and fasttext (default is 'fasttext')\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 6 will return a context of 13 words (6 + 1 + 6)) (default is 6)\n",
    "    min_count : int, optional\n",
    "        The minimum amount of times any term needs to appear in order for an embedding for it to be\n",
    "        generated (default is 5)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    emb_model : Model\n",
    "        The trained embeddings model\n",
    "    '''\n",
    "    \n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    \n",
    "    # use either the whole book for the embeddings or just windows near entity mentions\n",
    "    context = ''\n",
    "    if whole_book_embedding:\n",
    "        # set context as the whole book, split into sentences\n",
    "        context = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', get_book_text(gutenberg_id))\n",
    "    else:\n",
    "        context = get_all_name_windows(book_df['total_word_index'].to_list(), gutenberg_id, \n",
    "                                       window_size=context_window_size)\n",
    "    \n",
    "    # clean and prepare the text\n",
    "    processed_context = [re.sub(r'\\s+', ' ', re.sub('[^a-zA-Z]', ' ', c.lower())) for c in context]\n",
    "    all_sentences = [t for c in processed_context for t in nltk.sent_tokenize(c)]\n",
    "    all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "    # remove stopwords\n",
    "    for i in range(len(all_words)):\n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    \n",
    "    # select and create the embeddings\n",
    "    if any([embedding_model.lower() == sg for sg in ['skipgram', 'skip-gram', 'skip gram', 'sg']]):\n",
    "        # create SG model and return it\n",
    "        return Word2Vec(all_words, sg=1, size=100,  workers=4, min_count=min_count)\n",
    "        \n",
    "    elif any([embedding_model.lower() == cbow for cbow in ['continuousbagofwords', 'continuous-bag-of-words', \n",
    "                                                           'continuous bag of words', 'cbow']]):\n",
    "        # create CBOW model and return it\n",
    "        return Word2Vec(all_words, sg=0, size=100,  workers=4, min_count=min_count)\n",
    "    \n",
    "    # create FastText model and return it\n",
    "    return FastText(all_words, min_count=min_count)\n",
    "\n",
    "def get_entities_embeddings(gutenberg_id, emb_model, grouped_entities=False):\n",
    "    '''Given the book DF and the embeddings model, returns a dictionary for the embeddings\n",
    "    corresponding to each entity (entity list obtained from the book's DF).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    emb_model : Model\n",
    "        The trained (gensim) embeddings model\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent_vectors : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vector\n",
    "    '''\n",
    "    \n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    ent_vectors = {}\n",
    "    for n in book_df['full_word'].unique():\n",
    "        if n in emb_model.wv.vocab:\n",
    "            ent_vectors[n] = emb_model.wv[n]\n",
    "            \n",
    "    return ent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-conditioning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:12.791334Z",
     "start_time": "2021-06-11T13:17:12.779657Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_embeddings_2D(gutenberg_id, emb_vectors, title, max_vectors=None, min_count=5, grouped_entities=False):\n",
    "    '''Given the book DF, the embedding vectors and the title, plots them in 2D, using PCA for\n",
    "    the dimensionality reduction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    ent_vectors : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vector\n",
    "    title : str\n",
    "        The plot's title\n",
    "    max_vectors : int, optional\n",
    "        The maximum number of entities to display in the plot. If not None, the max_vector most common\n",
    "        entities will be plotted (default is None)\n",
    "    min_count : int, optional\n",
    "        The minimum amount of times any entity needs to appear in the books in order for it to be\n",
    "        plotted (default is 5)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "    '''\n",
    "    \n",
    "    # get entities and apply min_count\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    tmp_df = book_df.groupby(['full_word']).count().reset_index()\n",
    "    tmp_df = tmp_df[tmp_df['full_word'].str.isalpha()]\n",
    "    tmp_df = tmp_df[tmp_df['score'] >= min_count][['full_word', 'score']]\n",
    "    \n",
    "    # get embeddings (apply max_vectors if not None)\n",
    "    key_list = (tmp_df.sort_values(by='score', ascending=False).reset_index())\n",
    "    \n",
    "    if max_vectors:\n",
    "        key_list = key_list[:max_vectors]\n",
    "    key_list = key_list.sort_values(by='full_word')['full_word'].unique()\n",
    "    vec_to_plot = { key: emb_vectors[key] for key in key_list if key in emb_vectors}\n",
    "        \n",
    "    # apply PCA\n",
    "    sg_df = pd.DataFrame(vec_to_plot).T\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(sg_df)\n",
    "    \n",
    "    # plot the vectors\n",
    "    fig = px.scatter(components, x=0, y=1, color=sg_df.index, title=title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-substitute",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Test embedding functions (using David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-occupation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T16:07:04.337017Z",
     "start_time": "2021-06-06T16:06:08.912289Z"
    },
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "copperfield_emb_model = get_embeddings_model('766')\n",
    "copperfield_ent_embeddings = get_entities_embeddings('766', copperfield_emb_model)\n",
    "plot_embeddings_2D('766', copperfield_ent_embeddings, \n",
    "                   'FastText Embeddings - David Copperfield - Whole Book - 25 Most Frequent Names', \n",
    "                   max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-plasma",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-moscow",
   "metadata": {},
   "source": [
    "### Non-fine-tuned BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-samuel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:15.114392Z",
     "start_time": "2021-06-11T13:17:15.092471Z"
    },
    "code_folding": [
     0,
     88
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_BERT_embeddings(gutenberg_id, context_window_size=7, bert_model='bert-large-cased', \n",
    "                            grouped_entities=False):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each occurence of\n",
    "    each entity (in a context of the specified size), all the [CLS], [MASK] and mean context embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    bert_model : str, optional\n",
    "        The underlying pre-trained BERT model to use (default is 'bert-large-cased')\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, the lists of all their [MASK], [CLS], and \n",
    "        'context average' embeddings\n",
    "    '''\n",
    "    \n",
    "    # check if df already exists on the disk\n",
    "    embeddings_path = f'notebooks_data/BERT_embeddings/{gutenberg_id}_{context_window_size}_{bert_model}.json'\n",
    "    if grouped_entities:\n",
    "        embeddings_path = embeddings_path[:-5] + '_GE.json'\n",
    "    if os.path.isfile(embeddings_path):\n",
    "        with open(embeddings_path) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # get book df, all its entities and all their contexts\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities)\n",
    "    entities = book_df.drop_duplicates('total_word_index')['full_word'].to_list()\n",
    "    contexts = get_all_name_windows(book_df.drop_duplicates('total_word_index')['total_word_index'].to_list(), \n",
    "                         gutenberg_id, window_size=context_window_size)\n",
    "\n",
    "    # load BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = {}\n",
    "    for i, zipped in enumerate(tqdm(zip(contexts, entities), total=len(entities))):\n",
    "\n",
    "        # prepare context for embedding\n",
    "        context, main_entity = zipped\n",
    "        text = ''\n",
    "        if grouped_entities:\n",
    "            split_main_entity = main_entity.split()\n",
    "            if split_main_entity[0][0] == '#':\n",
    "                split_main_entity[0] = split_main_entity[0][2:]\n",
    "                \n",
    "            start_idx = context.lower().index(split_main_entity[0])\n",
    "            end_idx = start_idx + (len(main_entity) if main_entity[0] != '#' else len(main_entity)-2)\n",
    "            text = context[:start_idx] + '[MASK]' + context[end_idx:]\n",
    "        else:\n",
    "            entity_idx = context.lower().index(main_entity)\n",
    "            text = context[:entity_idx] + '[MASK]' + context[entity_idx+len(main_entity):]\n",
    "\n",
    "        # encode context and extract relevant indexes\n",
    "        encoded_input = tokenizer(text, return_tensors='pt') \n",
    "        cls_idx = 0 # it's always the first token\n",
    "        mask_idx = encoded_input.input_ids[0].tolist().index(103)\n",
    "\n",
    "        # get the relevant embeddings and add them to the dictionary\n",
    "        output = model(**encoded_input)\n",
    "        \n",
    "        if main_entity in embeddings_BERT:\n",
    "            embeddings_BERT[main_entity]['CLS'].append(output[0][0][cls_idx, :].tolist())\n",
    "            embeddings_BERT[main_entity]['MASK'].append(output[0][0][mask_idx, :].tolist())\n",
    "            \n",
    "            # embeddings for all other tokens, expect final [SEP] token (hence the -1)\n",
    "            embeddings_BERT[main_entity]['context'].append(torch.mean(torch.cat((output[0][0][1:mask_idx, :],\n",
    "                                                                      output[0][0][mask_idx+1:-1, :])), 0).tolist())\n",
    "        else:\n",
    "            embeddings_BERT[main_entity] = {'CLS': [output[0][0][cls_idx, :].tolist()],\n",
    "                                            'MASK': [output[0][0][mask_idx, :].tolist()],\n",
    "                                            'context': [torch.mean(torch.cat((output[0][0][1:mask_idx, :],\n",
    "                                                                   output[0][0][mask_idx+1:-1, :])), 0).tolist()]}\n",
    "              \n",
    "    # save embeddings to disk and then return them\n",
    "    with open(embeddings_path, 'w+') as f:\n",
    "        json.dump(embeddings_BERT, f)\n",
    "    return embeddings_BERT\n",
    "\n",
    "def get_averaged_BERT_embeddings(gutenberg_id, context_window_size=7, bert_model='bert-large-cased', \n",
    "                                 grouped_entities=False):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each occurence of\n",
    "    each entity three embeddings: the mean [CLS] vector, the mean [MASK] vector and the mean \n",
    "    of the mean context embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    bert_model : str, optional\n",
    "        The underlying pre-trained BERT model to use (default is 'bert-large-cased')\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, three average embeddings, respetively the mean of all \n",
    "        their [MASK] embeddings, of all their [CLS] embeddings, and of all their 'context average' embeddings\n",
    "    '''\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = get_all_BERT_embeddings(gutenberg_id, context_window_size, bert_model, grouped_entities)\n",
    "    \n",
    "    # average all the embeddings\n",
    "    avg_BERT_embs = {}\n",
    "    entities = embeddings_BERT.keys()\n",
    "    \n",
    "    for e in entities:\n",
    "        avg_BERT_embs[e] = {'CLS': np.array(embeddings_BERT[e]['CLS']).mean(axis=0),\n",
    "                            'MASK': np.array(embeddings_BERT[e]['MASK']).mean(axis=0),\n",
    "                            'context': np.array(embeddings_BERT[e]['context']).mean(axis=0)}\n",
    "    \n",
    "    return avg_BERT_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-attribute",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:15.294178Z",
     "start_time": "2021-06-11T13:17:15.283971Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_averaged_BERT_embeddings_2D(gutenberg_id, avg_BERT_embeddings, emb_type='MASK', \n",
    "                                     title_addition='', max_vectors=None, min_count=5, \n",
    "                                     grouped_entities=False):\n",
    "    '''Given a book ID and its average BERT embeddings (and optionally several other settings, \n",
    "    including which of the three embedding type to plot ['CLS', 'MASK' or 'context']), uses PCA \n",
    "    to reduce the embeddings to 2D and then plots them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    avg_BERT_embeddings : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vectors, for the three\n",
    "        possible types (MASK, CLS and context) \n",
    "    emb_type : str, optional\n",
    "        The type of embedding to plot, from the three possible choices (MASK, CLS and context) \n",
    "        (default is 'MASK')\n",
    "    title_addition : str, optional\n",
    "        The text to add to the title (containing for example the book name) (default is '')\n",
    "    max_vectors : int, optional\n",
    "        The maximum number of entities to display in the plot. If not None, the max_vector most common\n",
    "        entities will be plotted (default is None)\n",
    "    min_count : int, optional\n",
    "        The minimum amount of times any entity needs to appear in the books in order for it to be\n",
    "        plotted (default is 5)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "    '''\n",
    "    \n",
    "    # get entities and apply min_count\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    tmp_df = book_df.groupby(['full_word']).count().reset_index()\n",
    "    tmp_df = tmp_df[tmp_df['full_word'].str.isalpha()]\n",
    "    tmp_df = tmp_df[tmp_df['score'] >= min_count][['full_word', 'score']]\n",
    "    \n",
    "    # apply max_vectors if not None\n",
    "    key_list = (tmp_df.sort_values(by='score', ascending=False).reset_index())\n",
    "    if max_vectors:\n",
    "        key_list = key_list[:max_vectors]\n",
    "    key_list = key_list.sort_values(by='full_word')['full_word'].unique()\n",
    "     \n",
    "    # filter embeddings according to their chosen embedding type and min/max counts\n",
    "    if emb_type != 'CLS' and emb_type != 'context':\n",
    "        emb_type = 'MASK'\n",
    "    vec_to_plot = {key: avg_BERT_embeddings[key][emb_type] for key in key_list if key in avg_BERT_embeddings}\n",
    "        \n",
    "    # apply PCA\n",
    "    sg_df = pd.DataFrame(vec_to_plot).T\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(sg_df)\n",
    "    \n",
    "    # plot the vectors\n",
    "    fig = px.scatter(components, x=0, y=1, color=sg_df.index, \n",
    "                     title=f'{emb_type} BERT embeddings {title_addition}',\n",
    "                     color_discrete_sequence=px.colors.qualitative.Alphabet)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-protest",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test (using `bert-large-cased` and David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-aviation",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "plot_averaged_BERT_embeddings_2D('766', avg_BERT_embeddings, emb_type='MASK', title_addition='- David Copperfield',\n",
    "                                max_vectors=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-rental",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_averaged_BERT_embeddings_2D('766', avg_BERT_embeddings, emb_type='CLS', title_addition='- David Copperfield',\n",
    "                                max_vectors=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-imagination",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_averaged_BERT_embeddings_2D('766', avg_BERT_embeddings, emb_type='context', title_addition='- David Copperfield',\n",
    "                                max_vectors=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-corporation",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test (using `bert-large-uncased` and David Copperfield, PG #766)\n",
    "\n",
    "Overall very similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-harbor",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_uncased_BERT_embeddings = get_averaged_BERT_embeddings('766', bert_model='bert-large-uncased')\n",
    "plot_averaged_BERT_embeddings_2D('766', avg_uncased_BERT_embeddings, emb_type='MASK', \n",
    "                                 title_addition='- David Copperfield - Uncased model', \n",
    "                                 max_vectors=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-wallpaper",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test (using `grouped_entities` and David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-sweet",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_ge_BERT_embeddings = get_averaged_BERT_embeddings('766', grouped_entities=True)\n",
    "plot_averaged_BERT_embeddings_2D('766', avg_ge_BERT_embeddings, emb_type='MASK', \n",
    "                                 title_addition='- David Copperfield - Grouped entities', \n",
    "                                 max_vectors=25, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-martial",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-rotation",
   "metadata": {},
   "source": [
    "### Fine-tuned BERT embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-territory",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Fine-tune the (BERT) model\n",
    "\n",
    "The following cells fine-tune the BERT Large Cased pre-trained model on a book. Indeed, the idea behind the fine-tuning is exactly to conduct it for the book which will later be analyzed using the later generated embeddings. To achieve this, a dummy task is used -- for this training, the task was identifying whether, given a random sentence of the book, it contained at least one (NER-identified) character in it.\n",
    "\n",
    "The fine-tuning below is conducted for Charles Dicken's *David Copperfield* book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-allen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:02.387969Z",
     "start_time": "2021-05-12T06:21:45.153028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_df = get_book_df('766')\n",
    "book_text = get_book_text('766')\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentence_level_book]\n",
    "labels = [1 if any([n in s for n in book_df['full_word'].unique()]) else 0 for s in sentences]\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "input_ids=[]\n",
    "for i in tqdm(range(len(tokenized_texts))):\n",
    "    input_ids.append(tokenizer.convert_tokens_to_ids(tokenized_texts[i]))\n",
    "\n",
    "MAX_LEN = 512\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-bulgaria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:04.874423Z",
     "start_time": "2021-05-12T06:22:02.389916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate the attention masks\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# creating the training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,random_state=56, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=56, test_size=0.2)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# create an iterator of both our training and validation data with torch DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-rough",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T06:22:20.691328Z",
     "start_time": "2021-05-12T06:22:04.876204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = BertConfig.from_pretrained('bert-large-cased', output_hidden_states=True, num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-large-cased', config=config)\n",
    "model.to(device) \n",
    "\n",
    "lr = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "num_total_steps = 1000\n",
    "num_warmup_steps = 100\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
    "\n",
    "# to reproduce BertAdam specific behavior set correct_bias=False\n",
    "optimizer = AdamW(model.parameters(), lr=lr)  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_total_steps)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-compatibility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:16:35.927818Z",
     "start_time": "2021-05-12T06:22:20.693814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_step = len(train_dataloader)\n",
    "train_loss_set = []\n",
    "epochs = 2\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        train_loss_set.append(loss.item())    \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-floating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:16:40.534048Z",
     "start_time": "2021-05-12T08:16:35.929743Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model_finetuning/BERT/766/checkpoints/finetuned_bert-large-cased.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835a5d7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test the fine-tuned BERT model to check the fine-tuning was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-morris",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T08:20:29.631700Z",
     "start_time": "2021-05-12T08:16:40.537453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        prediction = torch.argmax(outputs[0],dim=1)\n",
    "        total += b_labels.size(0)\n",
    "        correct+=(prediction==b_labels).sum().item()\n",
    "        \n",
    "print('Test Accuracy of the model on val data is: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-implementation",
   "metadata": {},
   "source": [
    "#### Use the finetuned model to generate the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5952b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:17.452899Z",
     "start_time": "2021-06-11T13:17:17.430398Z"
    },
    "code_folding": [
     0,
     93
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_finetuned_BERT_embeddings(gutenberg_id, context_window_size=7, grouped_entities=False):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each occurence of\n",
    "    each entity (in a context of the specified size), all the [CLS], [MASK] and mean context embeddings,\n",
    "    using the fine-tuned BERT model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, the lists of all their [MASK], [CLS], and \n",
    "        'context average' embeddings\n",
    "    '''\n",
    "    \n",
    "    # check if df already exists on the disk\n",
    "    embeddings_path = f'notebooks_data/finetuned_BERT_embeddings/{gutenberg_id}_{context_window_size}.json'\n",
    "    if grouped_entities:\n",
    "        embeddings_path = embeddings_path[:-5] + '_GE.json'\n",
    "    if os.path.isfile(embeddings_path):\n",
    "        with open(embeddings_path) as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    # get book df, all its entities and all their contexts\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities)\n",
    "    entities = book_df.drop_duplicates('total_word_index')['full_word'].to_list()\n",
    "    contexts = get_all_name_windows(book_df.drop_duplicates('total_word_index')['total_word_index'].to_list(), \n",
    "                         gutenberg_id, window_size=context_window_size)\n",
    "\n",
    "    # load BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased') \n",
    "    m = torch.load(f'model_finetuning/BERT/{gutenberg_id}/checkpoints/finetuned_bert-large-cased.pt')\n",
    "    m.eval()\n",
    "    m.to(device)\n",
    "\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = {}\n",
    "    for i, zipped in enumerate(tqdm(zip(contexts, entities), total=len(entities))):\n",
    "        \n",
    "        # prepare context for embedding\n",
    "        context, main_entity = zipped\n",
    "        text = ''\n",
    "        if grouped_entities:\n",
    "            split_main_entity = main_entity.split()\n",
    "            if split_main_entity[0][0] == '#':\n",
    "                split_main_entity[0] = split_main_entity[0][2:]\n",
    "                \n",
    "            start_idx = context.lower().index(split_main_entity[0])\n",
    "            end_idx = start_idx + (len(main_entity) if main_entity[0] != '#' else len(main_entity)-2)\n",
    "            text = context[:start_idx] + '[MASK]' + context[end_idx:]\n",
    "        else:\n",
    "            entity_idx = context.lower().index(main_entity)\n",
    "            text = context[:entity_idx] + '[MASK]' + context[entity_idx+len(main_entity):]\n",
    "\n",
    "        # encode context and extract relevant indexes\n",
    "        encoded_input = tokenizer(text, return_tensors='pt') \n",
    "        encoded_input.to(device) \n",
    "        cls_idx = 0 # it's always the first token\n",
    "        mask_idx = encoded_input.input_ids[0].tolist().index(103)\n",
    "\n",
    "        # get the relevant embeddings and add them to the dictionary\n",
    "        hidden_states = np.array([t.cpu().detach().numpy()[0] for t in m(**encoded_input).hidden_states])\n",
    "        embeddings_output = hidden_states[-1] # use the output of the last layer in the encoder\n",
    "        \n",
    "        if main_entity in embeddings_BERT:\n",
    "            embeddings_BERT[main_entity]['CLS'].append(embeddings_output[cls_idx, :].tolist())\n",
    "            embeddings_BERT[main_entity]['MASK'].append(embeddings_output[mask_idx, :].tolist())\n",
    "            \n",
    "            # embeddings for all other tokens, expect final [SEP] token (hence the -1)\n",
    "            embeddings_BERT[main_entity]['context'].append(np.mean(np.concatenate((embeddings_output[1:mask_idx, :],\n",
    "                                                                      embeddings_output[mask_idx+1:-1, :]), axis=0), \n",
    "                                                                axis=0).tolist())\n",
    "        else:\n",
    "            embeddings_BERT[main_entity] = {'CLS': [embeddings_output[cls_idx, :].tolist()],\n",
    "                                            'MASK': [embeddings_output[mask_idx, :].tolist()],\n",
    "                                            'context': [np.mean(np.concatenate((embeddings_output[1:mask_idx, :],\n",
    "                                                                   embeddings_output[mask_idx+1:-1, :]), axis=0), \n",
    "                                                                axis=0).tolist()]}\n",
    "              \n",
    "    # save embeddings to disk and then return them\n",
    "    with open(embeddings_path, 'w+') as f:\n",
    "        json.dump(embeddings_BERT, f)\n",
    "    return embeddings_BERT\n",
    "\n",
    "def get_averaged_finetuned_BERT_embeddings(gutenberg_id, context_window_size=7, grouped_entities=False):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each occurence of\n",
    "    each entity three embeddings: the mean [CLS] vector, the mean [MASK] vector and the mean \n",
    "    of the mean context embeddings, using the fine-tuned BERT model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, three average embeddings, respetively the mean of all \n",
    "        their [MASK] embeddings, of all their [CLS] embeddings, and of all their 'context average' embeddings\n",
    "    '''\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = get_all_finetuned_BERT_embeddings(gutenberg_id, context_window_size, grouped_entities)\n",
    "    \n",
    "    # average all the embeddings\n",
    "    avg_BERT_embs = {}\n",
    "    entities = embeddings_BERT.keys()\n",
    "    \n",
    "    for e in entities:\n",
    "        avg_BERT_embs[e] = {'CLS': np.array(embeddings_BERT[e]['CLS']).mean(axis=0),\n",
    "                            'MASK': np.array(embeddings_BERT[e]['MASK']).mean(axis=0),\n",
    "                            'context': np.array(embeddings_BERT[e]['context']).mean(axis=0)}\n",
    "    \n",
    "    return avg_BERT_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b4f14",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Test (using David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8445b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:18:17.796740Z",
     "start_time": "2021-05-31T07:18:03.830034Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_finetuned_BERT_embeddings = get_averaged_finetuned_BERT_embeddings('766')\n",
    "plot_averaged_BERT_embeddings_2D('766', avg_finetuned_BERT_embeddings, emb_type='MASK', \n",
    "                                 title_addition='- David Copperfield [FINETUNED]',\n",
    "                                 max_vectors=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0006ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:18:22.613010Z",
     "start_time": "2021-05-31T07:18:22.360969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_averaged_BERT_embeddings_2D('766', avg_finetuned_BERT_embeddings, emb_type='CLS', \n",
    "                                 title_addition='- David Copperfield [FINETUNED]',\n",
    "                                 max_vectors=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c9bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:18:28.833059Z",
     "start_time": "2021-05-31T07:18:28.586444Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_averaged_BERT_embeddings_2D('766', avg_finetuned_BERT_embeddings, emb_type='context', \n",
    "                                 title_addition='- David Copperfield [FINETUNED]',\n",
    "                                 max_vectors=25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-independence",
   "metadata": {},
   "source": [
    "### Weighted, Unweighted and Mixed (Weighted) BERT embeddings\n",
    "\n",
    "This section average closest relations' embeddings to create new character vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-charleston",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:18.520185Z",
     "start_time": "2021-06-11T13:17:18.507742Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "precomputed_embeddings = {}\n",
    "\n",
    "def plot_dict_embeddings_2D(gutenberg_id, embeddings, title='Embeddings', max_vectors=None, \n",
    "                              min_count=5, grouped_entities=False):\n",
    "    '''Given a book ID and a dictionary of BERT embeddings, uses PCA to reduce the embeddings \n",
    "    to 2D and then plots them. Differs from `plot_averaged_BERT_embeddings_2D` in that the embeddings\n",
    "    passed should be have a single embedding vector value for each entity, and not three.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    embeddings : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vector\n",
    "    title : str, optional\n",
    "        The plot's title (default is 'Embeddings')\n",
    "    max_vectors : int, optional\n",
    "        The maximum number of entities to display in the plot. If not None, the max_vector most common\n",
    "        entities will be plotted (default is None)\n",
    "    min_count : int, optional\n",
    "        The minimum amount of times any entity needs to appear in the books in order for it to be\n",
    "        plotted (default is 5)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "    '''\n",
    "    \n",
    "    # get entities and apply min_count\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    tmp_df = book_df.groupby(['full_word']).count().reset_index()\n",
    "    tmp_df = tmp_df[tmp_df['full_word'].str.isalpha()]\n",
    "    tmp_df = tmp_df[tmp_df['score'] >= min_count][['full_word', 'score']]\n",
    "    \n",
    "    # apply max_vectors if not None\n",
    "    key_list = (tmp_df.sort_values(by='score', ascending=False).reset_index())\n",
    "    if max_vectors:\n",
    "        key_list = key_list[:max_vectors]\n",
    "    key_list = key_list.sort_values(by='full_word')['full_word'].unique()\n",
    "     \n",
    "    # filter embeddings according to their chosen embedding type and min/max counts\n",
    "    vec_to_plot = {key: embeddings[key] for key in key_list if key in embeddings}\n",
    "        \n",
    "    # apply PCA\n",
    "    sg_df = pd.DataFrame(vec_to_plot).T\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(sg_df)\n",
    "    \n",
    "    # plot the vectors\n",
    "    fig = px.scatter(components, x=0, y=1, color=sg_df.index, title=f'{title}',\n",
    "                     color_discrete_sequence=px.colors.qualitative.Alphabet)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-prior",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-australian",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Non-weigthed sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-exhibit",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get non-weighted embeddings for all results for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-constitutional",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "near_entities_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            char_ents_embeddings.append(avg_BERT_embeddings[c]['MASK'])\n",
    "        else:\n",
    "            # misclassified entity words (i.e. 'affectionately') end up here; we could use regular\n",
    "            # embeddings to include them in the characters averages, but it wouldn't make much \n",
    "            # sense, imo, when we're trying to extract inter-character relationships\n",
    "            pass \n",
    "    \n",
    "    near_entities_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-graham",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', near_entities_embeddings, title='Non-weighted (BERT) embeddings for all entities [MASK]',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-toronto",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get non-weighted embeddings for top N(=10) results for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-shirt",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "near_entities_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = (pd.DataFrame(nr[character])[['near_entities']].dropna()\n",
    "                 .sort_values(by='near_entities', ascending=False).index)\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    actual_entities_considered = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if actual_entities_considered >= 10:\n",
    "            break\n",
    "            \n",
    "        if c in avg_BERT_embeddings:\n",
    "            actual_entities_considered += 1\n",
    "            char_ents_embeddings.append(avg_BERT_embeddings[c]['MASK'])\n",
    "        else:\n",
    "            # misclassified entity words (i.e. 'affectionately') end up here; we could use regular\n",
    "            # embeddings to include them in the characters averages, but it wouldn't make much \n",
    "            # sense, imo, when we're trying to extract inter-character relationships\n",
    "            pass \n",
    "    \n",
    "    near_entities_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-administration",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', near_entities_embeddings, \n",
    "                        title='Non-weighted (BERT) embeddings for nearest 10 entities [MASK]',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-timothy",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get non-weighted embeddings for all results for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-picture",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "verb_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    # for each character, get their verbs\n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna().index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    # get each verb embedding, and then average them\n",
    "    for i, v in enumerate(char_verbs):\n",
    "        if v not in precomputed_embeddings:\n",
    "            encoded_input = tokenizer(v, return_tensors='pt') \n",
    "            output = model(**encoded_input)\n",
    "            precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "        char_verbs_embeddings.append(precomputed_embeddings[v])\n",
    "    \n",
    "    verb_embeddings[character] = np.sum(np.array(char_verbs_embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-detective",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', verb_embeddings, title='Non-weighted (BERT) embeddings for all verbs',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-walker",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get non-weighted embeddings for all post N(=10) results for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-oakland",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "verb_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna().index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    for i, v in enumerate(char_verbs):\n",
    "        if i < 10:\n",
    "            continue\n",
    "        if v not in precomputed_embeddings:\n",
    "            encoded_input = tokenizer(v, return_tensors='pt') \n",
    "            output = model(**encoded_input)\n",
    "            precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "        char_verbs_embeddings.append(precomputed_embeddings[v])\n",
    "    \n",
    "    verb_embeddings[character] = np.sum(np.array(char_verbs_embeddings), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-principal",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', verb_embeddings, title='Non-weighted (BERT) embeddings for less common verbs',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-devil",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Weighted sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-delta",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get weighted embeddings for all results for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-opportunity",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "near_entities_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "        else:\n",
    "            # misclassified entity words (i.e. 'affectionately') end up here; we could use regular\n",
    "            # embeddings to include them in the characters averages, but it wouldn't make much \n",
    "            # sense, imo, when we're trying to extract inter-character relationships\n",
    "            pass \n",
    "               \n",
    "    if total > 0:\n",
    "        near_entities_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        near_entities_embeddings[character] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-terrain",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', near_entities_embeddings, title='Weighted (BERT) embeddings for all entities [MASK]',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-aruba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', near_entities_embeddings, title='Weighted (BERT) embeddings for all entities [MASK]',\n",
    "                       max_vectors=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-defendant",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get weighted embeddings for top N(=10) results for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-kansas",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "near_entities_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = (pd.DataFrame(nr[character])[['near_entities']].dropna()\n",
    "                 .sort_values(by='near_entities', ascending=False).index)\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    actual_entities_considered = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if actual_entities_considered >= 10:\n",
    "            break\n",
    "            \n",
    "        if c in avg_BERT_embeddings:\n",
    "            actual_entities_considered += 1\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "        else:\n",
    "            # misclassified entity words (i.e. 'affectionately') end up here; we could use regular\n",
    "            # embeddings to include them in the characters averages, but it wouldn't make much \n",
    "            # sense, imo, when we're trying to extract inter-character relationships\n",
    "            pass \n",
    "               \n",
    "    if total > 0:\n",
    "        near_entities_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        near_entities_embeddings[character] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-bidding",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', near_entities_embeddings, \n",
    "                        title='Weighted (BERT) embeddings for nearest 10 entities [MASK]',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-roulette",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get weighted embeddings for all results for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-command",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "verb_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 0:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if v not in precomputed_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                output = model(**encoded_input)\n",
    "                precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "        verb_embeddings[character] = np.sum(np.array(char_verbs_embeddings), axis=0) / total\n",
    "    else:\n",
    "        verb_embeddings[character] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-socket",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', verb_embeddings, title='Weighted (BERT) embeddings for all verbs',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-system",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get weighted embeddings for all post N(=10) results for verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-marker",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "verb_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 10:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if i < 10:\n",
    "                continue\n",
    "            if v not in precomputed_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                output = model(**encoded_input)\n",
    "                precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "        verb_embeddings[character] = np.sum(np.array(char_verbs_embeddings), axis=0) / total\n",
    "    else:\n",
    "        verb_embeddings[character] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-extra",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', verb_embeddings, title='Weighted (BERT) embeddings for less common verbs',\n",
    "                       max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-gasoline",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-responsibility",
   "metadata": {},
   "source": [
    "#### Mixed weighted sums\n",
    "\n",
    "The mixed embeddings generated in this section make use of both the embedding values of each entity's nearest entities and verbs to generate the new embeddings values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-aspect",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Weighted sum of ALL entitires and POST N(=10) verbs (CONCATENATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-brain",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "mixed_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "               \n",
    "    if total > 0:\n",
    "        mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.zeros((1024,))\n",
    "         \n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 10:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if i < 10:\n",
    "                continue\n",
    "            if v not in precomputed_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                output = model(**encoded_input)\n",
    "                precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "        mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], np.sum(np.array(char_verbs_embeddings), axis=0) / total), axis=0)\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], np.zeros((1024,))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-outreach",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), concatenated',\n",
    "                        max_vectors=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-comment",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), concatenated',\n",
    "                        max_vectors=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-conversion",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Weighted sum of ALL entitires and POST N(=10) verbs (SUMMED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-reflection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T18:57:03.868060Z",
     "start_time": "2021-06-09T18:53:39.220820Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "bert_model = 'bert-large-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "mixed_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "               \n",
    "    if total > 0:\n",
    "        mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.zeros((1024,))\n",
    "         \n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 10:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if i < 10:\n",
    "                continue\n",
    "            if v not in precomputed_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                output = model(**encoded_input)\n",
    "                precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "        mixed_embeddings[character] = mixed_embeddings[character] + (np.sum(np.array(char_verbs_embeddings), axis=0) / total)\n",
    "    else:\n",
    "        mixed_embeddings[character] = mixed_embeddings[character] + np.zeros((1024,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-short",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T19:02:07.286341Z",
     "start_time": "2021-06-09T19:02:07.038133Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), summed',\n",
    "                        max_vectors=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-electron",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T14:00:21.036872Z",
     "start_time": "2021-06-08T14:00:20.803971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), summed',\n",
    "                        max_vectors=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-checklist",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f66dca6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Weighted sum of ALL entitires and POST N(=10) verbs (CONCATENATED) - FINETUNED VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd940fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:18:41.601107Z",
     "start_time": "2021-05-31T07:18:40.737247Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gutenberg_id = 766\n",
    "precomputed_finetuned_embeddings = {}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16eefca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:20:35.452310Z",
     "start_time": "2021-05-31T07:18:43.820124Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_finetuned_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased') \n",
    "m = torch.load(f'model_finetuning/BERT/{gutenberg_id}/checkpoints/finetuned_bert-large-cased.pt')\n",
    "m.eval()\n",
    "m.to(device)\n",
    "\n",
    "mixed_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "               \n",
    "    if total > 0:\n",
    "        mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.zeros((1024,))\n",
    "         \n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 10:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if i < 10:\n",
    "                continue\n",
    "            if v not in precomputed_finetuned_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                encoded_input.to(device) \n",
    "                hidden_states = np.array([t.cpu().detach().numpy()[0] for t in m(**encoded_input).hidden_states])\n",
    "                output = hidden_states[-1]\n",
    "                precomputed_finetuned_embeddings[v] = np.array(output[0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_finetuned_embeddings[v])\n",
    "\n",
    "        mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], np.sum(\n",
    "            np.array(char_verbs_embeddings), axis=0) / total), axis=0)\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], np.zeros((1024,))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e8bd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:20:35.697516Z",
     "start_time": "2021-05-31T07:20:35.454644Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), concatenated',\n",
    "                        max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6646ab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Weighted sum of ALL entitires and POST N(=10) verbs (SUMMED) - FINETUNED VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2422e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gutenberg_id = 766\n",
    "precomputed_finetuned_embeddings = {}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d116836",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:21:21.709053Z",
     "start_time": "2021-05-31T07:20:35.699243Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_finetuned_BERT_embeddings('766')\n",
    "nr = get_nearest_relations('766')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased') \n",
    "m = torch.load(f'model_finetuning/BERT/{gutenberg_id}/checkpoints/finetuned_bert-large-cased.pt')\n",
    "m.eval()\n",
    "m.to(device)\n",
    "\n",
    "mixed_embeddings = {}\n",
    "for character in tqdm(nr):\n",
    "    char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "    char_ents_embeddings = []\n",
    "    \n",
    "    total = 0\n",
    "    for i, c in enumerate(char_ents):\n",
    "        if c in avg_BERT_embeddings:\n",
    "            weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "            total = total + weight\n",
    "            char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "               \n",
    "    if total > 0:\n",
    "        mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "    else:\n",
    "        mixed_embeddings[character] = np.zeros((1024,))\n",
    "         \n",
    "    char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "    char_verbs_embeddings = []\n",
    "    \n",
    "    if len(char_verbs) > 10:\n",
    "        total = 0\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            if i < 10:\n",
    "                continue\n",
    "            if v not in precomputed_finetuned_embeddings:\n",
    "                encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                encoded_input.to(device) \n",
    "                hidden_states = np.array([t.cpu().detach().numpy()[0] for t in m(**encoded_input).hidden_states])\n",
    "                output = hidden_states[-1]\n",
    "                precomputed_finetuned_embeddings[v] = np.array(output[0, :].tolist())\n",
    "            weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            total = total + weight\n",
    "            char_verbs_embeddings.append(weight * precomputed_finetuned_embeddings[v])\n",
    "\n",
    "        mixed_embeddings[character] = mixed_embeddings[character] + (np.sum(np.array(char_verbs_embeddings), \n",
    "                                                                            axis=0) / total)\n",
    "    else:\n",
    "        mixed_embeddings[character] = mixed_embeddings[character] + np.zeros((1024,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4af65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T07:21:21.948868Z",
     "start_time": "2021-05-31T07:21:21.711309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_dict_embeddings_2D('766', mixed_embeddings, \n",
    "                        title='Mixed weighted (BERT) embeddings (all entities [MASK] and less common verbs), summed',\n",
    "                        max_vectors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245fcdc",
   "metadata": {},
   "source": [
    "##### Create two function to easily generate mixed BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae994b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:24.792354Z",
     "start_time": "2021-06-11T13:17:24.767180Z"
    },
    "code_folding": [
     0,
     80
    ]
   },
   "outputs": [],
   "source": [
    "def get_mixed_avg_BERT_embs(gutenberg_id, mode='concat'):\n",
    "    '''Given the book ID (and mixing mode for the weighted sum), returns, for each occurence of\n",
    "    each entity, its mixed embeddings, using the non-fine-tuned BERT model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    mode : str, optional\n",
    "        The type of merge (either 'concat' or 'sum') that should be done between the original \n",
    "        BERT embeddings and the ones generated via the weigthed sum of near entities and verbs \n",
    "        (default is 'concat')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, its mixed embeddings\n",
    "    '''\n",
    "    \n",
    "    avg_BERT_embeddings = get_averaged_BERT_embeddings(gutenberg_id)\n",
    "    nr = get_nearest_relations(gutenberg_id)\n",
    "\n",
    "    bert_model = 'bert-large-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "    mixed_embeddings = {}\n",
    "\n",
    "    for character in tqdm(nr):\n",
    "        char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "        char_ents_embeddings = []\n",
    "\n",
    "        total = 0\n",
    "        for i, c in enumerate(char_ents):\n",
    "            if c in avg_BERT_embeddings:\n",
    "                weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "                total = total + weight\n",
    "                char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "\n",
    "        if total > 0:\n",
    "            mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "        else:\n",
    "            mixed_embeddings[character] = np.zeros((1024,))\n",
    "\n",
    "        char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "        char_verbs_embeddings = []\n",
    "\n",
    "        if len(char_verbs) > 10:\n",
    "            total = 0\n",
    "            for i, v in enumerate(char_verbs):\n",
    "                if i < 10:\n",
    "                    continue\n",
    "                if v not in precomputed_embeddings:\n",
    "                    encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                    output = model(**encoded_input)\n",
    "                    precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "                weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "                total = total + weight\n",
    "                char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "    \n",
    "            if mode == 'concat':\n",
    "                mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], \n",
    "                                                              np.sum(np.array(char_verbs_embeddings), \n",
    "                                                                     axis=0) / total), axis=0)\n",
    "            elif mode == 'sum':\n",
    "                mixed_embeddings[character] = (mixed_embeddings[character] + \n",
    "                                               (np.sum(np.array(char_verbs_embeddings), axis=0) / total))\n",
    "        else:\n",
    "            if mode == 'concat':\n",
    "                mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], \n",
    "                                                              np.zeros((1024,))), axis=0)\n",
    "            elif mode == 'sum':\n",
    "                mixed_embeddings[character] = mixed_embeddings[character] + np.zeros((1024,))\n",
    "        \n",
    "    return mixed_embeddings\n",
    "        \n",
    "def get_mixed_avg_finetuned_BERT_embs(gutenberg_id, mode='concat'):\n",
    "    '''Given the book ID (and mixing mode for the weighted sum), returns, for each occurence of\n",
    "    each entity, its mixed embeddings, using the fine-tuned BERT model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    mode : str, optional\n",
    "        The type of merge (either 'concat' or 'sum') that should be done between the original \n",
    "        BERT embeddings and the ones generated via the weigthed sum of near entities and verbs \n",
    "        (default is 'concat')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, its mixed embeddings\n",
    "    '''\n",
    "    \n",
    "    avg_BERT_embeddings = get_averaged_finetuned_BERT_embeddings(gutenberg_id)\n",
    "    nr = get_nearest_relations(gutenberg_id)\n",
    "\n",
    "    bert_model = 'bert-large-cased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "    mixed_embeddings = {}\n",
    "    for character in tqdm(nr):\n",
    "        char_ents = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "        char_ents_embeddings = []\n",
    "\n",
    "        total = 0\n",
    "        for i, c in enumerate(char_ents):\n",
    "            if c in avg_BERT_embeddings:\n",
    "                weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "                total = total + weight\n",
    "                char_ents_embeddings.append(weight * avg_BERT_embeddings[c]['MASK'])\n",
    "\n",
    "        if total > 0:\n",
    "            mixed_embeddings[character] = np.sum(np.array(char_ents_embeddings), axis=0) / total\n",
    "        else:\n",
    "            mixed_embeddings[character] = np.zeros((1024,))\n",
    "\n",
    "        char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "        char_verbs_embeddings = []\n",
    "\n",
    "        if len(char_verbs) > 10:\n",
    "            total = 0\n",
    "            for i, v in enumerate(char_verbs):\n",
    "                if i < 10:\n",
    "                    continue\n",
    "                if v not in precomputed_embeddings:\n",
    "                    encoded_input = tokenizer(v, return_tensors='pt') \n",
    "                    output = model(**encoded_input)\n",
    "                    precomputed_embeddings[v] = np.array(output[0][0][0, :].tolist())\n",
    "                weight = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "                total = total + weight\n",
    "                char_verbs_embeddings.append(weight * precomputed_embeddings[v])\n",
    "\n",
    "            if mode == 'concat':\n",
    "                mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], \n",
    "                                                              np.sum(np.array(char_verbs_embeddings), \n",
    "                                                                     axis=0) / total), axis=0)\n",
    "            elif mode == 'sum':\n",
    "                mixed_embeddings[character] = (mixed_embeddings[character] + \n",
    "                                               (np.sum(np.array(char_verbs_embeddings), axis=0) / total))\n",
    "        else:\n",
    "            if mode == 'concat':\n",
    "                mixed_embeddings[character] = np.concatenate((mixed_embeddings[character], \n",
    "                                                              np.zeros((1024,))), axis=0)\n",
    "            elif mode == 'sum':\n",
    "                mixed_embeddings[character] = mixed_embeddings[character] + np.zeros((1024,))\n",
    "\n",
    "    return mixed_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-independence",
   "metadata": {},
   "source": [
    "### Create simpler \"embeddings\" based only on the nearest entities and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-force",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:27.222518Z",
     "start_time": "2021-06-11T13:17:27.212593Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_2D_embeddings_directly(gutenberg_id, embeddings, title='Embeddings', max_vectors=None, \n",
    "                                min_count=5, grouped_entities=False):\n",
    "    '''Given a book ID and 2D embeddings, plots them directly (no PCA dimensionality reduction \n",
    "    is applied).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    embeddings : dictionary\n",
    "        A dictionary containing each entity and their associated 2D-embeddings vector\n",
    "    title : str, optional\n",
    "        The plot's title (default is 'Embeddings')\n",
    "    max_vectors : int, optional\n",
    "        The maximum number of entities to display in the plot. If not None, the max_vector most common\n",
    "        entities will be plotted (default is None)\n",
    "    min_count : int, optional\n",
    "        The minimum amount of times any entity needs to appear in the books in order for it to be\n",
    "        plotted (default is 5)\n",
    "    grouped_entities : bool, optional\n",
    "        Flag indicating whether the NER pipeline used to create the entities was configured to output \n",
    "        grouped_entities or not (default is False)\n",
    "    '''\n",
    "    \n",
    "    # get entities and apply min_count\n",
    "    book_df = get_book_df(gutenberg_id, grouped_entities).drop_duplicates('total_word_index')\n",
    "    tmp_df = book_df.groupby(['full_word']).count().reset_index()\n",
    "    tmp_df = tmp_df[tmp_df['full_word'].str.isalpha()]\n",
    "    tmp_df = tmp_df[tmp_df['score'] >= min_count][['full_word', 'score']]\n",
    "    \n",
    "    # apply max_vectors if not None\n",
    "    key_list = (tmp_df.sort_values(by='score', ascending=False).reset_index())\n",
    "    if max_vectors:\n",
    "        key_list = key_list[:max_vectors]\n",
    "    key_list = key_list.sort_values(by='full_word')['full_word'].unique()\n",
    "     \n",
    "    # filter embeddings according to their chosen embedding type and min/max counts\n",
    "    vec_to_plot = {key: embeddings[key] for key in key_list if key in embeddings}\n",
    "    \n",
    "    # plot the vectors\n",
    "    tmp = pd.DataFrame([(key, vec_to_plot[key][0][0], vec_to_plot[key][1][0]) for key in vec_to_plot], \n",
    "                       columns=['Name', 'x', 'y'])\n",
    "    fig = px.scatter(tmp, x='x', y='y', color='Name', title=f'{title}',\n",
    "                     color_discrete_sequence=px.colors.qualitative.Alphabet)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-motorcycle",
   "metadata": {},
   "source": [
    "#### Entities\n",
    "\n",
    "These embeddings were created using a very different approach to all other embeddings presented before it. Each character reference was initialized with a random 2-dimensional vector, and were updated with a small fraction of the weighted average of the values of the embeddings of all other characters references which appeared near it (the \"nearest entities\") -- with their weight being given by the frequency with which that particular reference was a near neighbor of the original character reference -- until convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c80464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:31.643292Z",
     "start_time": "2021-06-11T13:17:31.627703Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_nearest_entities_based_embeddings(gutenberg_id):\n",
    "    '''Given the book DF, uses each entity's nearest neighbors to create 2D embeddings for each one,\n",
    "    through a convergence algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_entities : dictionary\n",
    "        A dictionary containing each entity and their associated 2D embedding vector\n",
    "    '''\n",
    "    \n",
    "    nr = get_nearest_relations(gutenberg_id)\n",
    "    \n",
    "    # get cached version if it exist, for efficiency\n",
    "    if os.path.isfile(f'notebooks_data/book_dfs/{gutenberg_id}_nearest_ent_embeddings.pk'): \n",
    "        with open(f'notebooks_data/book_dfs/{gutenberg_id}_nearest_ent_embeddings.pk', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    # otherwise create embeddings from scratch\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # for each entity, create a random vector of the given size\n",
    "    vector_dim = 2\n",
    "    all_entities = {}\n",
    "    for character in tqdm(nr):\n",
    "        if character not in all_entities:\n",
    "            all_entities[character] = np.random.rand(vector_dim, 1)\n",
    "\n",
    "    # iterate until converge: update each character according to ALL its nearest entities (and their frequency)\n",
    "    new_all_entities = {}\n",
    "    for character in tqdm(nr):\n",
    "        if character not in new_all_entities:\n",
    "            new_all_entities[character] = np.zeros((vector_dim, 1))\n",
    "\n",
    "    shift_size = 0.1 # should be a number between ]0, 1] (aka (0,1])\n",
    "    threshold = 0.05\n",
    "    while threshold < np.sum(np.abs(np.array([all_entities[k] for k in all_entities]) - \n",
    "                                    np.array([new_all_entities[k] for k in new_all_entities]))):\n",
    "\n",
    "        if np.sum(np.abs(np.array([new_all_entities[k] for k in new_all_entities]))) != 0: # don't copy it the first time\n",
    "            all_entities = new_all_entities.copy()\n",
    "\n",
    "        for character in nr:\n",
    "            related_entities = pd.DataFrame(nr[character])[['near_entities']].dropna().index\n",
    "\n",
    "            shift = np.zeros((vector_dim, 1))\n",
    "            total = 0\n",
    "            for i, c in enumerate(related_entities):\n",
    "                if c in all_entities:\n",
    "                    weight = pd.DataFrame(nr[character])[['near_entities']].dropna()['near_entities'][i]\n",
    "                    total = total + weight\n",
    "                    shift = shift + (weight * (all_entities[c] - all_entities[character]))\n",
    "\n",
    "            if total > 0:\n",
    "                new_all_entities[character] = all_entities[character] + shift_size * (shift / total)\n",
    "\n",
    "    with open(f'notebooks_data/book_dfs/{gutenberg_id}_nearest_ent_embeddings.pk', 'wb') as f:\n",
    "        pickle.dump(new_all_entities, f)\n",
    "                    \n",
    "    return all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-english",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T23:21:10.444895Z",
     "start_time": "2021-06-09T23:21:09.802759Z"
    }
   },
   "outputs": [],
   "source": [
    "target_book_id = 766\n",
    "new_all_entities = get_nearest_entities_based_embeddings(target_book_id)\n",
    "plot_2D_embeddings_directly(target_book_id, new_all_entities, title='Relational 2-dim \"embeddings\"', \n",
    "                            max_vectors=25, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-contamination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-09T23:21:10.643025Z",
     "start_time": "2021-06-09T23:21:10.446752Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_2D_embeddings_directly(target_book_id, new_all_entities, title='Relational 2-dim \"embeddings\"', \n",
    "                            max_vectors=50, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-newton",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-ocean",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Verbs\n",
    "\n",
    "These embeddings were created by leveraging each character reference's nearest verbs (the lemmatized action verbs that followed it within a 30-word window). After extracting the verbs for all character reference, each different verb is treated as an embedding dimension, with their value being the number of times that verb occurred near that character reference (or 0 if they never occurred shortly after that reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-wales",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:17:36.349390Z",
     "start_time": "2021-06-11T13:17:36.339184Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_nearest_verbs_based_embeddings(gutenberg_id):\n",
    "    '''Given the book DF, uses each entity's nearest verbs to create embeddings for each one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_entities : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vector\n",
    "    '''\n",
    "    \n",
    "    # get all verbs\n",
    "    nr = get_nearest_relations(gutenberg_id)\n",
    "\n",
    "    all_verbs_tuple = []\n",
    "    for character in tqdm(nr):\n",
    "        char_verbs = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'].index\n",
    "\n",
    "        for i, v in enumerate(char_verbs):\n",
    "            freq = pd.DataFrame(nr[character])[['near_verbs']].dropna()['near_verbs'][i]\n",
    "            all_verbs_tuple.append((character, v.lower(), freq))\n",
    "\n",
    "    # create the embeddings by using the verbs and their frequency as an embedding in itself\n",
    "    all_verbs_df = pd.DataFrame(all_verbs_tuple, columns=['Character', 'Verb', 'Frequency'])\n",
    "    all_verbs_df = (all_verbs_df\n",
    "                    .pivot_table(values=\"Frequency\", index=\"Character\", columns=\"Verb\", aggfunc=\"mean\")\n",
    "                    .fillna(0)\n",
    "                    .T)\n",
    "    # convert to a character -> vector format and return it\n",
    "    return {k: all_verbs_df[k].tolist() for k in all_verbs_df.to_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-donor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:18:31.365543Z",
     "start_time": "2021-06-11T13:18:01.462603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "verb_embs = get_nearest_verbs_based_embeddings(766)\n",
    "plot_dict_embeddings_2D(766, verb_embs, title='Verbs Naive \"Embeddings\"', \n",
    "                        max_vectors=25, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-patio",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-rapid",
   "metadata": {},
   "source": [
    "## Evalutation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-marble",
   "metadata": {},
   "source": [
    "These metrics need the underlying data to be created by hand. We will judge each approach on three different vectors: \n",
    "1. **same entity** cohesion (same entities (i.e. first and last name) should appear together)\n",
    "2. entities introduced in **similar circumstances and time** in the novel should reflect some similarity\n",
    "3. entities with **shared characteristics** should be grouped together\n",
    "    - gender\n",
    "    - occupation (similarity will rely on BERT embeddings of the professions)\n",
    "    - *(age and marital status not very adequate since they can easily change)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-comparative",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loading the ground truth dataset (for David Copperfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:56:00.828238Z",
     "start_time": "2021-06-06T17:56:00.795717Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ground_truth_data_df = pd.DataFrame([   \n",
    "('agnes', 'Agnes Wickfield', 8, 'F', 'daughter', 115327), \n",
    "('annie', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741), \n",
    "('barkis', 'Mr. Barkis', 10, 'M', 'carrier', 32305), \n",
    "('betsey', 'Betsey Trotwood', 1, 'F', 'caretaker', 927), \n",
    "('charles', 'Charles Mell', 11, 'M', 'teacher', 104678), \n",
    "('chillip', 'Mr. Chillip', 4, 'M', 'doctor', 4546), \n",
    "('clara', 'Clara Copperfield', 12, 'F', 'mother', 21475),\n",
    "('clara', 'Clara Peggotty', 49, 'F', 'servant', 24807), \n",
    "('clarissa', 'Clarissa Spenlow', 13, 'F', 'sister', 308525), \n",
    "('copperfield', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('copperfield', 'Clara Copperfield', 12, 'F', 'mother', 21475), \n",
    "('copperfull', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('creakle', 'Mr. Creakle', 14, 'M', 'headmaster', 39424), \n",
    "('crewler', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('crewler', 'Reverend Horace Crewler', 16, 'M', 'clergyman', 307170),\n",
    "('crewler', 'Mrs. Crewler', 17, 'F', 'wife', 307147),\n",
    "('crewler', 'Caroline Crewler', 18, 'F', 'daughter', 427626),\n",
    "('crewler', 'Sarah Creweler', 19, 'F', 'daughter', 256127),\n",
    "('crewler', 'Louisa Creweler', 20, 'F', 'daughter', 427668),\n",
    "('crewler', 'Lucy Creweler', 21, 'F', 'daughter', 428983),\n",
    "('crewler', 'Margaret Creweler', 22, 'F', 'daughter', 428981), \n",
    "('crupp', 'Mrs. Crupp', 23, 'F', 'landlady', 183694), \n",
    "('d', '', -1, '', '', -1), \n",
    "('daisy', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('dan', 'Daniel Peggotty', 5, 'M', 'fisherman', 14917), \n",
    "('dartle', 'Rosa Dartle', 24, 'F', 'companion', 151027), \n",
    "('david', 'David Copperfield', 2, 'M', 'writer', 1748), \n",
    "('davy', 'David Copperfield', 2, 'M', 'writer', 1748), \n",
    "('dick', 'Richard Babley', 25, 'M', 'writer', 99277), \n",
    "('doady', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('dolloby', 'Mr. Dolloby', 26, 'M', 'shop owner', 93122), \n",
    "('dora', 'Dora Spenlow', 27, 'F', 'wife', 201865), \n",
    "('em', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('emily', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('god', '', -1, '', '', -1), \n",
    "('gulpidge', 'Mr. Gulpidge', 28, 'M', 'banker', 193863), \n",
    "('gummidge', 'Mrs. Gummidge', 7, 'F', 'widow', 16396), \n",
    "('ham', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('heaven', '', -1, '', '', -1), \n",
    "('heep', 'Uriah Heep', 29, 'M', 'lawyer', 113168), \n",
    "('j', '', -1, '', '', -1), \n",
    "('jack', 'Jack Maldon', 30, 'M', 'unemployed', 116937), \n",
    "('james', 'James Steerforth', 31, 'M', 'aristocrat', 40315), \n",
    "('jane', 'Jane Murdstone', 32, 'F', 'sister', 24929), \n",
    "('janet', 'Janet', 33, 'F', 'servant', 99254), \n",
    "('jip', 'Jip', 34, 'M', 'dog', 203807), \n",
    "('joram', 'Joram', 35, 'M', 'carpenter', 65119), \n",
    "('jorkins', 'Mr. Jorkins', 36, 'M', 'businessman', 180559), \n",
    "('julia', 'Julia Mills', 37, 'F', 'daugther', 250095), \n",
    "('larkins', 'Miss Larkins', 38, 'F', 'wife', 138925), \n",
    "('lavinia', 'Miss Lavinia Spenlow', 39, 'F', 'sister', 308405), \n",
    "('littimer' 'Mr. Littimer', 40, 'M', 'valet', 154468), \n",
    "('little', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('ly', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('maldon', 'Jack Maldon', 30, 'M', 'unemployed', 116937), \n",
    "('markleham', 'Mrs. Markleham', 41, 'F', 'mother', 130353), \n",
    "('markleham', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741), \n",
    "('martha', 'Martha Endell', 42, 'F', 'young woman', 174661), \n",
    "('mell', 'Mr. Mell', 43, 'M', 'instructor', 39238), \n",
    "('mell', 'Charles Mell', 11, 'M', 'teacher', 104678), \n",
    "('micawber', 'Wilkins Micawber', 44, 'M', 'polymath', 80570), \n",
    "('micawber', 'Mrs. Emma Micawber', 45, 'F', 'wife', 88645), \n",
    "('mills', 'Julia Mills', 37, 'F', 'daugther', 250095),  \n",
    "('minnie', 'Minnie Omer', 46, 'F', 'daugther', 64233), \n",
    "('miss', '', -1, 'F', '', -1), \n",
    "('missis', 'Mrs. Gummidge', 7, 'F', 'widow', 16396), \n",
    "('mowcher', 'Miss Mowcher', 47, 'F', 'hairdresser', 168955), \n",
    "('murdstone', 'Edward Murdstone', 60, 'M', 'aristocrat', 10572), \n",
    "('murdstone', 'Jane Murdstone', 32, 'F', 'sister', 24929), \n",
    "('omer', 'Mr. Omer', 48, 'M', 'undertaker', 63913), \n",
    "('papa', '', -1, 'M', '', -1), \n",
    "('peggotty', 'Clara Peggotty', 49, 'F', 'servant', 21475), \n",
    "('peggotty', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('peggotty', 'Daniel Peggotty', 5, 'M', 'fisherman', 14917), \n",
    "('peggotty', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('quinion', 'Mr. Quinion', 4, 'M', 'accountant', 11279), \n",
    "('rosa', 'Rosa Dartle', 24, 'F', 'companion', 151027), \n",
    "('shepherd', 'Miss Shepherd', 50, 'F', 'student', 137243), \n",
    "('sophy', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('spenlow', 'Francis Spenlow', 51, 'M', 'proctor', 180557), \n",
    "('spenlow', 'Clarissa Spenlow', 13, 'F', 'sister', 308525), \n",
    "('spenlow', 'Lavinia Spenlow', 39, 'F', 'sister', 308405),\n",
    "('spenlow', 'Dora Spenlow', 27, 'F', 'wife', 201865), \n",
    "('spiker', 'Mr. Henry Spiker', 52, 'M', 'solicitor', 192673), \n",
    "('steerforth', 'Mrs. Steerforth', 53, 'F', 'mother', 43302), \n",
    "('steerforth', 'James Steerforth', 31, 'M', 'aristocrat', 40315), \n",
    "('strong', 'Dr. Strong', 54, 'M', 'headmaster', 116521),\n",
    "('strong', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741), \n",
    "('thomas', 'Thomas Traddles', 55, 'M', 'judge', 42805), \n",
    "('tiffey', 'Mr. Tiffey', 56, 'M', 'clerk', 200535), \n",
    "('traddles', 'Thomas Traddles', 55, 'M', 'judge', 42805), \n",
    "('trot', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('trotwood', 'Betsey Trotwood', 1, 'F', 'caretaker', 927), \n",
    "('trotwood', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('tungay', 'Tungay', 57, 'M', 'assistant', 44157), \n",
    "('uriah', 'Uriah Heep', 29, 'M', 'lawyer', 113168), \n",
    "('w', '', -1, '', '', -1), \n",
    "('waterbrook', 'Mr. Waterbrook', 58, 'M', 'agent', 189272), \n",
    "('wickfield', 'Agnes Wickfield', 8, 'F', 'daughter', 115327), \n",
    "('wickfield', 'Mr. Wickfield', 59, 'M', 'solicitor', 112599),\n",
    "('wilkins', 'Wilkins Micawber', 44, 'M', 'polymath', 80570)],\n",
    "columns=['name', 'entity', 'entity_ID', 'gender', 'occupation', 'first appearance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-flower",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:56:01.613822Z",
     "start_time": "2021-06-06T17:56:01.579902Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ground_truth_data_df[ground_truth_data_df['entity_ID'] != -1][33:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78d7cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:56:02.316872Z",
     "start_time": "2021-06-06T17:56:02.293388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_ent_ground_truth_df = pd.DataFrame([   \n",
    "('Agnes', 'Agnes Wickfield', 8, 'F', 'daughter', 115327),\n",
    "('Brooks', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Clarissa', 'Clarissa Spenlow', 13, 'F', 'sister', 308525), \n",
    "('Clarissa Clara', 'Clara Copperfield', 12, 'F', 'mother', 21475),\n",
    "('Clarissa Clara', 'Clara Peggotty', 49, 'F', 'servant', 24807), \n",
    "('Crewler', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('Crewler', 'Reverend Horace Crewler', 16, 'M', 'clergyman', 307170),\n",
    "('Crewler', 'Mrs. Crewler', 17, 'F', 'wife', 307147),\n",
    "('Crewler', 'Caroline Crewler', 18, 'F', 'daughter', 427626),\n",
    "('Crewler', 'Sarah Creweler', 19, 'F', 'daughter', 256127),\n",
    "('Crewler', 'Louisa Creweler', 20, 'F', 'daughter', 427668),\n",
    "('Crewler', 'Lucy Creweler', 21, 'F', 'daughter', 428983),\n",
    "('Crewler', 'Margaret Creweler', 22, 'F', 'daughter', 428981), \n",
    "('Crupp', 'Mrs. Crupp', 23, 'F', 'landlady', 183694), \n",
    "('Daisy', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Doady', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Doctor Strong', 'Dr. Strong', 54, 'M', 'headmaster', 116521),\n",
    "('Dora Sarah', 'Dora Spenlow', 27, 'F', 'wife', 201865), \n",
    "('Emma', 'Mrs. Emma Micawber', 45, 'F', 'wife', 88645), \n",
    "('Emma Emily', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('God', '', -1, '', '', -1), \n",
    "('Gulpidge', 'Mr. Gulpidge', 28, 'M', 'banker', 193863), \n",
    "('Ham', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('Henry Spiker', 'Mr. Henry Spiker', 52, 'M', 'solicitor', 192673), \n",
    "('Hopkins', '', -1, '', '', -1), \n",
    "('Jack Maldon', 'Jack Maldon', 30, 'M', 'unemployed', 116937), \n",
    "('Janet', 'Janet', 33, 'F', 'servant', 99254), \n",
    "('Jip', 'Jip', 34, 'M', 'dog', 203807), \n",
    "('Jip Heep', 'Uriah Heep', 29, 'M', 'lawyer', 113168), \n",
    "('Joram', 'Joram', 35, 'M', 'carpenter', 65119), \n",
    "('Lavinia', 'Miss Lavinia Spenlow', 39, 'F', 'sister', 308405), \n",
    "('Markleham', 'Mrs. Markleham', 41, 'F', 'mother', 130353), \n",
    "('Markleham', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741), \n",
    "('Martha', 'Martha Endell', 42, 'F', 'young woman', 174661), \n",
    "('Master David Copperfield', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('MasR Davy', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Minnie', 'Minnie Omer', 46, 'F', 'daugther', 64233), \n",
    "('Minnie Annie', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741),\n",
    "('Miss Betsey Trotwood', 'Betsey Trotwood', 1, 'F', 'caretaker', 927), \n",
    "('Miss Larkins', 'Miss Larkins', 38, 'F', 'wife', 138925), \n",
    "('Miss Mell Mills', 'Julia Mills', 37, 'F', 'daugther', 250095), \n",
    "('Miss Mowcher', 'Miss Mowcher', 47, 'F', 'hairdresser', 168955), \n",
    "('Miss Murdstone', 'Jane Murdstone', 32, 'F', 'sister', 24929), \n",
    "('Miss Rosa Dartle', 'Rosa Dartle', 24, 'F', 'companion', 151027), \n",
    "('Missis Gummidge', 'Mrs. Gummidge', 7, 'F', 'widow', 16396), \n",
    "('Mr Barkis', 'Mr. Barkis', 10, 'M', 'carrier', 32305), \n",
    "('Mr Chillip', 'Mr. Chillip', 4, 'M', 'doctor', 4546), \n",
    "('Mr Creakle', 'Mr. Creakle', 14, 'M', 'headmaster', 39424), \n",
    "('Mr Jack Dick', 'Jack Maldon', 30, 'M', 'unemployed', 116937), \n",
    "('Mr Larkins Jorkins', 'Mr. Jorkins', 36, 'M', 'businessman', 180559), \n",
    "('Mr Littimer', 'Mr. Littimer', 40, 'M', 'valet', 154468), \n",
    "('Mr Mell', 'Mr. Mell', 43, 'M', 'instructor', 39238), \n",
    "('Mr Mell', 'Charles Mell', 11, 'M', 'teacher', 104678), \n",
    "('Mr Micawber', 'Wilkins Micawber', 44, 'M', 'polymath', 80570), \n",
    "('Mr Omer', 'Mr. Omer', 48, 'M', 'undertaker', 63913), \n",
    "('Mr Peggotty', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('Mr Peggotty', 'Daniel Peggotty', 5, 'M', 'fisherman', 14917), \n",
    "('Mr Quinion', 'Mr. Quinion', 4, 'M', 'accountant', 11279), \n",
    "('Mr Spenlow', 'Francis Spenlow', 51, 'M', 'proctor', 180557),\n",
    "('Mr Thomas Traddles', 'Thomas Traddles', 55, 'M', 'judge', 42805), \n",
    "('Mr Waterbrook', 'Mr. Waterbrook', 58, 'M', 'agent', 189272), \n",
    "('Mr Wickfield', 'Mr. Wickfield', 59, 'M', 'solicitor', 112599),\n",
    "('Oh', '', -1, '', '', -1), \n",
    "('Old Soldier', '', -1, '', '', -1), \n",
    "('Sarah', 'Sarah Creweler', 19, 'F', 'daughter', 256127),\n",
    "('Sharp', '', -1, '', '', -1), \n",
    "('Shepherd', 'Miss Shepherd', 50, 'F', 'student', 137243), \n",
    "('Sophy Tiffey', 'Mr. Tiffey', 56, 'M', 'clerk', 200535), \n",
    "('Sophy Tiffey', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('Steerforth', 'James Steerforth', 31, 'M', 'aristocrat', 40315), \n",
    "('Tiffey', 'Mr. Tiffey', 56, 'M', 'clerk', 200535), \n",
    "('Trot', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Tungay', 'Tungay', 57, 'M', 'assistant', 44157)],\n",
    "columns=['name', 'entity', 'entity_ID', 'gender', 'occupation', 'first appearance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091c730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T17:56:02.962513Z",
     "start_time": "2021-06-06T17:56:02.938329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lax_ent_ground_truth_df = pd.DataFrame([  \n",
    "('Agnes', 'Agnes Wickfield', 8, 'F', 'daughter', 115327),\n",
    "('Brooks', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Clarissa Clara', 'Clarissa Spenlow', 13, 'F', 'sister', 308525), \n",
    "('Clarissa Clara', 'Clara Copperfield', 12, 'F', 'mother', 21475),\n",
    "('Clarissa Clara', 'Clara Peggotty', 49, 'F', 'servant', 24807), \n",
    "('Crewler', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('Crewler', 'Reverend Horace Crewler', 16, 'M', 'clergyman', 307170),\n",
    "('Crewler', 'Mrs. Crewler', 17, 'F', 'wife', 307147),\n",
    "('Crewler', 'Caroline Crewler', 18, 'F', 'daughter', 427626),\n",
    "('Crewler', 'Sarah Creweler', 19, 'F', 'daughter', 256127),\n",
    "('Crewler', 'Louisa Creweler', 20, 'F', 'daughter', 427668),\n",
    "('Crewler', 'Lucy Creweler', 21, 'F', 'daughter', 428983),\n",
    "('Crewler', 'Margaret Creweler', 22, 'F', 'daughter', 428981),\n",
    "('Crupp', 'Mrs. Crupp', 23, 'F', 'landlady', 183694), \n",
    "('Daisy', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Doady', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Doctor Strong', 'Dr. Strong', 54, 'M', 'headmaster', 116521),\n",
    "('Dora Sarah', 'Dora Spenlow', 27, 'F', 'wife', 201865), \n",
    "('Emma Emily', 'Mrs. Emma Micawber', 45, 'F', 'wife', 88645), \n",
    "('Emma Emily', 'Little Em\\'ly (Emily)', 6, 'F', 'working-class woman', 15845), \n",
    "('God', '', -1, '', '', -1), \n",
    "('Gulpidge', 'Mr. Gulpidge', 28, 'M', 'banker', 193863), \n",
    "('Ham', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('Henry Spiker', 'Mr. Henry Spiker', 52, 'M', 'solicitor', 192673), \n",
    "('Hopkins', '', -1, '', '', -1), \n",
    "('Janet', 'Janet', 33, 'F', 'servant', 99254), \n",
    "('Jip Heep', 'Jip', 34, 'M', 'dog', 203807), \n",
    "('Jip Heep', 'Uriah Heep', 29, 'M', 'lawyer', 113168), \n",
    "('Joram', 'Joram', 35, 'M', 'carpenter', 65119), \n",
    "('Lavinia', 'Miss Lavinia Spenlow', 39, 'F', 'sister', 308405), \n",
    "('Markleham', 'Mrs. Markleham', 41, 'F', 'mother', 130353), \n",
    "('Markleham', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741), \n",
    "('Martha', 'Martha Endell', 42, 'F', 'young woman', 174661), \n",
    "('Master David Copperfield', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('MasR Davy', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Minnie Annie', 'Minnie Omer', 46, 'F', 'daugther', 64233), \n",
    "('Minnie Annie', 'Mrs. Annie Markleham Strong', 9, 'F', 'wife', 116741),\n",
    "('Miss Betsey Trotwood', 'Betsey Trotwood', 1, 'F', 'caretaker', 927), \n",
    "('Miss Mell Mills', 'Julia Mills', 37, 'F', 'daugther', 250095), \n",
    "('Miss Mowcher', 'Miss Mowcher', 47, 'F', 'hairdresser', 168955), \n",
    "('Miss Murdstone', 'Jane Murdstone', 32, 'F', 'sister', 24929), \n",
    "('Miss Rosa Dartle', 'Rosa Dartle', 24, 'F', 'companion', 151027), \n",
    "('Missis Gummidge', 'Mrs. Gummidge', 7, 'F', 'widow', 16396), \n",
    "('Mr Barkis', 'Mr. Barkis', 10, 'M', 'carrier', 32305), \n",
    "('Mr Chillip', 'Mr. Chillip', 4, 'M', 'doctor', 4546), \n",
    "('Mr Creakle', 'Mr. Creakle', 14, 'M', 'headmaster', 39424), \n",
    "('Mr Jack Dick', 'Jack Maldon', 30, 'M', 'unemployed', 116937), \n",
    "('Mr Larkins Jorkins', 'Mr. Jorkins', 36, 'M', 'businessman', 180559), \n",
    "('Mr Littimer', 'Mr. Littimer', 40, 'M', 'valet', 154468), \n",
    "('Mr Micawber', 'Wilkins Micawber', 44, 'M', 'polymath', 80570), \n",
    "('Mr Omer', 'Mr. Omer', 48, 'M', 'undertaker', 63913), \n",
    "('Mr Peggotty', 'Ham Peggotty', 3, 'M', 'boat-builder', 4192), \n",
    "('Mr Peggotty', 'Daniel Peggotty', 5, 'M', 'fisherman', 14917), \n",
    "('Mr Quinion', 'Mr. Quinion', 4, 'M', 'accountant', 11279), \n",
    "('Mr Spenlow', 'Francis Spenlow', 51, 'M', 'proctor', 180557),\n",
    "('Mr Thomas Traddles', 'Thomas Traddles', 55, 'M', 'judge', 42805), \n",
    "('Mr Waterbrook', 'Mr. Waterbrook', 58, 'M', 'agent', 189272), \n",
    "('Mr Wickfield', 'Mr. Wickfield', 59, 'M', 'solicitor', 112599),\n",
    "('Oh', '', -1, '', '', -1), \n",
    "('Old Soldier', '', -1, '', '', -1), \n",
    "('Sharp', '', -1, '', '', -1), \n",
    "('Shepherd', 'Miss Shepherd', 50, 'F', 'student', 137243), \n",
    "('Sophy Tiffey', 'Mr. Tiffey', 56, 'M', 'clerk', 200535), \n",
    "('Sophy Tiffey', 'Miss Sophy Crewler', 15, 'F', 'wife', 255892),\n",
    "('Steerforth', 'James Steerforth', 31, 'M', 'aristocrat', 40315), \n",
    "('Trot', 'David Copperfield', 2, 'M', 'writer', 1748),\n",
    "('Tungay', 'Tungay', 57, 'M', 'assistant', 44157)],\n",
    "columns=['name', 'entity', 'entity_ID', 'gender', 'occupation', 'first appearance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-forth",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87287812",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Streamlined evaluation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f33e22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T22:54:10.519483Z",
     "start_time": "2021-06-06T22:54:10.467417Z"
    },
    "code_folding": [
     0,
     265
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, computes several clustering performance\n",
    "    metrics. The right `ground_truth_data_df`, `textually_close_ent_ground_truth_df` or \n",
    "    `lax_ent_ground_truth_df` should have been loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    same_entityness : list\n",
    "        A list containing the performance metrics with regards to the 'same_entityness' axis\n",
    "    gender : list\n",
    "        A list containing the performance metrics with regards to the 'gender' axis\n",
    "    occupations : list\n",
    "        A list containing the performance metrics with regards to the 'occupations' axis\n",
    "    first_appearance : list\n",
    "        A list containing the performance metrics with regards to the 'first_appearance' axis\n",
    "    '''\n",
    "    \n",
    "    # SAME ENTITY-NESS\n",
    "    same_entityness = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_entity = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['entity_ID'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    same_entityness.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                            np.array(tmp_df[2]), \n",
    "                                                            metric='euclidean', \n",
    "                                                            random_state=0))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                   np.array(tmp_df[2])))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                                                np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_entity)\n",
    "    entityness_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 45 # obtained by the elbow method\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(entityness_matrix, )\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    same_entityness.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    same_entityness.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), \n",
    "                                                                      predicted_clusters, \n",
    "                                                                      average_method='arithmetic'))\n",
    "    \n",
    "    \n",
    "    # GENDER\n",
    "    gender = []\n",
    "    \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_gender = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['gender'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "\n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                                   np.array(tmp_df[2] == 'M').astype(int), \n",
    "                                                   metric='euclidean', \n",
    "                                                   random_state=0))\n",
    "    gender.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    gender.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), np.array(tmp_df[2])))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_gender)\n",
    "    gender_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    k_choice = 2 # two genders in PG literature (men and women)\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(gender_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    gender.append(sklearn.metrics.rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_rand_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.mutual_info_score(np.array(tmp_df[2]), predicted_clusters))\n",
    "    gender.append(sklearn.metrics.adjusted_mutual_info_score(np.array(tmp_df[2]), predicted_clusters, \n",
    "                                                             average_method='arithmetic'))\n",
    "    \n",
    "    # OCCUPATION\n",
    "    occupations = []\n",
    "    \n",
    "    # get bert embeddings for each occupation, then do distance matrix    \n",
    "    if embeddings_type == 'first_version':\n",
    "        precomputed_occupation_embeddings = {}\n",
    "\n",
    "        bert_model = 'bert-large-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        mask_embs_occupation = []\n",
    "        for k in embeddings:\n",
    "            if k.lower() in ground_truth_data_df['name'].tolist():\n",
    "                occupation = ground_truth_data_df[ground_truth_data_df['name'] == k]['occupation'].values[0]\n",
    "                if occupation not in precomputed_occupation_embeddings:\n",
    "                    encoded_input = tokenizer(occupation, return_tensors='pt') \n",
    "                    output = model(**encoded_input)\n",
    "                    precomputed_occupation_embeddings[occupation] = np.array(output[0][0][0, :].tolist())\n",
    "\n",
    "                mask_embs_occupation.append((k, \n",
    "                                             embeddings[k], \n",
    "                                             precomputed_occupation_embeddings[occupation]))\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        precomputed_occupation_embeddings = {}\n",
    "\n",
    "        bert_model = 'bert-large-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        mask_embs_occupation = []\n",
    "        for k in embeddings:\n",
    "            if k in textually_close_ent_ground_truth_df['name'].tolist():\n",
    "                occupation = textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['occupation'].values[0]\n",
    "                if occupation not in precomputed_occupation_embeddings:\n",
    "                    encoded_input = tokenizer(occupation, return_tensors='pt') \n",
    "                    output = model(**encoded_input)\n",
    "                    precomputed_occupation_embeddings[occupation] = np.array(output[0][0][0, :].tolist())\n",
    "\n",
    "                mask_embs_occupation.append((k, \n",
    "                                             embeddings[k]['MASK'], \n",
    "                                             precomputed_occupation_embeddings[occupation]))\n",
    "    elif embeddings_type == 'lax':\n",
    "        precomputed_occupation_embeddings = {}\n",
    "\n",
    "        bert_model = 'bert-large-cased'\n",
    "        tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "        mask_embs_occupation = []\n",
    "        for k in embeddings:\n",
    "            if k in lax_ent_ground_truth_df['name'].tolist():\n",
    "                occupation = lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['occupation'].values[0]\n",
    "                if occupation not in precomputed_occupation_embeddings:\n",
    "                    encoded_input = tokenizer(occupation, return_tensors='pt') \n",
    "                    output = model(**encoded_input)\n",
    "                    precomputed_occupation_embeddings[occupation] = np.array(output[0][0][0, :].tolist())\n",
    "\n",
    "                mask_embs_occupation.append((k, \n",
    "                                             embeddings[k]['MASK'], \n",
    "                                             precomputed_occupation_embeddings[occupation]))\n",
    "                \n",
    "    tmp_df = pd.DataFrame(mask_embs_occupation)\n",
    "    occupation_matrix = np.array([np.array(emb) for emb in tmp_df[2]])\n",
    "    \n",
    "    # k based both on \"vector\" being predicted (occupation) and overall clustering\n",
    "    # using elbow method\n",
    "    k_choice = 12\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(occupation_matrix)\n",
    "\n",
    "    occupations.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                     kmean.predict(np.array([np.array(emb) for emb in tmp_df[2]])), \n",
    "                                     metric='euclidean', \n",
    "                                     random_state=0))\n",
    "    \n",
    "    occupations.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array([np.array(emb) for emb in tmp_df[2]]))))\n",
    "    \n",
    "    occupations.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array([np.array(emb) for emb in tmp_df[2]]))))\n",
    "    \n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_occupation)\n",
    "    ground_truth_based_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[2]]))\n",
    "    occupation_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(occupation_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    occupations.append(sklearn.metrics.rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    occupations.append(sklearn.metrics.adjusted_rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    occupations.append(sklearn.metrics.mutual_info_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    occupations.append(sklearn.metrics.adjusted_mutual_info_score(ground_truth_based_clusters, \n",
    "                                                                  predicted_clusters, average_method='arithmetic'))\n",
    "    \n",
    "    # FIRST APPEARANCE\n",
    "    first_appearance = []\n",
    "    \n",
    "    # build distance matrix \n",
    "    if embeddings_type == 'first_version':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k], \n",
    "                             ground_truth_data_df[ground_truth_data_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k.lower() in ground_truth_data_df['name'].tolist()]\n",
    "    elif embeddings_type == 'textually_close':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             textually_close_ent_ground_truth_df[textually_close_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in textually_close_ent_ground_truth_df['name'].tolist()]\n",
    "    elif embeddings_type == 'lax':\n",
    "        mask_embs_appear = [(k, \n",
    "                             embeddings[k]['MASK'], \n",
    "                             lax_ent_ground_truth_df[lax_ent_ground_truth_df['name'] == k]['first appearance'].values[0]) \n",
    "                            for k in embeddings \n",
    "                            if k in lax_ent_ground_truth_df['name'].tolist()]\n",
    "        \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    appear_matrix = np.array(tmp_df[2]).reshape(-1, 1)\n",
    "\n",
    "    # k based both on \"vector\" being predict (first appearance in book) and overall clustering\n",
    "    # using elbow shape\n",
    "    k_choice = 17\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "\n",
    "    first_appearance.append(sklearn.metrics.silhouette_score(np.array(tmp_df[1].tolist()), \n",
    "                                         kmean.predict(np.array(tmp_df[2]).reshape(-1,1)), \n",
    "                                         metric='euclidean', \n",
    "                                         random_state=0))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.calinski_harabasz_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.davies_bouldin_score(np.array(tmp_df[1].tolist()), \n",
    "                                 kmean.predict(np.array(tmp_df[2]).reshape(-1,1))))\n",
    "    \n",
    "    tmp_df = pd.DataFrame(mask_embs_appear)\n",
    "    ground_truth_based_clusters = kmean.predict(np.array(tmp_df[2]).reshape(-1,1))\n",
    "    appear_matrix = np.array([np.array(emb) for emb in tmp_df[1]])\n",
    "    kmean = KMeans(n_clusters=k_choice, random_state=0).fit(appear_matrix)\n",
    "    predicted_clusters = kmean.predict(np.array([np.array(emb) for emb in tmp_df[1]]))\n",
    "    \n",
    "    first_appearance.append(sklearn.metrics.rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_rand_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.mutual_info_score(ground_truth_based_clusters, predicted_clusters))\n",
    "    first_appearance.append(sklearn.metrics.adjusted_mutual_info_score(ground_truth_based_clusters, predicted_clusters, \n",
    "                                                                       average_method='arithmetic'))\n",
    "    \n",
    "    return same_entityness, gender, occupations, first_appearance\n",
    "\n",
    "def print_clustering_metrics(embeddings, embeddings_type):\n",
    "    '''Given embeddings, and their ground truth data type, display in a table several\n",
    "    clustering performance metrics. The right `ground_truth_data_df`, \n",
    "    `textually_close_ent_ground_truth_df` or `lax_ent_ground_truth_df` should have been \n",
    "    loaded into memory before calling this function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings : dictionary\n",
    "        The dictionary containing each entity and their associated embedding vector\n",
    "    embeddings_type : str\n",
    "        The matching ground truth data type for the given embeddings (either 'first_version',\n",
    "        'textually_close' or 'lax')\n",
    "    '''\n",
    "    \n",
    "    same_entityness, gender, occupation, first_appearance = get_clustering_metrics(embeddings, embeddings_type)\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print('|                            | Same Entity-ness |  Gender  | Occupation | First Appearance |')\n",
    "    print('--------------------------------------------------------------------------------------------')\n",
    "    print(f'| Silhouette Score           |     {same_entityness[0]:8.5f}     | {gender[0]:8.5f} |  {occupation[0]:8.5f}  |     {first_appearance[0]:8.5f}     |')\n",
    "    print(f'| Calinski Harabasz Score    |     {same_entityness[1]:8.5f}     | {gender[1]:8.5f} |  {occupation[1]:8.5f}  |     {first_appearance[1]:8.5f}     |')\n",
    "    print(f'| Davies Bouldin Score       |     {same_entityness[2]:8.5f}     | {gender[2]:8.5f} |  {occupation[2]:8.5f}  |     {first_appearance[2]:8.5f}     |')\n",
    "    print(f'| Rand Score                 |     {same_entityness[3]:8.5f}     | {gender[3]:8.5f} |  {occupation[3]:8.5f}  |     {first_appearance[3]:8.5f}     |')\n",
    "    print(f'| Adjusted Rand Score        |     {same_entityness[4]:8.5f}     | {gender[4]:8.5f} |  {occupation[4]:8.5f}  |     {first_appearance[4]:8.5f}     |')\n",
    "    print(f'| Mutual Info Score          |     {same_entityness[5]:8.5f}     | {gender[5]:8.5f} |  {occupation[5]:8.5f}  |     {first_appearance[5]:8.5f}     |')\n",
    "    print(f'| Adjusted Mutual Info Score |     {same_entityness[6]:8.5f}     | {gender[6]:8.5f} |  {occupation[6]:8.5f}  |     {first_appearance[6]:8.5f}     |')\n",
    "    print('--------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712ba5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:10:12.517538Z",
     "start_time": "2021-06-06T21:09:17.795304Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "copperfield_emb_model = get_embeddings_model('766', embedding_model='skipgram')\n",
    "embeddings = get_entities_embeddings('766', copperfield_emb_model)\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Word2Vec Embeddings - Skip-gram')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd98ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:11:04.923622Z",
     "start_time": "2021-06-06T21:10:12.520153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "copperfield_emb_model = get_embeddings_model('766', embedding_model='cbow')\n",
    "embeddings = get_entities_embeddings('766', copperfield_emb_model)\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Word2Vec Embeddings - CBOW')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ecc46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:12:09.706451Z",
     "start_time": "2021-06-06T21:11:04.926903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "copperfield_emb_model = get_embeddings_model('766')\n",
    "embeddings = get_entities_embeddings('766', copperfield_emb_model)\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('FastText Embeddings')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eca2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:12:36.460771Z",
     "start_time": "2021-06-06T21:12:09.709067Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_BERT_embeddings('766')\n",
    "embeddings = {k: avg_BERT_embeddings[k]['MASK'] for k in avg_BERT_embeddings}\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Non-Finetuned BERT [MASK] Embeddings')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800c82c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:13:05.715253Z",
     "start_time": "2021-06-06T21:12:36.463997Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_averaged_finetuned_BERT_embeddings('766')\n",
    "embeddings = {k: avg_BERT_embeddings[k]['MASK'] for k in avg_BERT_embeddings}\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Finetuned BERT [MASK] Embeddings')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a03a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:15:29.444520Z",
     "start_time": "2021-06-06T21:14:22.217507Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_mixed_avg_BERT_embs('766')\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Weigthed Non-Finetuned BERT [MASK] Embeddings - Concatenated')\n",
    "print_clustering_metrics(avg_BERT_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea067c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:16:37.380644Z",
     "start_time": "2021-06-06T21:15:29.447903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_mixed_avg_BERT_embs('766', mode='sum')\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Weigthed Non-Finetuned BERT [MASK] Embeddings - Summed')\n",
    "print_clustering_metrics(avg_BERT_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e769a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:17:47.091380Z",
     "start_time": "2021-06-06T21:16:37.383253Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_mixed_avg_finetuned_BERT_embs('766')\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Weigthed Finetuned BERT [MASK] Embeddings - Concatenated')\n",
    "print_clustering_metrics(avg_BERT_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b13312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T21:18:54.146102Z",
     "start_time": "2021-06-06T21:17:47.095988Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "avg_BERT_embeddings = get_mixed_avg_finetuned_BERT_embs('766', mode='sum')\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Weigthed Finetuned BERT [MASK] Embeddings - Summed')\n",
    "print_clustering_metrics(avg_BERT_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe7c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T22:43:40.466997Z",
     "start_time": "2021-06-06T22:43:26.044096Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "neb_embeddings = get_nearest_entities_based_embeddings('766')\n",
    "embeddings = {k: [v[0] for v in neb_embeddings[k]] for k in neb_embeddings}\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Nearest Entities Embeddings')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd98c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T22:44:33.951083Z",
     "start_time": "2021-06-06T22:43:50.223034Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embeddings = get_nearest_verbs_based_embeddings('766')\n",
    "embeddings_type = 'first_version'\n",
    "\n",
    "print('Nearest Verbs Embeddings')\n",
    "print_clustering_metrics(embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9acba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T22:54:32.264475Z",
     "start_time": "2021-06-06T22:54:18.170711Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gutenberg_id = '766'\n",
    "with open(f'notebooks_data/book_dfs/{gutenberg_id}_textually_close_ent_embeddings.pk', 'rb') as f:\n",
    "    textually_close_ent_embeddings = pickle.load(f)\n",
    "embeddings_type = 'textually_close'\n",
    "\n",
    "print('Textually-Close Merged BookEntities Embeddings')\n",
    "print_clustering_metrics(textually_close_ent_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d8ef0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T22:54:46.150951Z",
     "start_time": "2021-06-06T22:54:32.266434Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gutenberg_id = '766'\n",
    "with open(f'notebooks_data/book_dfs/{gutenberg_id}_lax_ent_embeddings.pk', 'rb') as f:\n",
    "    lax_ent_embeddings = pickle.load(f)\n",
    "embeddings_type = 'lax'\n",
    "\n",
    "print('Laxly Merged BookEntities Embeddings')\n",
    "print_clustering_metrics(lax_ent_embeddings, embeddings_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-peeing",
   "metadata": {},
   "source": [
    "## Book Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0727d12",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read in relevant background knowledge (honorifics, gendered names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-serum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:09.767357Z",
     "start_time": "2021-06-11T13:27:09.752252Z"
    },
    "code_folding": [
     0,
     27
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "honorifics = ['Ab\"d', 'Admo\"r', 'Adv', 'Advocate', 'BDS', 'Baron', 'Baroness', 'Baronet', \n",
    "              'Br', 'Brother', 'Cantor', 'Chancellor', 'Chief Executive', 'Chief Rabbi', \n",
    "              'Cl', 'Counsel', 'Countess', 'DDS', 'DMD', 'DO', 'DPhil', 'DVM', 'Dame', \n",
    "              'Dean', 'Director', 'Doc', 'Doctor', 'Dr', 'Earl', 'EdD', 'Elder', 'Emi', \n",
    "              'Eminent', 'Esq', 'Esq.', 'Esquire', 'Eur Ing', 'Excellence', 'Excellency', \n",
    "              'Father', 'Fr', 'Gaava\"d', 'Gentleman', 'Grand Rabbi', 'HAH', 'HE', 'HH', \n",
    "              'HMEH', 'Her Excellence', 'Her Excellency', 'Her Honour', 'His All Holiness', \n",
    "              'His Beatitude', 'His Eminence', 'His Excellence', 'His Excellency', 'His Grace', \n",
    "              'His Holiness', 'His Honour', 'His Most Eminent Highness', 'Holy Father', 'Hon.', \n",
    "              'Hfiz', 'Hfizah', 'Hj', 'Imm', 'KC', \"King\\'s Counsel\", 'Lady', 'Lord', \n",
    "              'Lordship', 'MBBS', 'MBChB', 'MD', \"Ma\\'am\", 'Madam', 'Marchioness', 'Marquess', \n",
    "              'Master', 'Mawln', 'Miss', 'Missis', 'Missus', 'Mister', 'Mistress', \n",
    "              'Most Reverend Eminence', 'Most Reverend Excellency', 'Mr', 'Mrs', 'Mrs.', \n",
    "              'Ms', 'Ms.', 'Muft.', 'Mx', 'My Lady', 'My Lord', 'Mz.', 'Nun', 'OD', 'Pastor', \n",
    "              'PhD', 'PharmD', 'Pope', 'Pope Emeritus', 'Pr', 'President', 'Principal', 'Prof', \n",
    "              'Professor', 'Provost', 'QC', \"Queen\\'s Counsel\", 'Qr', 'Raava\"d', 'Rabbi', \n",
    "              'Rav', 'Rebbe', 'Rebbetzin', 'Rector', 'Regent', 'Rev.', 'Reverend', 'Rt Hon', \n",
    "              'SCl', 'Saint', 'Sayyid', 'Sayyidah', 'Senior Counsel', 'Sharif', 'Shaykh', \n",
    "              'Sir', 'Sire', 'Sis', 'Sister', 'Sr', \"The Hon\\'ble\", 'The Hon.', 'The Honorable', \n",
    "              'The Honourable', 'The Most Blessed', 'The Most Honourable', 'The Most Rev', \n",
    "              'The Most Revd', 'The Most Reverend', 'The Rev', 'The Revd', 'The Reverend', \n",
    "              'The Right Honourable', 'The Right Reverend', 'The Rt Rev', 'Ven', 'Venerable', \n",
    "              'Vice-Chancellor', 'Viscount', 'Viscountess', 'Warden', 'Yeoman', 'Your All Holiness', \n",
    "              'Your Beatitude', 'Your Eminence', 'Your Excellence', 'Your Excellency', \n",
    "              'Your Grace', 'Your Holiness', 'Your Honour', 'Your Ladyship', 'Your Most Eminent Highness']\n",
    "        \n",
    "gender_from_honorific = {'Ab\"d': 'M', \n",
    "                         'Admo\"r': 'M', \n",
    "                         'Adv': 'M', \n",
    "                         'Advocate': 'M', \n",
    "                         'BDS': 'M', \n",
    "                         'Baron': 'M', \n",
    "                         'Baroness': 'F', \n",
    "                         'Baronet': 'M', \n",
    "                         'Br': 'M', \n",
    "                         'Brother': 'M', \n",
    "                         'Cantor': 'M', \n",
    "                         'Chancellor': None, \n",
    "                         'Chief Executive': None, \n",
    "                         'Chief Rabbi': 'M',  \n",
    "                         'Cl': 'M', \n",
    "                         'Counsel': 'M', \n",
    "                         'Countess': 'F', \n",
    "                         'DDS': None, #'M', \n",
    "                         'DMD': None, #'M', \n",
    "                         'DO': None, #'M', \n",
    "                         'DPhil': None, \n",
    "                         'DVM': None, #'M', \n",
    "                         'Dame': 'F', \n",
    "                         'Dean': None,\n",
    "                         'Director': None,\n",
    "                         'Doc': None, 'Doctor': None, 'Dr': None, \n",
    "                         'Earl': 'M', \n",
    "                         'EdD': None, \n",
    "                         'Elder': None, \n",
    "                         'Emi': 'M', \n",
    "                         'Eminent': 'M', \n",
    "                         'Esq': 'M', \n",
    "                         'Esq.': 'M', \n",
    "                         'Esquire': 'M', \n",
    "                         'Eur Ing': None, #'M',\n",
    "                         'Excellence': None, \n",
    "                         'Excellency': None, \n",
    "                         'Father': 'M', \n",
    "                         'Fr': 'M', \n",
    "                         'Gaava\"d': 'M', \n",
    "                         'Gentleman': 'M', \n",
    "                         'Grand Rabbi': 'M', \n",
    "                         'HAH': 'M',\n",
    "                         'HE': 'M', \n",
    "                         'HH': 'M', \n",
    "                         'HMEH': 'M', \n",
    "                         'Her Excellence': 'F', \n",
    "                         'Her Excellency': 'F', \n",
    "                         'Her Honour': 'F', \n",
    "                         'His All Holiness': 'M', \n",
    "                         'His Beatitude': 'M', \n",
    "                         'His Eminence': 'M', \n",
    "                         'His Excellence': 'M', \n",
    "                         'His Excellency': 'M', \n",
    "                         'His Grace': 'M', \n",
    "                         'His Holiness': 'M', \n",
    "                         'His Honour': 'M', \n",
    "                         'His Most Eminent Highness': 'M', \n",
    "                         'Holy Father': 'M', \n",
    "                         'Hon.': 'M', \n",
    "                         'Hfiz': 'M', \n",
    "                         'Hfizah': 'F', \n",
    "                         'Hj': 'M', \n",
    "                         'Imm': 'M', \n",
    "                         'KC': 'M', \n",
    "                         \"King\\'s Counsel\": None, #'M', \n",
    "                         'Lady': 'F', \n",
    "                         'Lord': 'M', \n",
    "                         'Lordship': 'M', \n",
    "                         'MBBS': 'M', \n",
    "                         'MBChB': 'M', \n",
    "                         'MD': None, \n",
    "                         \"Ma\\'am\": 'F', \n",
    "                         'Madam': 'F', \n",
    "                         'Marchioness': 'F', \n",
    "                         'Marquess': 'M', \n",
    "                         'Master': 'M', \n",
    "                         'Mawln': 'M', \n",
    "                         'Miss': 'F', \n",
    "                         'Missis': 'F', \n",
    "                         'Missus': 'F', \n",
    "                         'Mister': 'M', \n",
    "                         'Mistress': 'F', \n",
    "                         'Most Reverend Eminence': 'M', \n",
    "                         'Most Reverend Excellency': 'M', \n",
    "                         'Mr': 'M', \n",
    "                         'Mr.': 'M', \n",
    "                         'Mrs': 'F', \n",
    "                         'Mrs.': 'F', \n",
    "                         'Ms': 'F', \n",
    "                         'Ms.': 'F', \n",
    "                         'Muft.': 'M', \n",
    "                         'Mx': None, \n",
    "                         'Mx.': None, \n",
    "                         'My Lady': 'F', \n",
    "                         'My Lord': 'M', \n",
    "                         'Mz.': 'M', \n",
    "                         'Nun': 'F', \n",
    "                         'OD': 'M', \n",
    "                         'Pastor': 'M', \n",
    "                         'PhD': None, \n",
    "                         'PharmD': None, \n",
    "                         'Pope': 'M', \n",
    "                         'Pope Emeritus': 'M', \n",
    "                         'Pr': 'M', \n",
    "                         'President': 'M', \n",
    "                         'Principal': 'M', \n",
    "                         'Prof': 'M', \n",
    "                         'Professor': 'M', \n",
    "                         'Provost': 'M', \n",
    "                         'QC': 'M', \n",
    "                         \"Queen\\'s Counsel\": None, #'M', \n",
    "                         'Qr': 'M', \n",
    "                         'Raava\"d': 'M', \n",
    "                         'Rabbi': 'M', \n",
    "                         'Rav': 'M', \n",
    "                         'Rebbe': 'M', \n",
    "                         'Rebbetzin': 'M', \n",
    "                         'Rector': 'M', \n",
    "                         'Regent': 'M', \n",
    "                         'Rev.': 'M', \n",
    "                         'Reverend': 'M', \n",
    "                         'Rt Hon': 'M', \n",
    "                         'SCl': 'M', \n",
    "                         'Saint': 'M', \n",
    "                         'Sayyid': 'M', \n",
    "                         'Sayyidah': 'F', \n",
    "                         'Senior Counsel': 'M', \n",
    "                         'Sharif': 'M', \n",
    "                         'Shaykh': 'M', \n",
    "                         'Sir': 'M', \n",
    "                         'Sire': 'M', \n",
    "                         'Sis': 'F', \n",
    "                         'Sister': 'F', \n",
    "                         'Sr': 'F', \n",
    "                         \"The Hon\\'ble\": 'M', \n",
    "                         'The Hon.': 'M', \n",
    "                         'The Honorable': 'M', \n",
    "                         'The Honourable': 'M', \n",
    "                         'The Most Blessed': 'M', \n",
    "                         'The Most Honourable': 'M', \n",
    "                         'The Most Rev': 'M', \n",
    "                         'The Most Revd': 'M', \n",
    "                         'The Most Reverend': 'M', \n",
    "                         'The Rev': 'M', \n",
    "                         'The Revd': 'M', \n",
    "                         'The Reverend': 'M', \n",
    "                         'The Right Honourable': 'M', \n",
    "                         'The Right Reverend': 'M', \n",
    "                         'The Rt Rev': 'M', \n",
    "                         'Ven': None, \n",
    "                         'Venerable': None,\n",
    "                         'Vice-Chancellor': None, #'M', \n",
    "                         'Viscount': 'M', \n",
    "                         'Viscountess': 'F', \n",
    "                         'Warden': 'M', \n",
    "                         'Yeoman': 'M', \n",
    "                         'Your All Holiness': 'M', \n",
    "                         'Your Beatitude': 'M', \n",
    "                         'Your Eminence': 'M', \n",
    "                         'Your Excellence': None, \n",
    "                         'Your Excellency': None, \n",
    "                         'Your Grace': 'M', \n",
    "                         'Your Holiness': 'M', \n",
    "                         'Your Honour': None, \n",
    "                         'Your Ladyship': 'F', \n",
    "                         'Your Most Eminent Highness': None,\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721d96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:09.775254Z",
     "start_time": "2021-06-11T13:27:09.769091Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "male_names = []\n",
    "with open('male_names.txt', 'r') as f:\n",
    "    male_names = [*map(str.strip, f.readlines())]\n",
    "\n",
    "female_names = []\n",
    "with open('female_names.txt', 'r') as f:\n",
    "    female_names = [*map(str.strip, f.readlines())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b48cef0",
   "metadata": {},
   "source": [
    "### Define Book Entity and related classes and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-translation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:10.041082Z",
     "start_time": "2021-06-11T13:27:10.016766Z"
    },
    "code_folding": [
     8,
     25,
     43,
     73,
     105,
     132,
     168
    ]
   },
   "outputs": [],
   "source": [
    "constants = Constants()\n",
    "constants.titles.add(*[h for h in honorifics])\n",
    "\n",
    "stop_words = list(get_stop_words('en')) # about 900 stopwords\n",
    "nltk_words = list(stopwords.words('english')) # about 150 stopwords\n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "\n",
    "def count_reduce(obj1, obj2):\n",
    "    '''Applies a reductor to the two tuples passed, counting their total number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj1 : tuple\n",
    "        A (key, count) tuple\n",
    "    obj2 : tuple\n",
    "        A (key, count) tuple\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_tup : tuple\n",
    "        The reduced (key, count) tuple\n",
    "    '''\n",
    "    return (obj1[0], obj1[1], obj1[2]+obj2[2])\n",
    "\n",
    "def remove_non_ascii_chars(n):\n",
    "    '''Removes non-ascii characters from a given string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : str\n",
    "        The string from which to remove non-ascii characters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_n : str\n",
    "        The string with only ascii characters\n",
    "    '''\n",
    "    printable = set(string.printable)\n",
    "    \n",
    "    n = n.replace('', '\\'').replace('', '\\'')\n",
    "    return ''.join(filter(lambda x: x in printable, n))\n",
    "\n",
    "def is_nickname(n1, n2):\n",
    "    '''Given two names, checks whether any of them is a nickname or diminutive\n",
    "    form of the other.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : str\n",
    "        A name\n",
    "    n2 : str\n",
    "        A name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_nickname : bool\n",
    "        Whether any of them is a nickname or diminutive form of the other\n",
    "    '''\n",
    "    \n",
    "    nickname_finder = NameDenormalizer()\n",
    "    n1_alts = nickname_finder.get(n1)\n",
    "    n2_alts = nickname_finder.get(n2)\n",
    "    \n",
    "    if n1_alts != None:\n",
    "        if any([n1_alt == n2_form for n1_alt in n1_alts for n2_form in re.split('[^a-zA-Z]', n2)]):\n",
    "            return True\n",
    "    if n2_alts != None:\n",
    "        if any([n2_alt == n1_form for n2_alt in n2_alts for n1_form in re.split('[^a-zA-Z]', n1)]):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def phonetically_matches(n1, n2, stricter=True):\n",
    "    '''Given two names, checks whether they phonetically match (i.e. are phonetically\n",
    "    very similar).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : str\n",
    "        A name\n",
    "    n2 : str\n",
    "        A name\n",
    "    stricter : bool, optional\n",
    "        A flag indicating whether the names have to pass all tests (True) or at least \n",
    "        just one of them (default is True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_nickname : bool\n",
    "        Whether the two names phonetically match\n",
    "    '''\n",
    "\n",
    "    soundex = Soundex()\n",
    "    dmeta = fuzzy.DMetaphone()\n",
    "    \n",
    "    if stricter:\n",
    "        return (soundex.compare(n1, n2) >= 0) and\\\n",
    "               (editdistance.eval(fuzzy.nysiis(n1), fuzzy.nysiis(n2)) <= 1) and\\\n",
    "               (editdistance.eval(dmeta(remove_non_ascii_chars(n1))[0], dmeta(remove_non_ascii_chars(n2))[0]) <= 1)\n",
    "    \n",
    "    return (soundex.compare(n1, n2) >= 0) or\\\n",
    "           (editdistance.eval(fuzzy.nysiis(n1), fuzzy.nysiis(n2)) <= 1) or\\\n",
    "           (editdistance.eval(dmeta(remove_non_ascii_chars(n1))[0], dmeta(remove_non_ascii_chars(n2))[0]) <= 1)\n",
    "    \n",
    "def textually_matches(n1, n2, stricter=True):\n",
    "    '''Given two names, checks whether they textually match (i.e. are textually\n",
    "    very similar).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : str\n",
    "        A name\n",
    "    n2 : str\n",
    "        A name\n",
    "    stricter : bool, optional\n",
    "        A flag indicating whether the names have to pass all tests (True) or at least \n",
    "        just one of them (default is True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_nickname : bool\n",
    "        Whether the two names textually match\n",
    "    '''\n",
    "    \n",
    "    if stricter:\n",
    "        return (get_jaro_distance(n1, n2) > 0.85) and\\\n",
    "               (editdistance.eval(n1, n2) <= 1)\n",
    "\n",
    "    return (get_jaro_distance(n1, n2) > 0.85) or\\\n",
    "           (editdistance.eval(n1, n2) <= 1)\n",
    "\n",
    "def names_fuzzy_match(n1, n2):\n",
    "    '''Given two names, checks whether they fuzzily match (whether they are \n",
    "    considered very similar by any metric - whether they are equal strings,\n",
    "    a nickname or diminutive of the other, or phonetically or textually very\n",
    "    similar).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : str\n",
    "        A name\n",
    "    n2 : str\n",
    "        A name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_nickname : bool\n",
    "        Whether the two names textually match\n",
    "    '''\n",
    "    \n",
    "    if n1.lower() == n2.lower():\n",
    "        return True\n",
    "    \n",
    "    # handle nicknames\n",
    "    if is_nickname(n1, n2):\n",
    "        return True\n",
    "    \n",
    "    # handle phonetic matching\n",
    "    if phonetically_matches(n1, n2):\n",
    "        return True\n",
    "    \n",
    "    # handle similar strings (string distances)\n",
    "    if textually_matches(n1, n2):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def get_gender(honorific, names):\n",
    "    '''Given an entity's honorific and names, tries to extract their gender.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    honorific : str\n",
    "        The entity's honorific\n",
    "    names : list\n",
    "        A list of the entity's names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gender : str\n",
    "        The inferred gender ('M' or 'F') or None if unable to infer it\n",
    "    '''\n",
    "    \n",
    "    if honorific != '' and honorific.title() in gender_from_honorific:\n",
    "        return gender_from_honorific[honorific.title()] \n",
    "    for name in names:\n",
    "        if name != '':\n",
    "            if name.title() in male_names:\n",
    "                return 'M'\n",
    "            if name.title() in female_names:\n",
    "                return 'F'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edafcb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:10.473333Z",
     "start_time": "2021-06-11T13:27:10.428088Z"
    },
    "code_folding": [
     0,
     85,
     107,
     160,
     212,
     274,
     347
    ]
   },
   "outputs": [],
   "source": [
    "class BookListEntity:\n",
    "    '''\n",
    "    A class used to represent an entity as a list of honorifics, first, middle\n",
    "    and last names, suffixes, nicknames, all their addresses and their inferred\n",
    "    gender.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    honorific : list\n",
    "        a list of the entity's honorifics\n",
    "    first_name : list\n",
    "        a list of the entity's first names\n",
    "    middle_name : list\n",
    "        a list of the entity's middle names\n",
    "    last_name : list\n",
    "        a list of the entity's last names\n",
    "    suffix : list\n",
    "        a list of the entity's suffixes\n",
    "    nickname : list\n",
    "        a list of the entity's nicknames\n",
    "    gender : str\n",
    "        the entity's inferred gender\n",
    "    all_addresses : list\n",
    "        a list of the all the entity's addresses and mentions\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    get_universal_address()\n",
    "        Returns the entity's \"short name\" string representation, a consistent but\n",
    "        more readable output than simply stringifying the object.\n",
    "    exactly_references_entity(check_name)\n",
    "        Checks whether the given name exactly references this entity.\n",
    "    textually_close_references_entity(check_name)\n",
    "        Checks whether the given name matches this entity in a textually-close manner.\n",
    "    references_entity(check_name)\n",
    "        Checks whether the given name matches this entity in a general, approximated, \n",
    "        manner. Still requires a very high level of similarity, however.\n",
    "    fuzzy_references_entity(check_name)\n",
    "        Checks whether the given name matches this entity in a fuzzy manner,\n",
    "        by comparing name comparisons not covered by stricter functions.\n",
    "    merge(new_name, strictness='high')\n",
    "        Merges a new form of address to the current entity.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, name_str):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        name_str : str\n",
    "            The entity's name\n",
    "        '''\n",
    "        \n",
    "        hn = HumanName(name_str, constants=constants)\n",
    "        \n",
    "        self.honorific = [t.lower() for t in hn.title_list]\n",
    "        self.first_name = [n.lower() for n in hn.first_list]\n",
    "        self.middle_name = [n.lower() for n in hn.middle_list]\n",
    "        self.last_name = [n.lower() for n in hn.last_list]\n",
    "        self.suffix = [s.lower() for s in hn.suffix_list]\n",
    "        self.nickname = [n.lower() for n in hn.nickname_list]\n",
    "        self.gender = get_gender(self.honorific[0] if self.honorific else '', [n for n in self.first_name + \n",
    "                                                                       self.middle_name + self.nickname])\n",
    "        self.all_addresses = [name_str]\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns the BookListEntity's string representation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ent_str : str\n",
    "            The BookListEntity's string representation\n",
    "        '''\n",
    "        return f\"\"\"<BookListEntity : [\n",
    "\thonorific: '{self.honorific}'\n",
    "\tfirst: '{self.first_name}' \n",
    "\tmiddle: '{self.middle_name}' \n",
    "\tlast: '{self.last_name}' \n",
    "\tsuffix: '{self.suffix}'\n",
    "\tnickname: '{self.nickname}'\n",
    "\tgender: '{self.gender}'\n",
    "\tall adresses: '{self.all_addresses}'\n",
    "]>\"\"\"\n",
    "    \n",
    "    def get_universal_address(self):\n",
    "        '''Returns the entity's \"short name\" string representation, a consistent but\n",
    "        more readable output than simply stringifying the object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        address : str\n",
    "            the entity's short name\n",
    "        '''\n",
    "        address = ''\n",
    "        if self.honorific != []:\n",
    "            address = self.honorific[0] + ' '\n",
    "        if self.first_name != []:\n",
    "            valid_names = [first_n for first_n in self.first_name if first_n != '']\n",
    "            if valid_names != []:\n",
    "                address = address + valid_names[0] + ' '\n",
    "        if self.last_name != []:\n",
    "            valid_names = [last_n for last_n in self.last_name if last_n != '']\n",
    "            if valid_names != []:\n",
    "                address = address + valid_names[0] + ' '\n",
    "        return address\n",
    "    \n",
    "    def exactly_references_entity(self, check_name):\n",
    "        '''Checks whether the given name exactly references this entity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        check_name : str\n",
    "            The name against which to check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matches : bool\n",
    "            Whether the given name exactly references this entity\n",
    "        '''\n",
    "    \n",
    "        hn = HumanName(check_name, constants=constants)\n",
    "        check_name_gender = get_gender(hn.title, [n for n in hn.first_list + hn.middle_list + hn.nickname_list])\n",
    "\n",
    "        # first, if possible, check gender matches\n",
    "        if self.gender != None and check_name_gender != None and self.gender != check_name_gender:\n",
    "            return False\n",
    "        \n",
    "        # account for all names\n",
    "        last_name_matches = [(last_names.lower() == check_vals.lower())# or textually_matches(last_names, check_vals) \n",
    "                              for last_names in self.last_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if last_names != '' and check_vals != '']\n",
    "        first_name_matches = [(first_names.lower() == check_vals.lower())# or textually_matches(first_names, check_vals) \n",
    "                              for first_names in self.first_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if first_names != '' and check_vals != '']\n",
    "        middle_name_matches = [(middle_names.lower() == check_vals.lower())# or textually_matches(middle_names, check_vals) \n",
    "                               for middle_names in self.middle_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if middle_names != '' and check_vals != '']\n",
    "        nickname_matches = [(nicknames.lower() == check_vals.lower())# or textually_matches(nicknames, check_vals) \n",
    "                            for nicknames in self.nickname\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if nicknames != '' and check_vals != '']\n",
    "        \n",
    "        \n",
    "        if (last_name_matches != [] and any(last_name_matches)) or\\\n",
    "           (first_name_matches != [] and any(first_name_matches)) or\\\n",
    "           (middle_name_matches != [] and any(middle_name_matches)) or\\\n",
    "           (nickname_matches != [] and any(nickname_matches)):\n",
    "            return True\n",
    "        \n",
    "        honorific_matches = [(hn.title.lower() == honorific.lower()) for honorific in self.honorific]\n",
    "        if (last_name_matches == [] and first_name_matches == [] and \n",
    "            middle_name_matches == [] and nickname_matches == []) and any(honorific_matches):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "        \n",
    "    def textually_close_references_entity(self, check_name):\n",
    "        '''Checks whether the given name matches this entity in a textually-close manner.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        check_name : str\n",
    "            The name against which to check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matches : bool\n",
    "            Whether the given name matches this entity in a textually-close manner\n",
    "        '''\n",
    "        \n",
    "        hn = HumanName(check_name, constants=constants)\n",
    "        check_name_gender = get_gender(hn.title, [n for n in hn.first_list + hn.middle_list + hn.nickname_list])\n",
    "\n",
    "        # first, if possible, check gender matches\n",
    "        if self.gender != None and check_name_gender != None and self.gender != check_name_gender:\n",
    "            return False\n",
    "        \n",
    "        # account for all names\n",
    "        last_name_matches = [textually_matches(last_names.lower(), check_vals.lower()) \n",
    "                              for last_names in self.last_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if last_names != '' and check_vals != '']\n",
    "        first_name_matches = [textually_matches(first_names.lower(), check_vals.lower()) \n",
    "                              for first_names in self.first_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if first_names != '' and check_vals != '']\n",
    "        middle_name_matches = [textually_matches(middle_names.lower(), check_vals.lower()) \n",
    "                               for middle_names in self.middle_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if middle_names != '' and check_vals != '']\n",
    "        nickname_matches = [textually_matches(nicknames.lower(), check_vals.lower())\n",
    "                            for nicknames in self.nickname\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname, hn.last] \n",
    "                                if nicknames != '' and check_vals != '']\n",
    "        \n",
    "        if (last_name_matches != [] and any(last_name_matches)) or\\\n",
    "           (first_name_matches != [] and any(first_name_matches)) or\\\n",
    "           (middle_name_matches != [] and any(middle_name_matches)) or\\\n",
    "           (nickname_matches != [] and any(nickname_matches)):\n",
    "            return True\n",
    "        \n",
    "        honorific_matches = [(hn.title.lower() == honorific.lower()) for honorific in self.honorific]\n",
    "        if (last_name_matches == [] and first_name_matches == [] and \n",
    "            middle_name_matches == [] and nickname_matches == []) and any(honorific_matches):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "        \n",
    "    def references_entity(self, check_name):\n",
    "        '''Checks whether the given name matches this entity in a general, approximated, \n",
    "        manner. Still requires a very high level of similarity, however.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        check_name : str\n",
    "            The name against which to check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matches : bool\n",
    "            Whether the given name matches this entity in a general, approximated, \n",
    "            manner\n",
    "        '''\n",
    "        \n",
    "        hn = HumanName(check_name, constants=constants)\n",
    "        check_name_gender = get_gender(hn.title, [n for n in hn.first_list + hn.middle_list + hn.nickname_list])\n",
    "\n",
    "        # first, if possible, check gender matches\n",
    "        if self.gender != None and check_name_gender != None and self.gender != check_name_gender:\n",
    "            return False\n",
    "        \n",
    "        # then, if possible, check last names match\n",
    "        last_name_match = [names_fuzzy_match(last_name, hn.last) for last_name in self.last_name \n",
    "                           if last_name != '' and hn.last != '']\n",
    "        if last_name_match != [] and not any(last_name_match):\n",
    "            return False\n",
    "        \n",
    "        # then, for all non-last names check that non-null equivalent entries on both sides fuzzy-match\n",
    "        # (accounting for nicknames and diminuitive names as much as possible)\n",
    "        first_name_matches = [names_fuzzy_match(first_names, check_vals) for first_names in self.first_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname] \n",
    "                                if first_names != '' and check_vals != '']\n",
    "        middle_name_matches = [names_fuzzy_match(middle_names, check_vals) for middle_names in self.middle_name\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname] \n",
    "                                if middle_names != '' and check_vals != '']\n",
    "        nickname_matches = [names_fuzzy_match(nicknames, check_vals) for nicknames in self.nickname\n",
    "                                for check_vals in [hn.first, hn.middle, hn.nickname] \n",
    "                                if nicknames != '' and check_vals != '']\n",
    "        \n",
    "        \n",
    "        if (first_name_matches != [] and any(first_name_matches)) or\\\n",
    "           (middle_name_matches != [] and any(middle_name_matches)) or\\\n",
    "           (nickname_matches != [] and any(nickname_matches)) or\\\n",
    "           (hn.first == '' and  hn.middle == '' and hn.nickname == '' and \n",
    "            (self.first_name == [''] or self.first_name == []) and \n",
    "            (self.middle_name == [''] or self.middle_name == []) and \n",
    "            (self.nickname == [''] or self.nickname == [])):\n",
    "            return True\n",
    "\n",
    "        # to do the merging below, we first need to extract gender from name as much as possible\n",
    "        # and make it a necessary condition that genders match\n",
    "        if (((self.gender != None and check_name_gender != None and self.gender == check_name_gender) \n",
    "             and any(last_name_match)) and\n",
    "            ((self.first_name == [''] or self.first_name == [] or hn.first == '') and \n",
    "            (self.middle_name == [''] or self.middle_name == [] or hn.middle == '') and \n",
    "            (self.nickname == [''] or self.nickname == [] or hn.nickname == ''))):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "        \n",
    "    def fuzzy_references_entity(self, check_name):\n",
    "        '''Checks whether the given name matches this entity in a fuzzy manner,\n",
    "        by comparing name comparisons not covered by stricter functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        check_name : str\n",
    "            The name against which to check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matches : bool\n",
    "            Whether the given name matches this entity in a fuzzy manner\n",
    "        '''\n",
    "        \n",
    "        hn = HumanName(check_name, constants=constants)\n",
    "        check_name_gender = get_gender(hn.title, [n for n in hn.first_list + hn.middle_list + hn.nickname_list])\n",
    "\n",
    "        # first, if possible, check gender matches\n",
    "        if self.gender != None and check_name_gender != None and self.gender != check_name_gender:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        # scenario #1 - first name was recognized as last name\n",
    "        scenario1_match = [names_fuzzy_match(first_name, hn.last) for first_name in self.first_name \n",
    "                           if first_name != '' and hn.last != '']\n",
    "        if scenario1_match != [] and any(scenario1_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #2 - last name was recognized as first one\n",
    "        scenario2_match = [names_fuzzy_match(last_name, hn.first) for last_name in self.last_name \n",
    "                           if last_name != '' and hn.first != '']\n",
    "        if scenario2_match != [] and any(scenario2_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #3 - middle name was recognized as last name\n",
    "        scenario3_match = [names_fuzzy_match(middle_name, hn.last) for middle_name in self.middle_name \n",
    "                           if middle_name != '' and hn.last != '']\n",
    "        if scenario3_match != [] and any(scenario3_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #4 - middle name was recognized as first name\n",
    "        scenario4_match = [names_fuzzy_match(middle_name, hn.first) for middle_name in self.middle_name \n",
    "                           if middle_name != '' and hn.first != '']\n",
    "        if scenario4_match != [] and any(scenario4_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #5 - last name was recognized as middle name\n",
    "        scenario5_match = [names_fuzzy_match(last_name, hn.middle) for last_name in self.last_name \n",
    "                           if last_name != '' and hn.middle != '']\n",
    "        if scenario5_match != [] and any(scenario5_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #6 - first name was recognized as middle name\n",
    "        scenario6_match = [names_fuzzy_match(first_name, hn.middle) for first_name in self.first_name \n",
    "                           if first_name != '' and hn.middle != '']\n",
    "        if scenario6_match != [] and any(scenario6_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #7 - nickname was recognized as first name\n",
    "        scenario7_match = [names_fuzzy_match(nickname, hn.first) for nickname in self.nickname \n",
    "                           if nickname != '' and hn.first != '']\n",
    "        if scenario7_match != [] and any(scenario7_match):\n",
    "            return True\n",
    "        \n",
    "        # scenario #8 - nickname was recognized as last name\n",
    "        scenario8_match = [names_fuzzy_match(nickname, hn.last) for nickname in self.nickname \n",
    "                           if nickname != '' and hn.last != '']\n",
    "        if scenario8_match != [] and any(scenario8_match):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "        \n",
    "    def merge(self, new_name, strictness='high'):\n",
    "        '''Merges a new form of address to the current entity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        new_name : str\n",
    "            The new form of address\n",
    "        strictness : str, optional \n",
    "            The strictness to enforce in the merge (between 'high', 'fuzzy',\n",
    "            'exact' and 'textually_close') (default is 'high')\n",
    "        '''\n",
    "        \n",
    "        # check genders, honorifics, and names\n",
    "        if strictness == 'high' and not self.references_entity(new_name):\n",
    "            return\n",
    "        elif strictness == 'fuzzy' and not self.fuzzy_references_entity(new_name):\n",
    "            return\n",
    "        elif strictness == 'exact' and not self.exactly_references_entity(new_name):\n",
    "            return\n",
    "        elif strictness == 'textually_close' and not self.textually_close_references_entity(new_name):\n",
    "            retrun\n",
    "        \n",
    "#         # check not already taken into account\n",
    "#         # (commented out because these come in handy for the second class, (non-list) BookEntity)\n",
    "#         if new_name in self.all_addresses:\n",
    "#             return\n",
    "        \n",
    "        # decide which names are being added to entity\n",
    "        hn = HumanName(new_name, constants=constants)\n",
    "        \n",
    "        if self.gender == None:\n",
    "            updated_gender = gender_from_honorific[hn.title.title()] if hn.title != '' and\\\n",
    "                                hn.title.title() in honorifics else None\n",
    "            self.gender = updated_gender\n",
    "            \n",
    "        # add all non-null entries of new_names not already present in entity\n",
    "        if strictness == 'high' or strictness == 'exact':\n",
    "            if hn.title.lower() not in self.honorific and hn.title != '':\n",
    "                self.honorific += [hn.title.lower()]\n",
    "            if hn.first.lower() not in self.first_name and hn.first != '':\n",
    "                self.first_name += [hn.first.lower()]\n",
    "            if hn.middle.lower() not in self.middle_name and hn.middle != '':\n",
    "                self.middle_name += [hn.middle.lower()]\n",
    "            if hn.last.lower() not in self.last_name and hn.last != '':\n",
    "                self.last_name += [hn.last.lower()]\n",
    "            if hn.suffix.lower() not in self.suffix and hn.suffix != '':\n",
    "                self.suffix += [hn.suffix.lower()]\n",
    "            if hn.nickname.lower() not in self.nickname and hn.nickname != '':\n",
    "                self.nickname += [hn.nickname.lower()]\n",
    "        \n",
    "        elif strictness == 'fuzzy':\n",
    "            # accurately match non-misplaceable names\n",
    "            if hn.title.lower() not in self.honorific and hn.title != '':\n",
    "                self.honorific += [hn.title.lower()]\n",
    "            if hn.suffix.lower() not in self.suffix and hn.suffix != '':\n",
    "                self.suffix += [hn.suffix.lower()]\n",
    "                \n",
    "            # fuzzy match all possibly misplaced names\n",
    "            for name in [hn.first, hn.middle, hn.last, hn.nickname]:\n",
    "                if name != '':\n",
    "                    if any([names_fuzzy_match(first_name, name) for first_name in self.first_name \n",
    "                               if first_name != '']) and name.lower() not in self.first_name:\n",
    "                        self.first_name += [name.lower()]\n",
    "                    if any([names_fuzzy_match(middle_name, name) for middle_name in self.middle_name\n",
    "                               if middle_name != '']) and name.lower() not in self.middle_name:\n",
    "                        self.middle_name += [name.lower()]\n",
    "                    if any([names_fuzzy_match(last_name, name) for last_name in self.last_name \n",
    "                               if last_name != '']) and name.lower() not in self.last_name:\n",
    "                        self.last_name += [name.lower()]\n",
    "                    if any([names_fuzzy_match(nickname, name) for nickname in self.nickname \n",
    "                               if nickname != '']) and name.lower() not in self.nickname:\n",
    "                        self.nickname += [name.lower()]\n",
    "                        \n",
    "        # save added form of address\n",
    "        self.all_addresses.append(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb24a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:10.991925Z",
     "start_time": "2021-06-11T13:27:10.966623Z"
    },
    "code_folding": [
     38,
     70,
     89,
     101
    ]
   },
   "outputs": [],
   "source": [
    "class BookEntity:\n",
    "    '''\n",
    "    A class used to represent an entity as a list of honorifics, first, middle\n",
    "    and last names, suffixes, nicknames, all their addresses and their inferred\n",
    "    gender.\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    honorific : str\n",
    "        The character's main honorific\n",
    "    first_name : str\n",
    "        The character's main first name\n",
    "    middle_names : list\n",
    "        A list of the all the entity's middle names\n",
    "    last_name : str\n",
    "        The character's main last name\n",
    "    suffix : str\n",
    "        The character's main suffix\n",
    "    nicknames : list\n",
    "        A list of the all the entity's nicknames\n",
    "    gender : str\n",
    "        The character's inferred gender\n",
    "    all_addresses : list\n",
    "        A list of the all the entity's addresses and mentions\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    get_shortname()\n",
    "        Returns the entity's \"short name\" string representation, by doing a frequency\n",
    "        analysis to identify its most common forms of address.\n",
    "        \n",
    "    @staticmethod\n",
    "    from_list_entity(list_book_entity)\n",
    "        Converts a BookListEntity into a BookEntity one.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, honorific='', first_name='', middle_names=[], last_name='', suffix='',\n",
    "                 nicknames=[], gender=None, all_addresses=[]):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        honorific : str, optional\n",
    "            The character's main honorific (default is '')\n",
    "        first_name : str, optional\n",
    "            The character's main first name (default is '')\n",
    "        middle_names : list, optional\n",
    "            A list of the all the entity's middle names (default is [])\n",
    "        last_name : str, optional\n",
    "            The character's main last name (default is '')\n",
    "        suffix : str, optional\n",
    "            The character's main suffix (default is '')\n",
    "        nicknames : list, optional\n",
    "            A list of the all the entity's nicknames (default is [])\n",
    "        gender : str, optional\n",
    "            The character's inferred gender (default is None)\n",
    "        all_addresses : list, optional\n",
    "            A list of the all the entity's addresses and mentions (default is [])\n",
    "        '''\n",
    "        \n",
    "        self.honorific = honorific\n",
    "        self.first_name = first_name\n",
    "        self.middle_names = middle_names\n",
    "        self.last_name = last_name\n",
    "        self.suffix = suffix\n",
    "        self.nicknames = nicknames\n",
    "        self.gender = gender\n",
    "        self.all_addresses = all_addresses\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''Returns the BookEntity's string representation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ent_str : str\n",
    "            The BookListEntity's string representation\n",
    "        '''\n",
    "        return f\"\"\"<BookEntity : [\n",
    "\thonorific: '{self.honorific}'\n",
    "\tfirst: '{self.first_name}' \n",
    "\tmiddle: '{' '.join(self.middle_names)}' \n",
    "\tlast: '{self.last_name}' \n",
    "\tsuffix: '{self.suffix}'\n",
    "\tnickname: '{self.nicknames}'\n",
    "\tgender: '{self.gender}'\n",
    "\tall adresses: '{self.all_addresses}'\n",
    "]>\"\"\"\n",
    "\n",
    "    def get_shortname(self):\n",
    "        '''Returns the entity's \"short name\" string representation, by doing a frequency\n",
    "        analysis to identify its most common forms of address.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        address : str\n",
    "            the entity's short name\n",
    "        '''\n",
    "        return ' '.join([self.honorific, self.first_name, self.last_name]).strip().replace('  ', ' ')\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_list_entity(list_book_entity):\n",
    "        '''Converts a BookListEntity into a BookEntity one.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        list_book_entity : BookListEntity\n",
    "            The BookListEntity to convert into a BookEntity one\n",
    "        '''\n",
    "        \n",
    "        # prepare data to create new book entity\n",
    "        # (if several addresses, choose most common one to be the main one)\n",
    "        honorific = ''\n",
    "        if list_book_entity.honorific != []:\n",
    "            honorifics_by_freq = [tup[0] for tup in sorted([(hon, sum([hon in s.lower() \n",
    "                                    for s in list_book_entity.all_addresses])) \n",
    "                                    for hon in list_book_entity.honorific], key=lambda x: x[1])]\n",
    "            honorific = honorifics_by_freq[0]\n",
    "        \n",
    "        suffix = ''\n",
    "        if list_book_entity.suffix != []:\n",
    "            suffixes_freq = [(suf, sum([suf.lower() in s.lower() for s in list_book_entity.all_addresses])) \n",
    "                               for suf in list_book_entity.suffix]\n",
    "            suffix = suffixes_freq[np.argmax(suffixes_freq)][0]\n",
    "            \n",
    "        # do an n-gram analysis to get (most common) first and last names, sending the others\n",
    "        # into either the middle names list or to the nicknames list\n",
    "        main_names = set(list_book_entity.first_name + list_book_entity.middle_name + \n",
    "                        list_book_entity.last_name)\n",
    "        \n",
    "        all_addresses_by_freq = [tup[0] for tup in sorted([(address, sum([address.lower() in s.lower() \n",
    "                                                                          for s in list_book_entity.all_addresses]))\n",
    "                                                           for address in list_book_entity.all_addresses], \n",
    "                                                          key=lambda x: x[1])]\n",
    "        \n",
    "        # if addressed as Mr/Mr. we have an easy way of getting the last name with certainty\n",
    "        last_name = ''\n",
    "        for address in all_addresses_by_freq:\n",
    "            if 'mr' in address.lower():\n",
    "                last_name = address.split()[-1]\n",
    "                break\n",
    "                     \n",
    "        # handle first, middle and last names\n",
    "        first_name = ''\n",
    "        middle_names = []\n",
    "        nicknames = []\n",
    "        if len(main_names) > 1:\n",
    "            if last_name == '':\n",
    "                if len(list_book_entity.last_name) >= 1:\n",
    "                    last_names_per_freq = [tup[0] for tup in sorted([(ln, sum([ln.lower() in s.lower() \n",
    "                                                                          for s in list_book_entity.all_addresses]))\n",
    "                                                           for ln in list_book_entity.last_name], \n",
    "                                                          key=lambda x: x[1])]\n",
    "                    last_name = last_names_per_freq[0]\n",
    "                    \n",
    "                else: # len(main_names) > 1:\n",
    "                    end_name_per_freq = [tup[0] for tup in sorted([(name.split()[-1], \n",
    "                                                                    sum([name.split()[-1].lower() in s.lower() \n",
    "                                                                         for s in list_book_entity.all_addresses]))\n",
    "                                                           for name in main_names], \n",
    "                                                          key=lambda x: x[1])]\n",
    "                    last_name = end_name_per_freq[0]\n",
    "            \n",
    "            # last name is defined, now handle first name\n",
    "            if len(list_book_entity.first_name) >= 1:\n",
    "                first_names_per_freq = [tup[0] for tup in sorted([(fn, sum([fn.lower() in s.lower() \n",
    "                                                                      for s in list_book_entity.all_addresses]))\n",
    "                                                       for fn in list_book_entity.first_name\n",
    "                                                       if fn.lower() != last_name.lower()], \n",
    "                                                      key=lambda x: x[1])]\n",
    "                first_name = first_names_per_freq[0]\n",
    "\n",
    "            else: # len(main_names) > 1:\n",
    "                first_name_per_freq = [tup[0] for tup in sorted([(name.split()[0], \n",
    "                                                                sum([name.split()[0].lower() in s.lower() \n",
    "                                                                     for s in list_book_entity.all_addresses]))\n",
    "                                                       for all_names in main_names for name in all_names.split()\n",
    "                                                       if name != '' and \n",
    "                                                          name.lower() != last_name.lower() and \n",
    "                                                          not any(name in list_book_entity.honorific)], \n",
    "                                                      key=lambda x: x[1])]\n",
    "                first_name = first_name_per_freq[0]\n",
    "            \n",
    "            # all remaining names \"must be\" middle names\n",
    "            middle_names = [name for name in main_names \n",
    "                            if (name.lower() != last_name.lower()) and (name.lower() != first_name.lower())]\n",
    "            \n",
    "            nicknames = list_book_entity.nickname\n",
    "            \n",
    "        elif len(list_book_entity.nickname) > 1:\n",
    "            # extract names from nicknames field\n",
    "            nicknames_per_freq = sorted([(nn, sum([nn.lower() in s.lower() for s in list_book_entity.all_addresses]),\n",
    "                                          sum([s.index(nn)  for s in list_book_entity.all_addresses if nn in s]))\n",
    "                                         for nn in list_book_entity.nickname], key=lambda x: x[1])\n",
    "            \n",
    "            first_name = nicknames_per_freq[0][0] if (nicknames_per_freq[0][2]/nicknames_per_freq[0][1] > \n",
    "                                                      nicknames_per_freq[1][2]/nicknames_per_freq[1][1]) else nicknames_per_freq[1][0]\n",
    "            last_name = nicknames_per_freq[0][0] if nicknames_per_freq[0][0] != first_name else nicknames_per_freq[1][0]\n",
    "            \n",
    "            \n",
    "            nicknames = [nick for nick in list_book_entity.nickname \n",
    "                         if (nick.lower() != last_name.lower()) and (nick.lower() != first_name.lower())]\n",
    "            \n",
    "        elif last_name == '':\n",
    "            only_available_name = (main_names.pop() if len(main_names) == 1 else \n",
    "                                   list_book_entity.nickname[0] if len(list_book_entity.nickname) == 1 else\n",
    "                                   '')\n",
    "            if honorific == '':\n",
    "                first_name = only_available_name\n",
    "            else:\n",
    "                last_name = only_available_name\n",
    "        \n",
    "            \n",
    "        return BookEntity(honorific.strip().title(), first_name.strip().title(), \n",
    "                          [mn.strip().title() for mn in middle_names], \n",
    "                          last_name.strip().title(), suffix.strip(), \n",
    "                          [nn.strip().title() for nn in nicknames], \n",
    "                          list_book_entity.gender, all_addresses_by_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c931a8",
   "metadata": {},
   "source": [
    "### Embedding and plotting related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca0731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:12.678019Z",
     "start_time": "2021-06-11T13:27:12.659917Z"
    },
    "code_folding": [
     0,
     77
    ]
   },
   "outputs": [],
   "source": [
    "def get_all_entity_based_BERT_embeddings(gutenberg_id, entities, context_window_size=7, \n",
    "                                         bert_model='bert-large-cased'):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each BookEntity in\n",
    "    the given list (using a context of the specified size), all the [CLS], [MASK] and mean context \n",
    "    embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    entities: list\n",
    "        A list of BookEntity instances\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    bert_model : str, optional\n",
    "        The underlying pre-trained BERT model to use (default is 'bert-large-cased')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, the lists of all their [MASK], [CLS], and \n",
    "        'context average' embeddings\n",
    "    '''\n",
    "    \n",
    "    # get book df, all its entities and all their contexts\n",
    "    book_df = get_book_df(gutenberg_id)\n",
    "    contexts = get_all_name_windows(book_df.drop_duplicates('total_word_index')['total_word_index'].to_list(), \n",
    "                         gutenberg_id, window_size=context_window_size)\n",
    "\n",
    "    # load BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "    model = BertModel.from_pretrained(bert_model)\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = {}\n",
    "    for i, context in enumerate(tqdm(contexts)):\n",
    "\n",
    "        # prepare context for embedding\n",
    "        matching_entity = None\n",
    "        for ent in entities:\n",
    "            for address in ent.all_addresses:\n",
    "                if address in context:\n",
    "                    matching_entity = (BookEntity.from_list_entity(ent).get_shortname(), address)\n",
    "        if not matching_entity:\n",
    "            continue\n",
    "        \n",
    "        entity_key_form, entity_address = matching_entity\n",
    "        entity_idx = context.lower().index(entity_address.lower())\n",
    "        text = context[:entity_idx] + '[MASK]' + context[entity_idx+len(entity_address):]\n",
    "\n",
    "        # encode context and extract relevant indexes\n",
    "        encoded_input = tokenizer(text, return_tensors='pt') \n",
    "        cls_idx = 0 # it's always the first token\n",
    "        mask_idx = encoded_input.input_ids[0].tolist().index(103)\n",
    "\n",
    "        # get the relevant embeddings and add them to the dictionary\n",
    "        output = model(**encoded_input)\n",
    "        \n",
    "        if entity_key_form in embeddings_BERT:\n",
    "            embeddings_BERT[entity_key_form]['CLS'].append(output[0][0][cls_idx, :].tolist())\n",
    "            embeddings_BERT[entity_key_form]['MASK'].append(output[0][0][mask_idx, :].tolist())\n",
    "            \n",
    "            # embeddings for all other tokens, expect final [SEP] token (hence the -1)\n",
    "            embeddings_BERT[entity_key_form]['context'].append(torch.mean(torch.cat((output[0][0][1:mask_idx, :],\n",
    "                                                                      output[0][0][mask_idx+1:-1, :])), 0).tolist())\n",
    "        else:\n",
    "            embeddings_BERT[entity_key_form] = {'CLS': [output[0][0][cls_idx, :].tolist()],\n",
    "                                                'MASK': [output[0][0][mask_idx, :].tolist()],\n",
    "                                                'context': [torch.mean(torch.cat((output[0][0][1:mask_idx, :],\n",
    "                                                                   output[0][0][mask_idx+1:-1, :])), 0).tolist()]}\n",
    "              \n",
    "#     # save embeddings to disk and then return them\n",
    "#     with open(embeddings_path, 'w+') as f:\n",
    "#         json.dump(embeddings_BERT, f)\n",
    "    return embeddings_BERT\n",
    "\n",
    "def get_averaged_entity_based_BERT_embeddings(gutenberg_id, entities, \n",
    "                                              context_window_size=7, bert_model='bert-large-cased'):\n",
    "    '''Given the book ID (and optionally several other settings), returns, for each BookEntity in\n",
    "    the given list, three embeddings: the mean [CLS] vector, the mean [MASK] vector and the mean \n",
    "    of the mean context embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gutenberg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "    entities: list\n",
    "        A list of BookEntity instances\n",
    "    context_window_size : int, optional\n",
    "        The context window size, in number of words, both backwards and forward (i.e. a window_size \n",
    "        of 7 will return a context of 15 words (7 + 1 + 7)) (default is 7)\n",
    "    bert_model : str, optional\n",
    "        The underlying pre-trained BERT model to use (default is 'bert-large-cased')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embeddings_BERT : dictionary\n",
    "        A dictionary containing, for each entity, three average embeddings, respetively the mean of all \n",
    "        their [MASK] embeddings, of all their [CLS] embeddings, and of all their 'context average' embeddings\n",
    "    '''\n",
    "\n",
    "    # get the embeddings\n",
    "    embeddings_BERT = get_all_entity_based_BERT_embeddings(gutenberg_id, entities, context_window_size, bert_model)\n",
    "    \n",
    "    # average all the embeddings\n",
    "    avg_BERT_embs = {}\n",
    "    emb_entities = embeddings_BERT.keys()\n",
    "    \n",
    "    for e in emb_entities:\n",
    "        avg_BERT_embs[e] = {'CLS': np.array(embeddings_BERT[e]['CLS']).mean(axis=0),\n",
    "                            'MASK': np.array(embeddings_BERT[e]['MASK']).mean(axis=0),\n",
    "                            'context': np.array(embeddings_BERT[e]['context']).mean(axis=0)}\n",
    "    \n",
    "    return avg_BERT_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd4a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:13.334491Z",
     "start_time": "2021-06-11T13:27:13.327469Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_entity_embeddings_2D(avg_BERT_embeddings, emb_type='MASK', title_addition=''):\n",
    "    '''Given a dictionary of average BERT embeddings (and optionally several other settings, \n",
    "    including which of the three embedding type to plot ['CLS', 'MASK' or 'context']), uses PCA \n",
    "    to reduce the embeddings to 2D and then plots them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    avg_BERT_embeddings : dictionary\n",
    "        A dictionary containing each entity and their associated embedding vectors, for the three\n",
    "        possible types (MASK, CLS and context) \n",
    "    emb_type : str, optional\n",
    "        The type of embedding to plot, from the three possible choices (MASK, CLS and context) \n",
    "        (default is 'MASK')\n",
    "    title_addition : str, optional\n",
    "        The text to add to the title (containing for example the book name) (default is '')\n",
    "    '''\n",
    "    \n",
    "    # filter embeddings according to their chosen embedding type and min/max counts\n",
    "    if emb_type != 'CLS' and emb_type != 'context':\n",
    "        emb_type = 'MASK'\n",
    "    vec_to_plot = {key: avg_BERT_embeddings[key][emb_type] for key in avg_BERT_embeddings}\n",
    "        \n",
    "    # apply PCA\n",
    "    sg_df = pd.DataFrame(vec_to_plot).T\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(sg_df)\n",
    "    \n",
    "    # plot the vectors\n",
    "    fig = px.scatter(components, x=0, y=1, color=sg_df.index, \n",
    "                     title=f'{emb_type} BERT embeddings {title_addition}',\n",
    "                     color_discrete_sequence=px.colors.qualitative.Alphabet)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad303a",
   "metadata": {},
   "source": [
    "### Create a wrapper function for creating BookEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f76126",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:27:14.369626Z",
     "start_time": "2021-06-11T13:27:14.351663Z"
    },
    "code_folding": [
     0,
     29,
     44,
     56,
     68,
     80
    ]
   },
   "outputs": [],
   "source": [
    "def get_book_entities(book_pg_id):\n",
    "    '''Given a book ID, computes the list of textually-close BookEntities and more laxly\n",
    "    match BookEntities, and returns them.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    book_pg_id : int\n",
    "        The book's Project Gutenberg ID\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    textually_close_merged_book_entities: list\n",
    "        A list of the textually-close matched BookEntity instances\n",
    "    lax_merged_book_entities: list\n",
    "        A list of the more laxly matched BookEntity instances\n",
    "    '''\n",
    "    \n",
    "    luke_df = pd.read_csv(f'notebooks_data/book_dfs/luke_{book_pg_id}_df.csv', skiprows=[0],\n",
    "                          names=['full_word', 'sentence_word_index', 'total_word_index'])\n",
    "\n",
    "    entity_list = [' '.join([word.strip(',,,.\"\"\\'!?;:-')\n",
    "                             for word in row['full_word'].split() if word.lower() not in stop_words]) \n",
    "                   for i, row in luke_df.iterrows()]\n",
    "    entity_list = [(ent, ent.lower(), 1) for ent in entity_list if ent != '']\n",
    "    entity_list = [reduce(count_reduce, group) for _, group in groupby(sorted(entity_list), key=itemgetter(1))]\n",
    "\n",
    "    final_entity_list = [ent[0] for ent in entity_list if ent[2] > 4]\n",
    "\n",
    "    all_book_entities = []\n",
    "    for name in final_entity_list:\n",
    "        # merge with already existing entity if possible\n",
    "        merged = False\n",
    "        for ent in all_book_entities:\n",
    "            if ent.exactly_references_entity(name):\n",
    "                ent.merge(name, strictness='exact')\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        # create new entity if not compatible with any of the previously existing ones\n",
    "        if not merged:\n",
    "            new_ent = BookListEntity(name)\n",
    "            all_book_entities.append(new_ent)\n",
    "\n",
    "    merged_book_entities = []\n",
    "    for ent in all_book_entities:\n",
    "        merged = False\n",
    "        for final_ent in merged_book_entities:\n",
    "            if any([final_ent.exactly_references_entity(address) for address in ent.all_addresses]):\n",
    "                [final_ent.merge(address, strictness='none') for address in ent.all_addresses]\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            merged_book_entities.append(ent)\n",
    "\n",
    "    textually_close_merged_book_entities = []\n",
    "    for ent in merged_book_entities:\n",
    "        merged = False\n",
    "        for final_ent in textually_close_merged_book_entities:\n",
    "            if any([final_ent.textually_close_references_entity(address) for address in ent.all_addresses]):\n",
    "                [final_ent.merge(address, strictness='none') for address in ent.all_addresses]\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            textually_close_merged_book_entities.append(ent)\n",
    "\n",
    "    fuzzy_merged_book_entities = []\n",
    "    for ent in merged_book_entities:\n",
    "        merged = False\n",
    "        for final_ent in fuzzy_merged_book_entities:\n",
    "            if any([final_ent.references_entity(address) for address in ent.all_addresses]):\n",
    "                [final_ent.merge(address) for address in ent.all_addresses]\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            fuzzy_merged_book_entities.append(ent)\n",
    "\n",
    "    lax_merged_book_entities = []\n",
    "    for ent in fuzzy_merged_book_entities:\n",
    "        merged = False\n",
    "        for final_ent in lax_merged_book_entities:\n",
    "            if any([final_ent.fuzzy_references_entity(address) for address in ent.all_addresses]):\n",
    "                [final_ent.merge(address, strictness='fuzzy') for address in ent.all_addresses]\n",
    "                merged = True\n",
    "                break\n",
    "\n",
    "        if not merged:\n",
    "            lax_merged_book_entities.append(ent)\n",
    "            \n",
    "    return textually_close_merged_book_entities, lax_merged_book_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81328b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Test (using David Copperfield, PG #766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3e533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:07:06.564812Z",
     "start_time": "2021-06-11T12:06:53.132444Z"
    },
    "code_folding": [
     1,
     17,
     30,
     42,
     54
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_merged_book_entities, lax_merged_book_entities = get_book_entities(766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a95840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:16:18.381848Z",
     "start_time": "2021-06-11T12:07:34.025439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(766, textually_close_merged_book_entities)\n",
    "# with open('notebooks_data/book_dfs/766_textually_close_ent_embeddings.pk', 'wb') as f:\n",
    "#     pickle.dump(textually_close_ent_embeddings, f)\n",
    "\n",
    "with open('notebooks_data/book_dfs/766_textually_close_ent_embeddings.pk', 'rb') as f:\n",
    "    textually_close_ent_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4019f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:16:18.901088Z",
     "start_time": "2021-06-11T12:16:18.384785Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6964c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:17:16.663881Z",
     "start_time": "2021-06-11T12:17:16.396884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='CLS', \n",
    "                          title_addition='- Textually Close Matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7637a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:17:20.652872Z",
     "start_time": "2021-06-11T12:17:20.386924Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='context', \n",
    "                          title_addition='- Textually Close Matching')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28e1b8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e70a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:26:17.233842Z",
     "start_time": "2021-06-11T12:17:47.723585Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# lax_ent_embeddings = get_averaged_entity_based_BERT_embeddings(766, lax_merged_book_entities)\n",
    "# with open('notebooks_data/book_dfs/766_lax_ent_embeddings.pk', 'wb') as f:\n",
    "#     pickle.dump(lax_ent_embeddings, f)\n",
    "\n",
    "with open('notebooks_data/book_dfs/766_lax_ent_embeddings.pk', 'rb') as f:\n",
    "    lax_ent_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f6a87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:26:17.461079Z",
     "start_time": "2021-06-11T12:26:17.236270Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='MASK', title_addition='- More Laxly Matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82681dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:26:17.674124Z",
     "start_time": "2021-06-11T12:26:17.462601Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='CLS', title_addition='- More Laxly Matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb9f50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T12:26:17.887643Z",
     "start_time": "2021-06-11T12:26:17.675474Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='context', title_addition='- More Laxly Matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed12d0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-jesus",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e54a7",
   "metadata": {},
   "source": [
    "## Expanding to other books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b25cac",
   "metadata": {},
   "source": [
    "Picking a few other books from Project Gutenberg, and applying the developed pipeline to them.\n",
    "\n",
    "| Book | Book's PG ID |\n",
    "| :------------- | ------------- |\n",
    "| **Moby Dick; or The Whale**, by Herman Melville | 2701 |\n",
    "| **The Brothers Karamazov**, by Fyodor Dostoyevsky *(Translator: Constance Garnett)* | 28054 |\n",
    "| **Pride and Prejudice**, by Jane Austen | 42671 |\n",
    "| **The Great Gatsby**, by F. Scott Fitzgerald | 64317 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ef7ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T18:18:15.373633Z",
     "start_time": "2021-06-10T18:18:15.369478Z"
    }
   },
   "outputs": [],
   "source": [
    "book_pg_id = 2701 # 42671 # 64317 # 28054"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930cbed",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 1 in Pipeline - use LUKE model for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0a075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T18:18:16.252730Z",
     "start_time": "2021-06-10T18:18:16.248865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_luke_checkpoints_tmp_dir = f'notebooks_data/book_dfs/tmp/{book_pg_id}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc197d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T18:18:18.146118Z",
     "start_time": "2021-06-10T18:18:17.716860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!mkdir $book_luke_checkpoints_tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10087c88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T21:42:25.377326Z",
     "start_time": "2021-06-10T18:18:19.992987Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continue_from_ckpt = False\n",
    "\n",
    "if not continue_from_ckpt:\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, book_luke_checkpoints_tmp_dir)\n",
    "    \n",
    "else:\n",
    "    # continue from checkpoint, if needed\n",
    "    latest_checkpoint = 15000\n",
    "    luke_df = pd.read_csv(f'{book_luke_checkpoints_tmp_dir}luke_tmp_df_{latest_checkpoint:05d}.csv')\n",
    "    ner_entities_checkpoint = [dict(zip(['full_word', 'sentence_word_index', 'total_word_index'], v)) \n",
    "                               for v in [tuple(r) for r in luke_df.to_numpy()]]\n",
    "\n",
    "    book_text, ner_entities_words = get_LUKE_person_entities(book_pg_id, \n",
    "                                                             book_luke_checkpoints_tmp_dir,\n",
    "                                                             last_checkpoint=latest_checkpoint,\n",
    "                                                             ner_entities_words=ner_entities_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9411218",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 2 in Pipeline - create BookEntities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936737e2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Moby Dick (PG #2701)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01107797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T11:11:06.513286Z",
     "start_time": "2021-06-10T11:11:06.509324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = 2701\n",
    "textually_close_merged_book_entities, lax_merged_book_entities = get_book_entities(book_pg_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e305d78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T11:13:27.159381Z",
     "start_time": "2021-06-10T11:11:09.698232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, \n",
    "                                                                           textually_close_merged_book_entities)\n",
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching (Moby Dick)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fa282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T07:36:53.615268Z",
     "start_time": "2021-06-07T07:34:24.092803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lax_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, lax_merged_book_entities)\n",
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- More Laxly Matched (Moby Dick)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638f22a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### The Brothers Karamazov (PG #28054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdadd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T01:32:57.614458Z",
     "start_time": "2021-06-11T01:32:57.609757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = 28054\n",
    "textually_close_merged_book_entities, lax_merged_book_entities = get_book_entities(book_pg_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2abb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T01:43:12.436427Z",
     "start_time": "2021-06-11T01:33:15.669552Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, \n",
    "                                                                           textually_close_merged_book_entities)\n",
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching (The Brothers Karamazov)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdbe29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T08:15:32.839032Z",
     "start_time": "2021-06-07T08:06:14.201155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lax_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, lax_merged_book_entities)\n",
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- More Laxly Matched (The Brothers Karamazov)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad9828",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Pride and Prejudice (PG #42671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591319e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T11:23:52.302845Z",
     "start_time": "2021-06-10T11:23:52.300578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = 42671\n",
    "textually_close_merged_book_entities, lax_merged_book_entities = get_book_entities(book_pg_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08f5bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T17:38:48.760673Z",
     "start_time": "2021-06-10T17:34:34.524042Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, \n",
    "                                                                           textually_close_merged_book_entities)\n",
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching (Pride and Prejudice)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89800742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T23:49:58.276774Z",
     "start_time": "2021-06-06T23:45:44.977320Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lax_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, lax_merged_book_entities)\n",
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- More Laxly Matched (Pride and Prejudice)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f6c6c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### The Great Gatsby (PG #64317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55d08b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T13:05:56.966642Z",
     "start_time": "2021-06-10T13:05:56.962151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "book_pg_id = 64317\n",
    "textually_close_merged_book_entities, lax_merged_book_entities = get_book_entities(book_pg_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f773211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T15:26:11.716292Z",
     "start_time": "2021-06-10T15:24:39.185133Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, \n",
    "                                                                           textually_close_merged_book_entities)\n",
    "plot_entity_embeddings_2D(textually_close_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching (The Great Gatsby)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360471fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T07:41:29.369823Z",
     "start_time": "2021-06-07T07:39:59.666046Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lax_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, lax_merged_book_entities)\n",
    "plot_entity_embeddings_2D(lax_ent_embeddings, emb_type='MASK', \n",
    "                          title_addition='- More Laxly Matched (The Great Gatsby)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cc172d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bonus - combine several books' characters' visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8aae4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:34:50.541711Z",
     "start_time": "2021-06-11T13:27:24.867857Z"
    },
    "code_folding": [
     19,
     35,
     47
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "books_to_merge = [2701, 42671, 64317]\n",
    "book_id_to_title = {'2701': 'Moby Dick', \n",
    "                    '42671': 'Pride and Prejudice', \n",
    "                    '64317': 'The Great Gatsby'}\n",
    "\n",
    "all_embs = {}\n",
    "for book_pg_id in books_to_merge:\n",
    "    textually_close_merged_book_entities, _ = get_book_entities(book_pg_id)\n",
    "    textually_close_ent_embeddings = get_averaged_entity_based_BERT_embeddings(book_pg_id, \n",
    "                                                        textually_close_merged_book_entities)\n",
    "    \n",
    "    for ent in textually_close_ent_embeddings:\n",
    "        all_embs[' - '.join([book_id_to_title[str(book_pg_id)], ent])] = textually_close_ent_embeddings[ent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40d33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T13:34:50.848421Z",
     "start_time": "2021-06-11T13:34:50.543719Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_entity_embeddings_2D(all_embs, emb_type='MASK', \n",
    "                          title_addition='- Textually Close Matching (3 books)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-berlin",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Future work suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9c89f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This section contains a starting point for both future n-gram and noun-phrases based approaches at improving the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e05ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T11:32:33.349830Z",
     "start_time": "2021-06-11T11:32:33.313239Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa342",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6b6f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_n_grams(gutenberg_id):\n",
    "    honorifics = ['Ab\\\"d', 'Admo\\\"r', 'Adv', 'Advocate', 'BDS', 'Baron', 'Baroness', 'Baronet', \n",
    "                  'Br', 'Brother', 'Cantor', 'Chancellor', 'Chief Executive', 'Chief Rabbi', \n",
    "                  'Cl', 'Counsel', 'Countess', 'DDS', 'DMD', 'DO', 'DPhil', 'DVM', 'Dame', \n",
    "                  'Dean', 'Director', 'Doc', 'Doctor', 'Dr', 'Earl', 'EdD', 'Elder', 'Emi', \n",
    "                  'Eminent', 'Esq', 'Esq.', 'Esquire', 'Eur Ing', 'Excellence', 'Excellency', \n",
    "                  'Father', 'Fr', 'Gaava\\\"d', 'Gentleman', 'Grand Rabbi', 'HAH', 'HE', 'HH', \n",
    "                  'HMEH', 'Her Excellence', 'Her Excellency', 'Her Honour', 'His All Holiness', \n",
    "                  'His Beatitude', 'His Eminence', 'His Excellence', 'His Excellency', 'His Grace', \n",
    "                  'His Holiness', 'His Honour', 'His Most Eminent Highness', 'Holy Father', 'Hon.', \n",
    "                  'Hfiz', 'Hfizah', 'Hj', 'Imm', 'KC', \\\"King\\\\'s Counsel\\\", 'Lady', 'Lord', \n",
    "                  'Lordship', 'MBBS', 'MBChB', 'MD', \\\"Ma\\\\'am\\\", 'Madam', 'Marchioness', 'Marquess', \n",
    "                  'Master', 'Mawln', 'Miss', 'Missis', 'Missus', 'Mister', 'Mistress', \n",
    "                  'Most Reverend Eminence', 'Most Reverend Excellency', 'Mr', 'Mrs', 'Mrs.', \n",
    "                  'Ms', 'Ms.', 'Muft.', 'Mx', 'My Lady', 'My Lord', 'Mz.', 'Nun', 'OD', 'Pastor', \n",
    "                  'PhD', 'PharmD', 'Pope', 'Pope Emeritus', 'Pr', 'President', 'Principal', 'Prof', \n",
    "                  'Professor', 'Provost', 'QC', \\\"Queen\\\\'s Counsel\\\", 'Qr', 'Raava\\\"d', 'Rabbi', \n",
    "                  'Rav', 'Rebbe', 'Rebbetzin', 'Rector', 'Regent', 'Rev.', 'Reverend', 'Rt Hon', \n",
    "                  'SCl', 'Saint', 'Sayyid', 'Sayyidah', 'Senior Counsel', 'Sharif', 'Shaykh', \n",
    "                  'Sir', 'Sire', 'Sis', 'Sister', 'Sr', \\\"The Hon\\\\'ble\\\", 'The Hon.', 'The Honorable', \n",
    "                  'The Honourable', 'The Most Blessed', 'The Most Honourable', 'The Most Rev', \n",
    "                  'The Most Revd', 'The Most Reverend', 'The Rev', 'The Revd', 'The Reverend', \n",
    "                  'The Right Honourable', 'The Right Reverend', 'The Rt Rev', 'Ven', 'Venerable', \n",
    "                  'Vice-Chancellor', 'Viscount', 'Viscountess', 'Warden', 'Yeoman', 'Your All Holiness', \n",
    "                  'Your Beatitude', 'Your Eminence', 'Your Excellence', 'Your Excellency', \n",
    "                  'Your Grace', 'Your Holiness', 'Your Honour', 'Your Ladyship', 'Your Most Eminent Highness']\n",
    "    \n",
    "    \n",
    "    # get book text and df\n",
    "    book_df = get_book_df(gutenberg_id)\n",
    "    sequence = get_book_text(gutenberg_id)\n",
    "    \n",
    "    tmp_entities_df = book_df.drop_duplicates('total_word_index').groupby(['full_word']).count()[['score']].reset_index()\n",
    "    entities = tmp_entities_df[(tmp_entities_df['score'] > 10) & (tmp_entities_df['full_word'].apply(len) > 2)]['full_word'].apply(lambda s: s.title()).tolist()\n",
    "    entities = entities + honorifics\n",
    "    \n",
    "    lemma_fucntion = WordNetLemmatizer()\n",
    "\n",
    "    # get n-grams\n",
    "    ngram_list = []\n",
    "    for n in range(2,5):\n",
    "        ngram = ngrams(sequence.split(), n)\n",
    "        for item in ngram:\n",
    "            for subitem in re.split('[,,,\\\"\\\"!?;:-]', ' '.join(item)):\n",
    "                split_subitem = subitem.split()\n",
    "                if len(split_subitem) > 1:\n",
    "                    if any([(word in entities) and (word == word.title()) for word in split_subitem]) \\\\\n",
    "                       and not any([word.strip(',,,.\\\"\\\"\\'!?;:-') in stopwords.words('english') for word in split_subitem]) \\\\\n",
    "                       and not any([w[1][0] in 'CIMPRVTW' for w in nltk.pos_tag( # all categories - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "                        word_tokenize(' '.join([i.strip(',,,.\\\"\\\"\\'!?;:-') for i in split_subitem])))]):\n",
    "                        ngram_list.append([i.strip(',,,.\\\"\\\"\\'!?;:-') for i in split_subitem])\n",
    "    return ngram_list\n",
    "            \n",
    "n_grams_list = extract_n_grams(766)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd0bf9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams_df = pd.DataFrame([' '.join(ngram).lower().strip(',,,.\"\"\\'!?;:-')\n",
    "                                         .replace('\\'s', '').replace('s', '').replace('s', '') \n",
    "                          for ngram in n_grams_list], columns=['ngram'])\n",
    "ngrams_df['count'] = 1\n",
    "ngrams_count_df = ngrams_df.groupby(['ngram'], as_index=False).sum().sort_values(by=['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1451ba3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams_count_df[ngrams_count_df['count'] > 5].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a48cd9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams_count_df[ngrams_count_df['count'] > 5].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee116c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams_count_df[ngrams_count_df['ngram'].str.contains('jane ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99c714",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ngrams_count_df[ngrams_count_df['ngram'].str.contains(' heep')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8703881",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# starting suggestion:\n",
    "\n",
    "common_words_to_ignore = ['dear', 'friend', 'towards', 'yes', 'no', 'towards']\n",
    "ngrams_to_consider = ngrams_count_df[ngrams_count_df['count'] > 5].ngram.values\n",
    "\n",
    "for ngram in ngrams_to_consider:\n",
    "    # ignore identified common words\n",
    "    ngram = ' '.join([w for w in ngram.split() if w not in common_words_to_ignore])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-regulation",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Extract Noun Phrases instead of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-concrete",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T11:32:59.786338Z",
     "start_time": "2021-06-11T11:32:55.954951Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gutenberg_id = 946\n",
    "book_text = get_book_text(gutenberg_id)\n",
    "sentence_level_book = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', book_text)\n",
    "\n",
    "\n",
    "full_name_occurences = []\n",
    "extractor = ConllExtractor()\n",
    "\n",
    "for i, s in enumerate(tqdm(sentence_level_book)):\n",
    "    \n",
    "    blob = TextBlob(s, np_extractor=extractor)\n",
    "    for n in blob.noun_phrases:\n",
    "        if any([n for w in lady_susan_df.drop_duplicates('total_word_index')['full_word'].unique()\n",
    "               if w in list(tokenize(n.lower()))]):\n",
    "            full_name_occurences.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-shame",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T11:34:47.705203Z",
     "start_time": "2021-06-11T11:34:47.682777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nouns_df = pd.DataFrame(full_name_occurences, columns=['name'])\n",
    "nouns_df['count'] = 1\n",
    "nouns_df = nouns_df.groupby('name').count()\n",
    "nouns_df[nouns_df['count'] > 1].sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e7ef3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# starting suggestion:\n",
    "\n",
    "common_words_to_ignore = ['street']\n",
    "noun_phrases_to_consider = nouns_df[nouns_df['count'] > 1].name.values\n",
    "\n",
    "for noun_p in noun_phrases_to_consider:\n",
    "    # ignore identified common words\n",
    "    noun_phrases_to_consider = ' '.join([w for w in noun_p.split() if w not in common_words_to_ignore])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-monitor",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.pinimg.com/originals/a6/36/fc/a636fc44b6502370617271d3088acd02.png\" style=\"height:100px; overflow: hidden\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dh] *",
   "language": "python",
   "name": "conda-env-dh-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
